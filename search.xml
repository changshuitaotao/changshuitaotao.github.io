<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>回归算法</title>
    <url>/2022/12/24/1.%E5%9B%9E%E5%BD%92%E7%AE%97%E6%B3%95/</url>
    <content><![CDATA[<p>​</p>
<span id="more"></span>
<h1 id="符号">符号</h1>
<p><span class="math inline">\(X\)</span>：数据矩阵，<span
class="math inline">\(X\in R^{n*p}\)</span></p>
<p><span class="math inline">\(\vec
y\)</span>：标签向量（列向量），<span class="math inline">\(\vec y\in
R^n\)</span></p>
<p><span class="math inline">\(\vec
w\)</span>：参数向量（列向量），<span class="math inline">\(\vec w\in
R^p\)</span></p>
<p><span class="math inline">\(\vec
x\)</span>：一条观测数据的特征向量（列向量），<span
class="math inline">\(\vec x\in R^p\)</span></p>
<p><span class="math inline">\(l\)</span>：一条数据的损失</p>
<p><span class="math inline">\(L\)</span>：总损失</p>
<p><span class="math inline">\(dL\)</span>：损失函数的微分</p>
<p><span class="math inline">\(\nabla_{\vec w}L\)</span>：损失函数<span
class="math inline">\(L\)</span>对<span class="math inline">\(\vec
w\)</span>的梯度</p>
<p><span class="math inline">\(\nabla^2_\vec w L\)</span>：损失函数<span
class="math inline">\(L\)</span>对<span class="math inline">\(\vec
w\)</span>的Hessian矩阵</p>
<h1 id="线性回归">线性回归</h1>
<h2 id="模型">模型</h2>
<p><span class="math display">\[
\hat y=\vec x^T\vec w
\]</span></p>
<h2 id="损失">损失</h2>
<p><span class="math display">\[
l=(y-\hat y)^2=(y-\vec x^T\vec w)^2
\]</span></p>
<p><span class="math display">\[
L=(\vec y-X\vec w)^T(\vec y-X\vec w)\\
=\vec y^T\vec y-\vec y^TX\vec w-\vec w^TX^T\vec y+\vec w^TX^TX\vec w\\
=\vec y^T\vec y-2\vec y^TX\vec w+\vec w^TX^TX\vec w
\]</span></p>
<h2 id="优化">优化</h2>
<h3 id="梯度">梯度</h3>
<p>微分： <span class="math display">\[
dL=-2\vec y^TXd\vec w+d\vec w^T(X^TX\vec w)+\vec w^TX^TXd\vec w\\
=2(\vec w^TX^TX-\vec y^TX)d\vec w\\
=2(X^TX\vec w-X^T\vec y)^Td\vec w
\]</span> 梯度： <span class="math display">\[
\nabla_{\vec w}L=\frac{\partial L}{\partial \vec w}=2(X^TX\vec w-X^T\vec
y)
\]</span></p>
<h3 id="hessian矩阵">Hessian矩阵</h3>
<p>微分： <span class="math display">\[
d\nabla_{\vec w}L=2X^TXd\vec w\\
=2[X^TX]^Td\vec w
\]</span> hessian矩阵： <span class="math display">\[
\nabla_{\vec w}^2L=\frac{\partial^2L}{\partial \vec w\partial \vec
w^T}=2X^TX
\]</span></p>
<h3 id="解">解</h3>
<h4 id="x满秩">X满秩</h4>
<p>hessian矩阵正定： <span class="math display">\[
\vec v^TX^TX\vec v=(X\vec v)^T(X\vec v)\gt0,\vec v\ne0
\]</span></p>
<p>解为： <span class="math display">\[
\vec w=(X^TX)^{-1}X^T\vec y
\]</span> 矩阵<span class="math inline">\(X\)</span>的左逆：<span
class="math inline">\((X^TX)^{-1}X^T\)</span>。</p>
<p>投影矩阵：<span class="math inline">\(X(X^TX)^{-1}X^T\)</span></p>
<h4 id="x不满秩">X不满秩</h4>
<p>hessian矩阵半正定。 <span class="math display">\[
X=U\Sigma V^{-1}\\
X^{+}=V\Sigma^{-1}U^{-1}\\
XX^{+}=U\Sigma\Sigma^{-1}U^{-1}=U\Sigma\Sigma^{-1}U^T
\]</span></p>
<p><span class="math inline">\(XX^{+}\)</span>是投影矩阵，解为： <span
class="math display">\[
\vec w=X^{+}\vec y
\]</span></p>
<h1 id="岭回归">岭回归</h1>
<h2 id="模型-1">模型</h2>
<p><span class="math display">\[
\hat y=\vec x^T\vec w
\]</span></p>
<h2 id="损失-1">损失</h2>
<p><span class="math display">\[
L=(\vec y-X\vec w)^T(\vec y-X\vec w)+\lambda\vec w^T\vec w\\
=(\vec y^T-\vec w^TX^T)(\vec y-X\vec w)+\lambda \vec w^T\vec w\\
=\vec y^T\vec y-\vec y^TX\vec w-\vec w^TX^T\vec y+\vec w^TX^TX\vec
w+\lambda w^T\vec w\\
=\vec y^T\vec y-2\vec y^TX\vec w+\vec w^TX^TX\vec w+\lambda \vec w^T\vec
w
\]</span></p>
<h2 id="优化-1">优化</h2>
<h3 id="梯度-1">梯度</h3>
<p>微分： <span class="math display">\[
dL=-2\vec y^TXd\vec w+d\vec w^T(X^TX\vec w)+\vec w^TX^TXd\vec
w+\lambda(d\vec w^T)\vec w+\lambda \vec w^Td\vec w\\
=-2\vec y^TXd\vec w+2\vec w^TX^TXd\vec w+2\lambda \vec w^Td\vec w\\
=(2\vec w^TX^TX+2\lambda \vec w^T-2\vec y^TX)d\vec w\\
=(2X^TX\vec w+2\lambda \vec w-2X^T\vec y)^Td\vec w
\]</span></p>
<p>梯度： <span class="math display">\[
\nabla_{\vec w}L=2(X^TX\vec w+\lambda\vec w-X^T\vec y)
\]</span></p>
<h3 id="hessian矩阵-1">Hessian矩阵</h3>
<p>微分： <span class="math display">\[
d\nabla_{\vec w}L=2X^TXd\vec w+2\lambda Id\vec w\\
=2(X^TX+\lambda I)^Td\vec w
\]</span></p>
<p>hessian矩阵为： <span class="math display">\[
\nabla_{\vec w}^2L=2(X^TX+\lambda I)
\]</span></p>
<p>hessian矩阵必然正定： <span class="math display">\[
\vec v^T(X^TX+\lambda I)\vec v,\vec v\ne 0\\
=\vec v^TX^TX\vec v+\vec v^T\lambda I\vec v\gt0
\]</span></p>
<h3 id="解-1">解</h3>
<p><span class="math display">\[
2(X^TX\vec w+\lambda \vec w-X^T\vec y)=0\\
\rightarrow\\
\vec w=(X^TX+\lambda I)^{-1}X^T\vec y
\]</span></p>
<h1 id="lasso回归">lasso回归</h1>
<h2 id="模型-2">模型</h2>
<p><span class="math display">\[
\hat y=\vec x^T\vec w=\sum_{j=1}^px_jw_j
\]</span></p>
<h2 id="损失-2">损失</h2>
<p><span class="math display">\[
L=\sum_{i=1}^n(y_i-\sum_{j=1}^px_{ij}w_j)^2+\alpha\sum_{j=1}^p|w_j|
\]</span></p>
<h2 id="优化-2">优化</h2>
<p>用坐标下降法进行优化，计算<span
class="math inline">\(w_k(k=1,2,...,p)\)</span>的偏导数： <span
class="math display">\[
\frac{\partial L}{\partial
w_k}=2\sum_{i=1}^n(y_i-\sum_{j=1}^px_{ij}w_j)(-x_{ik})+\alpha\frac{\partial
|w_k|}{\partial w_k}\\
=2\sum_{i=1}^n(y_i-\sum_{j\ne
k}^px_{ij}w_j-x_{ik}w_k)(-x_{ik})+\alpha\frac{\partial |w_k|}{\partial
w_k}\\
=-2\sum_{i=1}^n(y_i-\sum_{j\ne
k}^px_{ij}w_j)x_{ik}+2w_k\sum_{i=1}^nx_{ik}^2+\alpha\frac{\partial
|w_k|}{\partial w_k}\\
=-A_k+w_kB_k+\alpha\frac{\partial |w_k|}{\partial w_k}
\]</span> 其中<span
class="math inline">\(A_k=2\sum_{i=1}^n(y_i-\sum_{j\ne
k}^px_{ij}w_j)x_{ik},B_k=2\sum_{i=1}^nx_{ik}^2\gt 0\)</span>。进一步：
<span class="math display">\[
\frac{\partial L}{\partial w_k}=-A_k+w_kB_k+\alpha \frac{\partial
|w_k|}{\partial w_k}\\=
\left\{\begin{aligned}
  -A_k+w_kB_k+\alpha,w_k&gt;0\\
  -A_k±\alpha,w_k=0\\
  -A_k+w_kB_k-\alpha,w_k&lt;0
\end{aligned}\right.
\]</span> 令偏导数为0，解<span class="math inline">\(w_k^*\)</span>为：
<span class="math display">\[
w_k^*=
\left\{\begin{aligned}
  \frac{A_k-\alpha}{B_k},w_k^*&gt;0\\
  0,w_k^*=0\\
  \frac{A_k+\alpha}{B_k},w_k^*&lt;0
\end{aligned}\right.
\]</span> 进而为： <span class="math display">\[
w_k^*=
\left\{\begin{aligned}
  \frac{A_k-\alpha}{B_k},A_k&gt;\alpha\\
  0,-\alpha \le A_k\le\alpha\\
  \frac{A_k+\alpha}{B_k},A_k&lt;-\alpha
\end{aligned}\right.
\]</span> 从<span
class="math inline">\(w_1\)</span>开始，依次对参数<span
class="math inline">\(w_2,...,w_p\)</span>进行迭代，同时判断是否满足收敛条件。到<span
class="math inline">\(w_p\)</span>后仍然不收敛则进行下一轮迭代，直到最终收敛。</p>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>回归</tag>
      </tags>
  </entry>
  <entry>
    <title>可视化库seaborn快速入门</title>
    <url>/2023/01/09/10.%E5%8F%AF%E8%A7%86%E5%8C%96%E5%BA%93seaborn/</url>
    <content><![CDATA[<p>​</p>
<span id="more"></span>
<p>Seaborn可以分为<code>Figure-level</code>的接口（操作对象是<code>matplotlib</code>中figure）和<code>axes-level</code>的接口（对应操作对象是matplotlib中的axes）。</p>
<h1 id="连续变量统计图">连续变量统计图</h1>
<h2 id="单变量图">单变量图</h2>
<p>所用的数据集为威斯康星州乳腺癌数据集。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_breast_cancer</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">%matplotlib</span><br><span class="line">cancer = load_breast_cancer()</span><br><span class="line">df = cancer[<span class="string">&#x27;data&#x27;</span>]</span><br><span class="line">df = pd.DataFrame(df,columns=cancer[<span class="string">&#x27;feature_names&#x27;</span>])</span><br><span class="line">df[<span class="string">&#x27;y&#x27;</span>] = cancer[<span class="string">&#x27;target&#x27;</span>]</span><br></pre></td></tr></table></figure>
<h3 id="直方图">直方图</h3>
<p><code>sns.histplot</code>接口绘制直方图。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">sns.histplot(df <span class="comment"># 数据集</span></span><br><span class="line">             ,x=<span class="string">&#x27;mean radius&#x27;</span> <span class="comment"># 分析变量</span></span><br><span class="line">             ,hue = <span class="string">&#x27;y&#x27;</span> <span class="comment"># 分类变量</span></span><br><span class="line">             ,stat = <span class="string">&#x27;count&#x27;</span> <span class="comment"># 统计量</span></span><br><span class="line">             ,kde=<span class="literal">True</span> <span class="comment"># 绘制核密度估计</span></span><br><span class="line">            )</span><br></pre></td></tr></table></figure>
<figure>
<img src="sns_直方图.png" alt="sns_直方图" />
<figcaption aria-hidden="true">sns_直方图</figcaption>
</figure>
<h3 id="核密度图">核密度图</h3>
<p><code>sns.kdeplot</code>接口绘制核密度图。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">sns.kdeplot(data = df <span class="comment"># 数据集</span></span><br><span class="line">            ,x = <span class="string">&#x27;mean radius&#x27;</span> <span class="comment"># 分析变量</span></span><br><span class="line">            ,hue = <span class="string">&#x27;y&#x27;</span> <span class="comment"># 分类变量</span></span><br><span class="line">            )</span><br></pre></td></tr></table></figure>
<figure>
<img src="C:\Users\Yujie\Desktop\机器学习笔记\可视化\sns_核密度图.png"
alt="sns_核密度图" />
<figcaption aria-hidden="true">sns_核密度图</figcaption>
</figure>
<h3 id="累积分布图">累积分布图</h3>
<p><code>sns.ecdfplot</code>接口绘制累积分布图。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">sns.ecdfplot(data = df  <span class="comment"># 数据集</span></span><br><span class="line">             ,x = <span class="string">&#x27;mean radius&#x27;</span> <span class="comment"># 分析变量</span></span><br><span class="line">             ,hue = <span class="string">&#x27;y&#x27;</span> <span class="comment"># 分类变量</span></span><br><span class="line">             ,stat= <span class="string">&#x27;count&#x27;</span> <span class="comment"># 统计量</span></span><br><span class="line">             )</span><br></pre></td></tr></table></figure>
<figure>
<img src="C:\Users\Yujie\Desktop\机器学习笔记\可视化\sns累积分布图.png"
alt="sns累积分布图" />
<figcaption aria-hidden="true">sns累积分布图</figcaption>
</figure>
<h3 id="其他绘图函数">其他绘图函数</h3>
<p><code>sns.displot</code>接口可以绘制<code>直方图，核密度图，经验分布图</code>。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#1. 直方图</span></span><br><span class="line">sns.displot(data = df <span class="comment"># 数据集</span></span><br><span class="line">            ,x = <span class="string">&#x27;mean radius&#x27;</span> <span class="comment"># 分析变量</span></span><br><span class="line">            ,hue = <span class="string">&#x27;y&#x27;</span> <span class="comment"># 分类变量</span></span><br><span class="line">            ,stat = <span class="string">&#x27;count&#x27;</span> <span class="comment"># 统计量</span></span><br><span class="line">            ,kind = <span class="string">&#x27;hist&#x27;</span> <span class="comment"># 图形类别</span></span><br><span class="line">    )</span><br><span class="line"><span class="comment"># 2.核密度图</span></span><br><span class="line">sns.displot(data = df <span class="comment"># 数据集</span></span><br><span class="line">            ,x = <span class="string">&#x27;mean radius&#x27;</span> <span class="comment"># 分析变量</span></span><br><span class="line">            ,row = <span class="string">&#x27;y&#x27;</span> <span class="comment"># 分类变量</span></span><br><span class="line">            ,kind = <span class="string">&#x27;kde&#x27;</span> <span class="comment"># 图形类别</span></span><br><span class="line">    )</span><br><span class="line"><span class="comment"># 3.累计分布图</span></span><br><span class="line">sns.displot(data = df <span class="comment"># 数据集</span></span><br><span class="line">            ,x = <span class="string">&#x27;mean radius&#x27;</span> <span class="comment"># 分析变量</span></span><br><span class="line">            ,col = <span class="string">&#x27;y&#x27;</span> <span class="comment"># 分类变量</span></span><br><span class="line">            ,stat = <span class="string">&#x27;count&#x27;</span> <span class="comment"># 统计量</span></span><br><span class="line">            ,kind = <span class="string">&#x27;ecdf&#x27;</span> <span class="comment"># 图形类别</span></span><br><span class="line">    )</span><br></pre></td></tr></table></figure>
<p><strong><code>row,col,hue</code>三个参数都指定一个分类变量，表示对这个分类变量的不同水平分别绘图。<code>hue</code>绘出的图会显示在同一个<code>axes</code>中，<code>row</code>和<code>col</code>参数绘制的不会在同一个<code>axes</code>中，<code>row</code>会使<code>axes</code>按行排列，<code>col</code>参数会使<code>axes</code>按列排列。</strong></p>
<h2 id="双变量">双变量</h2>
<h3 id="散点图">散点图</h3>
<p><code>sns.scatterplot</code>接口绘制散点图</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 散点图</span></span><br><span class="line">sns.scatterplot(data = df <span class="comment"># 散点图</span></span><br><span class="line">                ,x = <span class="string">&#x27;mean radius&#x27;</span> <span class="comment"># 分析变量1</span></span><br><span class="line">                ,y = <span class="string">&#x27;mean texture&#x27;</span> <span class="comment"># 分析变量2 </span></span><br><span class="line">                ,hue = <span class="string">&#x27;y&#x27;</span> <span class="comment"># 分类变量</span></span><br><span class="line">                )</span><br></pre></td></tr></table></figure>
<figure>
<img src="C:\Users\Yujie\Desktop\机器学习笔记\可视化\sns_散点图.png"
alt="sns_散点图" />
<figcaption aria-hidden="true">sns_散点图</figcaption>
</figure>
<h3 id="lineplot">lineplot</h3>
<p>sns.lineplot将每一个x对应的y取均值和置信区间绘图：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">sns.lineplot(data = df <span class="comment"># 数据集</span></span><br><span class="line">            ,x = <span class="string">&#x27;mean radius&#x27;</span> <span class="comment"># 分析变量1</span></span><br><span class="line">            ,y = <span class="string">&#x27;mean texture&#x27;</span> <span class="comment"># 分析变量2 </span></span><br><span class="line">            ,hue = <span class="string">&#x27;y&#x27;</span> <span class="comment"># 分类变量 </span></span><br><span class="line">            )</span><br></pre></td></tr></table></figure>
<figure>
<img src="C:\Users\Yujie\Desktop\机器学习笔记\可视化\sns_lineplot.png"
alt="sns_lineplot" />
<figcaption aria-hidden="true">sns_lineplot</figcaption>
</figure>
<h3 id="relplot">relplot</h3>
<p><code>sns.relplot</code>既能画散点图也可以画<code>lineplot</code>，不过它是一个<code>Figure-level</code>的接口，而上面两个是<code>axes-level</code>的接口</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 画散点图</span></span><br><span class="line">sns.relplot(data = df <span class="comment"># 数据集</span></span><br><span class="line">            ,x = <span class="string">&#x27;mean radius&#x27;</span> <span class="comment"># 分析变量1</span></span><br><span class="line">            ,y = <span class="string">&#x27;mean texture&#x27;</span> <span class="comment"># 分析变量2 </span></span><br><span class="line">            ,hue = <span class="string">&#x27;y&#x27;</span> <span class="comment"># 分类变量</span></span><br><span class="line">            ,kind = <span class="string">&#x27;scatter&#x27;</span></span><br><span class="line">            )</span><br></pre></td></tr></table></figure>
<figure>
<img src="C:\Users\Yujie\Desktop\机器学习笔记\可视化\sns_relplot.png"
alt="sns_relplot" />
<figcaption aria-hidden="true">sns_relplot</figcaption>
</figure>
<h3 id="回归分析图">回归分析图</h3>
<p><code>sns.regplot</code>绘制散点图、回归直线和置信区间</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">sns.regplot(data = df <span class="comment"># 数据集</span></span><br><span class="line">            ,x = <span class="string">&#x27;mean radius&#x27;</span> <span class="comment"># 分析变量1</span></span><br><span class="line">            ,y = <span class="string">&#x27;mean texture&#x27;</span> <span class="comment"># 分析变量2 </span></span><br><span class="line">            )</span><br></pre></td></tr></table></figure>
<figure>
<img src="C:\Users\Yujie\Desktop\机器学习笔记\可视化\sns_回归图.png"
alt="sns_回归图" />
<figcaption aria-hidden="true">sns_回归图</figcaption>
</figure>
<p><code>sns.lmplot</code>可以对分类变量的不同水平分别做回归</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 分层回归</span></span><br><span class="line">sns.lmplot(data = df</span><br><span class="line">            ,x = <span class="string">&#x27;mean radius&#x27;</span> <span class="comment"># 分析变量1</span></span><br><span class="line">            ,y = <span class="string">&#x27;mean texture&#x27;</span> <span class="comment"># 分析变量2 </span></span><br><span class="line">            ,hue = <span class="string">&#x27;y&#x27;</span>)</span><br></pre></td></tr></table></figure>
<figure>
<img src="C:\Users\Yujie\Desktop\机器学习笔记\可视化\sns_分层回归.png"
alt="sns_分层回归" />
<figcaption aria-hidden="true">sns_分层回归</figcaption>
</figure>
<p><code>sns.lmplot</code>中还可添加row和col参数（二者均为FacetGrid中的常规参数，用于添加多子图的行和列）实现更多的分类回归关系。</p>
<p><code>sns.residplot</code>提供了拟合后的残差分布图.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">sns.residplot(data = df</span><br><span class="line">            ,x = <span class="string">&#x27;mean radius&#x27;</span> <span class="comment"># 分析变量1</span></span><br><span class="line">            ,y = <span class="string">&#x27;mean texture&#x27;</span> <span class="comment"># 分析变量2 </span></span><br><span class="line">             )</span><br></pre></td></tr></table></figure>
<figure>
<img src="C:\Users\Yujie\Desktop\机器学习笔记\可视化\sns_残差图.png"
alt="sns_残差图" />
<figcaption aria-hidden="true">sns_残差图</figcaption>
</figure>
<h3 id="热力图">热力图</h3>
<p><code>sns.heatmap</code>接口绘制热力图。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">corr = df[[<span class="string">&#x27;mean radius&#x27;</span>, <span class="string">&#x27;mean texture&#x27;</span>, <span class="string">&#x27;mean smoothness&#x27;</span>, <span class="string">&#x27;mean compactness&#x27;</span>]].corr()</span><br><span class="line">sns.heatmap(corr)</span><br></pre></td></tr></table></figure>
<figure>
<img src="C:\Users\Yujie\Desktop\机器学习笔记\可视化\sns_热力图.png"
alt="sns_热力图" />
<figcaption aria-hidden="true">sns_热力图</figcaption>
</figure>
<h3 id="联合分布图">联合分布图</h3>
<p><code>sns.jointplot</code>接口可以绘制<code>联合分布图</code>。<code>联合分布图</code>可以是散点图、带回归直线的散点图、密度图、直方图、残差图和六边形分箱图等。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 2.双连续变量图</span></span><br><span class="line"><span class="comment"># =============================================================================</span></span><br><span class="line"><span class="comment"># 2.1.1 散点图</span></span><br><span class="line">sns.jointplot(data = df <span class="comment"># 数据集</span></span><br><span class="line">              ,x = <span class="string">&#x27;mean radius&#x27;</span> <span class="comment"># 分析变量1</span></span><br><span class="line">              ,y = <span class="string">&#x27;mean texture&#x27;</span> <span class="comment"># 分析变量2</span></span><br><span class="line">              ,kind = <span class="string">&#x27;scatter&#x27;</span> <span class="comment"># 散点图</span></span><br><span class="line">              )</span><br></pre></td></tr></table></figure>
<figure>
<img src="C:\Users\Yujie\Desktop\机器学习笔记\可视化\sns_散点图.png"
alt="sns_散点图" />
<figcaption aria-hidden="true">sns_散点图</figcaption>
</figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 2.1.2 带回归直线的散点图</span></span><br><span class="line">sns.jointplot(data = df <span class="comment"># 数据集</span></span><br><span class="line">              ,x = <span class="string">&#x27;mean radius&#x27;</span> <span class="comment"># 分析变量1</span></span><br><span class="line">              ,y = <span class="string">&#x27;mean texture&#x27;</span> <span class="comment"># 分析变量2</span></span><br><span class="line">              ,kind = <span class="string">&#x27;reg&#x27;</span> <span class="comment"># 带回归直线的散点图</span></span><br><span class="line">              )</span><br></pre></td></tr></table></figure>
<figure>
<img
src="C:\Users\Yujie\Desktop\机器学习笔记\可视化\sns_散点图带回归直线.png"
alt="sns_散点图带回归直线" />
<figcaption aria-hidden="true">sns_散点图带回归直线</figcaption>
</figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 2.1.3 回归残差图</span></span><br><span class="line">sns.jointplot(data = df <span class="comment"># 数据集</span></span><br><span class="line">              ,x = <span class="string">&#x27;mean radius&#x27;</span> <span class="comment"># 分析变量1</span></span><br><span class="line">              ,y = <span class="string">&#x27;mean texture&#x27;</span> <span class="comment"># 分析变量2</span></span><br><span class="line">              ,kind = <span class="string">&#x27;resid&#x27;</span>  <span class="comment"># 回归残差图</span></span><br><span class="line">              )</span><br></pre></td></tr></table></figure>
<figure>
<img src="C:\Users\Yujie\Desktop\机器学习笔记\可视化\sns_回归残差图.png"
alt="sns_回归残差图" />
<figcaption aria-hidden="true">sns_回归残差图</figcaption>
</figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 2.1.4 密度图</span></span><br><span class="line">sns.jointplot(data = df <span class="comment"># 数据集</span></span><br><span class="line">              ,x = <span class="string">&#x27;mean radius&#x27;</span> <span class="comment"># 分析变量1</span></span><br><span class="line">              ,y = <span class="string">&#x27;mean texture&#x27;</span> <span class="comment"># 分析变量2</span></span><br><span class="line">              ,kind = <span class="string">&#x27;kde&#x27;</span> <span class="comment"># 核密度图</span></span><br><span class="line">              )</span><br></pre></td></tr></table></figure>
<figure>
<img
src="C:\Users\Yujie\Desktop\机器学习笔记\可视化\sns_二维核密度图.png"
alt="sns_二维核密度图" />
<figcaption aria-hidden="true">sns_二维核密度图</figcaption>
</figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 2.1.5 直方图</span></span><br><span class="line">sns.jointplot(data = df <span class="comment"># 数据集</span></span><br><span class="line">              ,x = <span class="string">&#x27;mean radius&#x27;</span> <span class="comment"># 分析变量1</span></span><br><span class="line">              ,y = <span class="string">&#x27;mean texture&#x27;</span> <span class="comment"># 分析变量2</span></span><br><span class="line">              ,kind = <span class="string">&#x27;hist&#x27;</span> <span class="comment"># 二维直方图</span></span><br><span class="line">              )</span><br></pre></td></tr></table></figure>
<figure>
<img src="C:\Users\Yujie\Desktop\机器学习笔记\可视化\sns_二维直方图.png"
alt="sns_二维直方图" />
<figcaption aria-hidden="true">sns_二维直方图</figcaption>
</figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 2.1.6 六边形分箱图</span></span><br><span class="line">sns.jointplot(data = df <span class="comment"># 数据集</span></span><br><span class="line">              ,x = <span class="string">&#x27;mean radius&#x27;</span> <span class="comment"># 分析变量1</span></span><br><span class="line">              ,y = <span class="string">&#x27;mean texture&#x27;</span> <span class="comment"># 分析变量2</span></span><br><span class="line">              ,kind = <span class="string">&#x27;hex&#x27;</span>  <span class="comment"># 六边形分箱图</span></span><br><span class="line">              )</span><br></pre></td></tr></table></figure>
<figure>
<img
src="C:\Users\Yujie\Desktop\机器学习笔记\可视化\sns_六边形分箱图.png"
alt="sns_六边形分箱图" />
<figcaption aria-hidden="true">sns_六边形分箱图</figcaption>
</figure>
<h3 id="两两联合分布矩阵图">两两联合分布矩阵图</h3>
<p><code>sns.pairplot</code>接口可以绘制<code>两两联合分布图</code>。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 2.2.1 散点图</span></span><br><span class="line">sns.pairplot(data = df <span class="comment"># 数据集</span></span><br><span class="line">             ,<span class="built_in">vars</span> = [<span class="string">&#x27;mean radius&#x27;</span>, <span class="string">&#x27;mean texture&#x27;</span>, <span class="string">&#x27;mean smoothness&#x27;</span>, <span class="string">&#x27;mean compactness&#x27;</span>] <span class="comment"># 分析变量</span></span><br><span class="line">             ,kind = <span class="string">&#x27;scatter&#x27;</span> <span class="comment"># 绘制散点图</span></span><br><span class="line">             ,diag_kind = <span class="string">&#x27;auto&#x27;</span> <span class="comment"># 对角线上图形自动选择</span></span><br><span class="line">             )</span><br></pre></td></tr></table></figure>
<figure>
<img
src="C:\Users\Yujie\Desktop\机器学习笔记\可视化\sns_多维两两散点图.png"
alt="sns_多维两两散点图" />
<figcaption aria-hidden="true">sns_多维两两散点图</figcaption>
</figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 2.2.2 带回归线的散点图</span></span><br><span class="line">sns.pairplot(data = df <span class="comment"># 数据集</span></span><br><span class="line">             ,<span class="built_in">vars</span> = [<span class="string">&#x27;mean radius&#x27;</span>, <span class="string">&#x27;mean texture&#x27;</span>, <span class="string">&#x27;mean smoothness&#x27;</span>, <span class="string">&#x27;mean compactness&#x27;</span>] <span class="comment"># 分析变量</span></span><br><span class="line">             ,kind = <span class="string">&#x27;reg&#x27;</span> <span class="comment"># 绘制带回归线的散点图</span></span><br><span class="line">             ,diag_kind = <span class="string">&#x27;kde&#x27;</span> <span class="comment"># 对角线画密度图</span></span><br><span class="line">             )</span><br></pre></td></tr></table></figure>
<figure>
<img
src="C:\Users\Yujie\Desktop\机器学习笔记\可视化\sns_多维两两带回归直线散点图.png"
alt="sns_多维两两带回归直线散点图" />
<figcaption aria-hidden="true">sns_多维两两带回归直线散点图</figcaption>
</figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 2.2.3 密度图</span></span><br><span class="line">sns.pairplot(data = df <span class="comment"># 数据集</span></span><br><span class="line">             ,<span class="built_in">vars</span> = [<span class="string">&#x27;mean radius&#x27;</span>, <span class="string">&#x27;mean texture&#x27;</span>, <span class="string">&#x27;mean smoothness&#x27;</span>, <span class="string">&#x27;mean compactness&#x27;</span>] <span class="comment"># 分析变量</span></span><br><span class="line">             ,kind = <span class="string">&#x27;kde&#x27;</span> <span class="comment"># 绘制密度图</span></span><br><span class="line">             ,diag_kind = <span class="string">&#x27;auto&#x27;</span> <span class="comment"># 对角线上图形自动选择</span></span><br><span class="line">             )</span><br></pre></td></tr></table></figure>
<figure>
<img
src="C:\Users\Yujie\Desktop\机器学习笔记\可视化\sns_多维两两密度图.png"
alt="sns_多维两两密度图" />
<figcaption aria-hidden="true">sns_多维两两密度图</figcaption>
</figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 2.2.3 直方图</span></span><br><span class="line">sns.pairplot(data = df <span class="comment"># 数据集</span></span><br><span class="line">             ,<span class="built_in">vars</span> = [<span class="string">&#x27;mean radius&#x27;</span>, <span class="string">&#x27;mean texture&#x27;</span>, <span class="string">&#x27;mean smoothness&#x27;</span>, <span class="string">&#x27;mean compactness&#x27;</span>] <span class="comment"># 分析变量</span></span><br><span class="line">             ,kind = <span class="string">&#x27;hist&#x27;</span> <span class="comment"># 绘制直方图</span></span><br><span class="line">             ,diag_kind = <span class="string">&#x27;kde&#x27;</span> <span class="comment"># 对角线画密度图</span></span><br><span class="line">             )</span><br></pre></td></tr></table></figure>
<figure>
<img
src="C:\Users\Yujie\Desktop\机器学习笔记\可视化\sns_多维两两二维直方图.png"
alt="sns_多维两两二维直方图" />
<figcaption aria-hidden="true">sns_多维两两二维直方图</figcaption>
</figure>
<p>实际上<code>sns.pairplot</code>依赖<code>sns.PairGrid</code>类来实现，我们也可以先调用<code>sns.PairGrid</code>类，然后可以自定义矩阵的对角线、上三角和下三角的图形种类。类似地<code>sns.jointplot</code>依赖<code>sns.JointGrid</code>类。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 2.2.4 自定义</span></span><br><span class="line">pp = sns.PairGrid(data = df <span class="comment"># 数据集</span></span><br><span class="line">                  ,<span class="built_in">vars</span> = [<span class="string">&#x27;mean radius&#x27;</span>, <span class="string">&#x27;mean texture&#x27;</span>, <span class="string">&#x27;mean smoothness&#x27;</span>, <span class="string">&#x27;mean compactness&#x27;</span>] <span class="comment"># 分析变量</span></span><br><span class="line">                  )</span><br><span class="line">pp.map_diag(plt.hist)</span><br><span class="line">pp.map_lower(plt.scatter)</span><br><span class="line">pp.map_upper(sns.kdeplot)</span><br></pre></td></tr></table></figure>
<figure>
<img
src="C:\Users\Yujie\Desktop\机器学习笔记\可视化\sns_多维两两自定义图.png"
alt="sns_多维两两自定义图" />
<figcaption aria-hidden="true">sns_多维两两自定义图</figcaption>
</figure>
<h1 id="分类变量统计图">分类变量统计图</h1>
<h2 id="条形图">条形图</h2>
]]></content>
      <categories>
        <category>数据可视化</category>
      </categories>
      <tags>
        <tag>数据可视化</tag>
        <tag>seaborn</tag>
      </tags>
  </entry>
  <entry>
    <title>线性混合模型</title>
    <url>/2023/01/27/12.%E7%BA%BF%E6%80%A7%E6%B7%B7%E5%90%88%E6%A8%A1%E5%9E%8B/</url>
    <content><![CDATA[<p>​</p>
<span id="more"></span>
<h1 id="模型形式和假定">模型形式和假定</h1>
<p>假设数据按照某分组变量分为<span
class="math inline">\(I\)</span>组，用<span
class="math inline">\(i\)</span>表示组索引，<span
class="math inline">\(i=1,2,...,I\)</span>，每组包括的观测数目为<span
class="math inline">\(n_i\)</span>，总样本量为<span
class="math inline">\(n=\sum_{i=1}^In_i\)</span>。</p>
<p>线性混合模型的基本形式为： <span class="math display">\[
\begin{aligned}
\vec y_{i}&amp;=X_i\vec \beta+Z_i\vec \alpha_i +\vec \epsilon_i\\
\vec y_i\in R^{n_i},X_i\in R^{n_i×p},&amp;\vec \beta\in R^p,Z_i\in
R^{n_i×q},\vec \alpha_i \in R^{q},\vec \epsilon_i \in R^{n_i}
\end{aligned}
\]</span> 模型中：</p>
<ul>
<li><span class="math inline">\(\vec y_i\)</span>表示第<span
class="math inline">\(i\)</span>组标签向量，向量长度为<span
class="math inline">\(n_i\)</span>；</li>
<li><span class="math inline">\(X_i\)</span>表示第<span
class="math inline">\(i\)</span>组固定效应的数据矩阵，矩阵大小为<span
class="math inline">\(n_i×p\)</span>；</li>
<li><span class="math inline">\(\vec
\beta\)</span>表示固定效应向量，向量长度为<span
class="math inline">\(p\)</span>，注意没有下标<span
class="math inline">\(i\)</span>，这意味着<span
class="math inline">\(X_i\)</span>变量的效应不会跨组发生变化；</li>
<li><span class="math inline">\(Z_i\)</span>表示第<span
class="math inline">\(i\)</span>组随机效应的数据矩阵，矩阵大小为<span
class="math inline">\(n_i×q\)</span>，<span
class="math inline">\(Z_i\)</span><strong>来源于<span
class="math inline">\(X_i\)</span>中部分列</strong>；</li>
<li><span class="math inline">\(\vec
\alpha_i\)</span>表示随机效应向量，向量长度为<span
class="math inline">\(q\)</span>。注意<span class="math inline">\(\vec
\alpha_i\)</span>有下标<span
class="math inline">\(i\)</span>，这意味着<strong>对于同一组共享同一个<span
class="math inline">\(\vec
\alpha_i\)</span>，这将导致同一组内数据产生组内相关。同时对于不同组，<span
class="math inline">\(\vec
\alpha_i\)</span>相互独立</strong>。<strong>另外<span
class="math inline">\(\vec
\alpha_i\)</span>中的每一个分量，可以相互独立也可以存在相关</strong>，需要根据数据确定；</li>
<li><span class="math inline">\(\vec \epsilon_i\)</span>表示第<span
class="math inline">\(i\)</span>组的随机误差向量，长度为<span
class="math inline">\(n_i\)</span>，<strong>对于同一组误差向量，其每一个分量都是相互独立的，对于不同的组，误差也是相互独立</strong>。</li>
</ul>
<p>模型中固定效应参数<span class="math inline">\(\vec
\beta\)</span>是需要估计和推断的，随机效应<span
class="math inline">\(\vec
\alpha_i\)</span>是一个随机变量，无法进行估计，只能进行预测。 关于<span
class="math inline">\(\vec \alpha_i\)</span>和<span
class="math inline">\(\vec \epsilon_i\)</span>一般还有如下的分布假定：
<span class="math display">\[
\begin{bmatrix}
\vec \alpha_i\\
\vec \epsilon_i
\end{bmatrix}\sim
N_{q+n_i}
\begin{bmatrix}
\begin{bmatrix}
\vec 0\\
\vec 0
\end{bmatrix},
\begin{bmatrix}
\tilde G&amp;0_{n_i}\\
0_{q}&amp;R_i
\end{bmatrix}
\end{bmatrix}
\]</span> <span class="math inline">\(\vec \alpha_i\sim N(\vec 0,\tilde
G),\vec \epsilon_i\sim N(\vec 0,R_i)\)</span>。<strong>注意<span
class="math inline">\(\vec \alpha_i\)</span>的协方差矩阵没有下标<span
class="math inline">\(i\)</span>，这意味着任一组有相同的协方差矩阵</strong>。<span
class="math inline">\(\vec\alpha_i\)</span>是一个<span
class="math inline">\(q\)</span>维的随机向量，<span
class="math inline">\(\vec \epsilon_i\)</span>是一个<span
class="math inline">\(n_i\)</span>维的随机向量，组合起来就是<span
class="math inline">\(q+n_i\)</span>维的随机向量。这个<span
class="math inline">\(q+n_i\)</span>维随机向量的均值为<span
class="math inline">\(\vec 0\)</span>，协方差矩阵为维度为<span
class="math inline">\((q+n_i)×(q+n_i)\)</span>，协方差矩阵的主对角线元素为<span
class="math inline">\(G\)</span>和<span
class="math inline">\(R_i\)</span>，其他为0，这意味着<span
class="math inline">\(\vec \alpha_i\)</span>和<span
class="math inline">\(\vec \epsilon_i\)</span>不相关。</p>
<p>以上只是第<span
class="math inline">\(i\)</span>组的模型，现将所有组合并给出总模型。我们将所有的<span
class="math inline">\(\vec y_i\)</span>摞起来得到列向量<span
class="math inline">\(\vec y\)</span>，其长度为<span
class="math inline">\(n\)</span>，<span
class="math inline">\(X_i\)</span>摞起来得到<span
class="math inline">\(X\)</span>，其大小为<span
class="math inline">\(n×p\)</span>，<span class="math inline">\(\vec
\beta\)</span>不变，<span
class="math inline">\(Z_i\)</span>沿着对角线合并成大小为<span
class="math inline">\(n×qI\)</span>大小的矩阵（矩阵没有元素的位置用0补全），即<span
class="math inline">\(Z=diag(Z_1,...Z_i,...Z_I)\)</span>，<span
class="math inline">\(\vec \alpha_i\)</span>摞起来得到长度为<span
class="math inline">\(qI\)</span>的列向量，<span
class="math inline">\(\vec \epsilon_i\)</span>摞起来得到长度为<span
class="math inline">\(n\)</span>的列向量。因此线性混合模型的完整表达为：
<span class="math display">\[
\begin{aligned}
\vec y=&amp;X\vec \beta+Z\vec \alpha+\vec \epsilon\\
\vec y\in R^n,X\in R^{n×p},\vec \beta\in R^{p},&amp;Z\in R^{n×qI},\vec
\alpha\in R^{qI},\vec \epsilon\in R^n
\end{aligned}
\]</span> 其中 <span class="math display">\[
\begin{bmatrix}
\vec \alpha\\
\vec \epsilon
\end{bmatrix}\sim
N_{qI+n}
\begin{bmatrix}
\begin{bmatrix}
\vec 0\\
\vec 0
\end{bmatrix},
\begin{bmatrix}
G&amp;0_{n}\\
0_{qI}&amp;R
\end{bmatrix}
\end{bmatrix}
\]</span> <span class="math inline">\(\vec\alpha\)</span>是一个<span
class="math inline">\(qI\)</span>维的随机向量，<span
class="math inline">\(\vec \epsilon\)</span>是一个<span
class="math inline">\(n\)</span>维随机向量，组合起来就是<span
class="math inline">\(qI+n\)</span>维随机向量。这个<span
class="math inline">\(qI+n\)</span>维随机向量的均值为<span
class="math inline">\(\vec 0\)</span>，协方差矩阵为维度为<span
class="math inline">\((qI+n)*(qI+n)\)</span>，协方差矩阵的主对角线元素为<span
class="math inline">\(G\)</span>和<span
class="math inline">\(R\)</span>，其他为0，这意味着<span
class="math inline">\(\vec \alpha\)</span>和<span
class="math inline">\(\vec \epsilon\)</span>不相关。另外<span
class="math inline">\(G=diag(\tilde G,\tilde G,...,\tilde
G),R=diag(R_1,...R_i,...,R_I)\)</span>。</p>
<h1 id="矩阵构造">矩阵构造</h1>
<p>下面简单介绍一下重要矩阵的构造方法： <span class="math display">\[
\begin{aligned}
\vec y=\begin{bmatrix}
\vec y_1\\
\cdot\cdot\cdot\\
\vec y_i\\
\cdot\cdot\cdot\\
\vec y_I
\end{bmatrix};
X=&amp;\begin{bmatrix}
X_1\\
\cdot\cdot\cdot\\
X_i\\
\cdot\cdot\cdot\\
X_I
\end{bmatrix};
\vec \beta=\vec\beta;
Z=\begin{bmatrix}
Z_1&amp;0&amp;\cdot\cdot\cdot&amp;0\\
0&amp;Z_2&amp;\cdot\cdot\cdot&amp;0\\
\cdot\cdot\cdot&amp;\cdot\cdot\cdot&amp;\cdot\cdot\cdot&amp;\cdot\cdot\cdot\\
0&amp;0&amp;0&amp;Z_I\\
\end{bmatrix};
\vec \alpha=\begin{bmatrix}
\vec \alpha_1\\
\cdot\cdot\cdot\\
\vec \alpha_i\\
\cdot\cdot\cdot\\
\vec \alpha_I
\end{bmatrix};
\vec \epsilon=\begin{bmatrix}
\vec \epsilon_1\\
\cdot\cdot\cdot\\
\vec \epsilon_i\\
\cdot\cdot\cdot\\
\vec \epsilon_I
\end{bmatrix};
\\
G=&amp;\begin{bmatrix}
G&amp;0&amp;\cdot\cdot\cdot&amp;0\\
0&amp;G&amp;\cdot\cdot\cdot&amp;0\\
\cdot\cdot\cdot&amp;\cdot\cdot\cdot&amp;\cdot\cdot\cdot&amp;\cdot\cdot\cdot\\
0&amp;0&amp;0&amp;G\\
\end{bmatrix};
R=\begin{bmatrix}
R_1&amp;0&amp;\cdot\cdot\cdot&amp;0\\
0&amp;R_2&amp;\cdot\cdot\cdot&amp;0\\
\cdot\cdot\cdot&amp;\cdot\cdot\cdot&amp;\cdot\cdot\cdot&amp;\cdot\cdot\cdot\\
0&amp;0&amp;0&amp;R_I\\
\end{bmatrix}
\end{aligned}
\]</span></p>
<h1 id="因变量的分布">因变量的分布</h1>
<h2 id="条件分布">条件分布</h2>
<p>对于线性混合模型： <span class="math display">\[
\begin{aligned}
\vec y=&amp;X\vec \beta+Z\vec \alpha+\vec \epsilon\\
\vec y\in R^n,X\in R^{n×p},\vec \beta\in R^{p},&amp;Z\in R^{n×qI},\vec
\alpha\in R^{qI},\vec \epsilon\in R^n
\end{aligned}
\]</span> <span class="math inline">\(\vec \alpha\)</span>和<span
class="math inline">\(\vec \epsilon\)</span>均为随机的，当<span
class="math inline">\(\vec \alpha\)</span>固定时，可以导出<span
class="math inline">\(\vec y\)</span>的条件分布为多元正态分布： <span
class="math display">\[
\begin{aligned}
\vec y|\vec \alpha&amp;\sim N(X\vec \beta+Z\vec \alpha,R)\\
\vec \alpha&amp;\sim N(\vec 0,G)
\end{aligned}
\]</span></p>
<h2 id="边缘分布">边缘分布</h2>
<p><span class="math inline">\(\vec
y\)</span>的边缘分布同样为多元正态分布： <span class="math display">\[
\vec y\sim N(X\vec\beta,ZGZ^T+R)
\]</span> 方便起见，我们可以定义<span class="math inline">\(V\)</span>：
<span class="math display">\[
V=ZGZ^T+R
\]</span> 从给定<span class="math inline">\(\vec \alpha\)</span>后<span
class="math inline">\(\vec
y\)</span>的条件分布及其边缘分布来看，固定效应系数<span
class="math inline">\(\vec \beta\)</span>起到了相同的作用，
可以将其看成总体（所有受试者）所共有的特性。也就是说，对于总体的平均估计为<span
class="math inline">\(X\vec \beta\)</span>，对于某个特定组的预测为<span
class="math inline">\(X\vec \beta+Z\vec \alpha\)</span>。</p>
<h1 id="参数估计">参数估计</h1>
<h2 id="混合模型方程">混合模型方程</h2>
<p>由于已经假定<span class="math inline">\(\vec \alpha\sim N(\vec
0,G)\)</span>，其密度函数为： <span class="math display">\[
f(\vec \alpha)=\frac{1}{\sqrt{(2\pi)^{qI}|G|}}exp^{[-\frac{1}{2}\vec
\alpha^TG^{-1}\vec \alpha]}
\]</span> 并且我们推导出<span class="math inline">\(\vec y|\vec
\alpha\sim N(X\vec \beta+Z\vec \alpha,R)\)</span>，其密度函数为： <span
class="math display">\[
f(\vec y|\vec
\alpha)=\frac{1}{\sqrt{(2\pi)^{qI}|R|}}exp^{[-\frac{1}{2}(\vec y-X\vec
\beta-Z\vec \alpha)^TR^{-1}(\vec y-X\vec \beta-Z\vec \alpha)]}
\]</span> 这样就可以写出<span class="math inline">\(\vec
\alpha\)</span>与<span class="math inline">\(\vec y\)</span>的联合密度：
<span class="math display">\[
\begin{aligned}
f(\vec y,\vec \alpha)=&amp;f(\vec y|\vec \alpha)f(\vec \alpha)\\
=\frac{1}{\sqrt{(2\pi)^{qJ}|G|}}exp^{[-\frac{1}{2}\vec
\alpha^TG^{-1}\vec
\alpha]}&amp;\frac{1}{\sqrt{(2\pi)^{qJ}|R|}}exp^{[-\frac{1}{2}(\vec
y-X\vec \beta-Z\vec \alpha)^TR^{-1}(\vec y-X\vec \beta-Z\vec \alpha)]}
\end{aligned}
\]</span> 将其看作是参数为<span class="math inline">\(\vec
\alpha\)</span>和<span class="math inline">\(\vec
\beta\)</span>的似然函数，并取对数为： <span class="math display">\[
\mathop{ln}f(\vec y,\vec \alpha)=-\mathop
{ln}\sqrt{(2\pi)^{qI}|G|}-\mathop{ln}\sqrt{(2\pi)^{qI}|R|}-\frac{1}{2}\vec
\alpha^TG^{-1}\vec \alpha-\frac{1}{2}(\vec y-X\vec \beta-Z\vec
\alpha)^TR^{-1}(\vec y-X\vec \beta-Z\vec \alpha)
\]</span> 只需要最小化下面式子即可： <span class="math display">\[
L=\vec \alpha^TG^{-1}\vec \alpha+(\vec y-X\vec \beta-Z\vec
\alpha)^TR^{-1}(\vec y-X\vec \beta-Z\vec \alpha)
\]</span> 将式子展开： <span class="math display">\[
\begin{aligned}
L=&amp;\vec \alpha^TG^{-1}\vec \alpha\\+\vec y^TR^{-1}\vec y-\vec
y^TR^{-1}&amp;X\vec \beta-\vec y^TR^{-1}Z\vec \alpha\\-\vec
\beta^TX^TR^{-1}\vec y+\vec \beta^T X^TR^{-1}&amp;X\vec \beta+\vec
\beta^TX^TR^{-1}Z\vec \alpha\\-\vec \alpha^TZ^TR^{-1}\vec y+\vec
\alpha^TZ^TR^{-1}&amp;X\vec \beta+\vec \alpha^TZ^TR^{-1}Z\vec \alpha
\end{aligned}
\]</span> 合并同类项得： <span class="math display">\[
\begin{aligned}
L=&amp;\vec \alpha^TG^{-1}\vec \alpha\\+\vec y^T&amp;R^{-1}\vec
y\\-2\vec \beta^TX^TR^{-1}\vec y+\vec \beta^T&amp; X^TR^{-1}X\vec
\beta+\vec 2\beta^TX^TR^{-1}Z\vec \alpha\\-2\vec \alpha^TZ^TR^{-1}\vec
y+&amp;\vec \alpha^TZ^TR^{-1}Z\vec \alpha
\end{aligned}
\]</span> 该式子分别对<span class="math inline">\(\vec
\alpha\)</span>和<span class="math inline">\(\vec
\beta\)</span>求偏导可得： <span class="math display">\[
\begin{aligned}
\frac{\partial L}{\partial \vec \alpha}=2(G^{-1}\vec
\alpha+Z^TR^{-1}X\vec \beta-Z^TR^{-1}\vec y+Z^TR^{-1}Z\vec \alpha)\\
\frac{\partial L}{\partial \vec \beta}=2(X^TR^{-1}X\vec
\beta+X^TR^{-1}Z\vec \alpha-X^TR^{-1}\vec y)
\end{aligned}
\]</span> 令偏导数为0，可以得到下面的方程系统： <span
class="math display">\[
\begin{bmatrix}
Z^TR^{-1}X&amp;G^{-1}+Z^TR^{-1}Z\\
X^TR^{-1}X&amp;X^TR^{-1}Z
\end{bmatrix}
\begin{bmatrix}
\vec \beta\\
\vec \alpha
\end{bmatrix}=
\begin{bmatrix}
Z^TR^{-1}\vec y\\
X^TR^{-1}\vec y
\end{bmatrix}
\]</span></p>
<p>这个方程系统被称为混合模型方程。这个方程的解为： <span
class="math display">\[
\begin{bmatrix}
\tilde{\vec \beta}\\
\tilde{\vec \alpha}
\end{bmatrix}=
\begin{bmatrix}
(X^TV^{-1}X)^{-1}X^TV^{-1}\vec y\\
GZ^TV^{-1}(\vec y-X(X^TV^{-1}X)^{-1}X^TV^{-1}\vec y)
\end{bmatrix}
\]</span></p>
<h2 id="混合模型方程求解">混合模型方程求解</h2>
<p>我们令两个偏导数为0，可得： <span class="math display">\[
\begin{aligned}
X^TR^{-1}X\tilde {\vec \beta}+X^TR^{-1}Z\vec \alpha=X^TR^{-1}\vec y\\
(G^{-1}+Z^TR^{-1}Z)\tilde{\vec \alpha}+Z^TR^{-1}X\vec
\beta=Z^TR^{-1}\vec y
\end{aligned}
\]</span></p>
]]></content>
      <categories>
        <category>数理统计</category>
      </categories>
      <tags>
        <tag>回归</tag>
      </tags>
  </entry>
  <entry>
    <title></title>
    <url>/2023/04/05/14.%E5%B8%B8%E7%94%A8%E7%9A%84%E5%B9%BF%E4%B9%89%E7%BA%BF%E5%9E%8B%E6%A8%A1%E5%9E%8B/</url>
    <content><![CDATA[<h1 id="广义线性模型框架">广义线性模型框架</h1>
<p>模型 <span class="math display">\[
y|X\sim exp(\theta,\phi,a,b,c)
\]</span> <span
class="math inline">\(exp(\theta,\phi,a,b,c)\)</span>表示指数分布族</p>
<p>回归方程： <span class="math display">\[
\mu=E(y|X)\\
g(\mu)=\eta\\
\eta=X\vec\beta
\]</span></p>
<h1 id="高斯模型">高斯模型</h1>
<p>这里的高斯模型主要指的就是线性回归模型，它其实要求<span
class="math inline">\(y\)</span>的分布来源于正态分布。已知正态分布的密度为：
<span class="math display">\[
f(y)=\frac{1}{\sqrt{2\pi}\sigma}exp(-\frac{(y-\mu)^2}{2\sigma^2})\\
=exp(-ln({\sqrt{2\pi}\sigma})-\frac{y^2-2y\mu+\mu^2}{2\sigma^2})\\
=exp(\frac{y\mu-\frac{1}{2}\mu^2}{\sigma^2}-ln({\sqrt{2\pi}\sigma})-\frac{y^2}{2\sigma^2})
\]</span> 因此可知： <span class="math display">\[
\theta=\mu\\
b(\theta)=\frac{1}{2}\mu^2=\frac{1}{2}\theta^2\\
a(\phi)=\sigma^2
\]</span> 分布的均值为<span
class="math inline">\(b\prime(\theta)\)</span>的一阶导数： <span
class="math display">\[
\mu=b\prime(\theta)
\]</span> <span
class="math inline">\(b\prime\prime(\theta)\)</span>为方差函数<span
class="math inline">\(v(\mu)\)</span>，该例中为常数1，这意味着正态分布中均值和方差相互独立。分布的方差为<span
class="math inline">\(b\prime\prime(\theta)a(\phi)\)</span>： <span
class="math display">\[
a(\phi)b\prime\prime(\theta)=\sigma^2
\]</span></p>
<h2 id="模型设定">模型设定</h2>
<p>若<span
class="math inline">\(y|X\)</span>为正态分布，当使用规范连接函数时，要求<span
class="math inline">\(\theta=\eta\)</span>，因此模型的形式为： <span
class="math display">\[
g(\mu)=\eta=\theta=\mu
\]</span> 即要求<span
class="math inline">\(g(\mu)=\mu\)</span>，这意味着<span
class="math inline">\(g\)</span>函数可以选择恒等连接函数，因此回归方程实际上为：
<span class="math display">\[
\mu=\eta=X\vec \beta
\]</span> 这就是线性回归。</p>
<h2 id="参数估计">参数估计</h2>
<p>可以使用<code>IRLS</code>法进行参数估计，其迭代公式为： <span
class="math display">\[
\vec \beta_{k+1}=(X^TW_{k}X)^{-1}(X^TW_k\vec z_k)
\]</span> <span class="math inline">\(\vec z_k\)</span>为工作向量，<span
class="math inline">\(\vec z_k=X\hat{\vec \beta}_k+g\prime(\vec
\mu_k)(\vec y-\vec \mu_k)\)</span>，因为选择了恒等连接，所以<span
class="math inline">\(g(\mu)=\mu\)</span>，进而<span
class="math inline">\(g\prime(\mu_k)=1\)</span>，所以<span
class="math inline">\(\vec z_k=X\vec \beta_k+\vec y-\vec \mu_k=\vec
y\)</span>。<span
class="math inline">\(W_k\)</span>为权重矩阵，是一个对角矩阵，对角线上元素为<span
class="math inline">\(\frac{1}{Var(y|X)[g\prime(\mu_k)]^2}\)</span>，如果假定了方差齐，即<span
class="math inline">\(Var(y|X)=\sigma^2\)</span>，因此<span
class="math inline">\(W_k\)</span>为： <span class="math display">\[
W_k=\begin{bmatrix}
\frac{1}{\sigma^2}&amp;\cdot\cdot\cdot&amp;0&amp;\cdot\cdot\cdot&amp;0\\
\cdot\cdot\cdot&amp;\cdot\cdot\cdot&amp;\cdot\cdot\cdot&amp;\cdot\cdot\cdot&amp;\cdot\cdot\cdot\\
0&amp;\cdot\cdot\cdot&amp;\frac{1}{\sigma^2}&amp;\cdot\cdot\cdot&amp;0\\
\cdot\cdot\cdot&amp;\cdot\cdot\cdot&amp;\cdot\cdot\cdot&amp;\cdot\cdot\cdot&amp;\cdot\cdot\cdot\\
0&amp;\cdot\cdot\cdot&amp;0&amp;\cdot\cdot\cdot&amp;\frac{1}{\sigma^2}\\
\end{bmatrix}
\]</span> 化简后得到迭代公式为: <span class="math display">\[
\vec \beta_{k+1}=(X^TX)^{-1}X^T\vec y
\]</span> 这和最小二乘法结果是一致的。如果方差不齐，则<span
class="math inline">\(W_k\)</span>为： <span class="math display">\[
W_k=\begin{bmatrix}
\frac{1}{\sigma^2_1}&amp;\cdot\cdot\cdot&amp;0&amp;\cdot\cdot\cdot&amp;0\\
\cdot\cdot\cdot&amp;\cdot\cdot\cdot&amp;\cdot\cdot\cdot&amp;\cdot\cdot\cdot&amp;\cdot\cdot\cdot\\
0&amp;\cdot\cdot\cdot&amp;\frac{1}{\sigma^2_i}&amp;\cdot\cdot\cdot&amp;0\\
\cdot\cdot\cdot&amp;\cdot\cdot\cdot&amp;\cdot\cdot\cdot&amp;\cdot\cdot\cdot&amp;\cdot\cdot\cdot\\
0&amp;\cdot\cdot\cdot&amp;0&amp;\cdot\cdot\cdot&amp;\frac{1}{\sigma^2_n}\\
\end{bmatrix}=W
\]</span> 此时迭代公式为： <span class="math display">\[
\vec \beta_{k+1}=(X^TWX)^{-1}(X^TW\vec y)
\]</span> 这是异方差最小二乘法。</p>
<h2 id="拟合优度">拟合优度</h2>
<h3 id="偏差deviance统计量">偏差deviance统计量</h3>
<p>首先写出似然函数<span class="math inline">\(L\)</span>： <span
class="math display">\[
L=\prod_{i=1}^n\frac{1}{\sqrt{2\pi}\sigma}exp(-\frac{(y_i-\mu_i)^2}{2\sigma^2})
\]</span> -2倍对数似然函数<span class="math inline">\(-2LL\)</span>为：
<span class="math display">\[
-2LL=nln(2\pi\sigma^2)+\frac{1}{\sigma^2}\sum_{i=1}^n(y_i-\mu_i)^2
\]</span> 饱和模型的-2倍对数似然函数<span
class="math inline">\(-2LL_s\)</span>就是令<span
class="math inline">\(\mu_i=y_i\)</span>，即为： <span
class="math display">\[
-2LL_s=nln(2\pi\sigma^2)
\]</span> 因此deviance统计量<span class="math inline">\(D\)</span>为：
<span class="math display">\[
D=(-2LL-(-2LL_s))a(\phi)\\=\frac{1}{\sigma^2}\sum_{i=1}^n(y_i-\mu_i)^2\sigma^2\\=\sum_{i=1}^n(y_i-\mu_i)^2==SSE
\]</span> 因此对于线性回归，deviance统计量实际上就是残差平方和<span
class="math inline">\(SSE\)</span>。</p>
<p>更进一步，scaled deviance统计量<span
class="math inline">\(D^*\)</span>等于： <span class="math display">\[
D^*=\frac{D}{a(\phi)}=\sum_{i=1}^n\frac{(y_i-\mu_i)^2}{\sigma^2}\sim
\chi^2(n-p-1)
\]</span> 与此同时，null deviance统计量<span
class="math inline">\(D_0\)</span>等于： <span class="math display">\[
D_0=\sum_{i=1}^n(y_i-\bar y)^2==SST
\]</span> 这正好是线性回归中的总平方和<span
class="math inline">\(SST\)</span>，而决定系数<span
class="math inline">\(R^2\)</span>为： <span class="math display">\[
R^2=1-\frac{D_0}{D}=1-\frac{SSE}{SST}
\]</span></p>
<h3 id="pearson卡方统计量">Pearson卡方统计量</h3>
<p>对观测值<span class="math inline">\(y_i\)</span>和期望值<span
class="math inline">\(\mu_i\)</span>做Pearson卡方检验，有： <span
class="math display">\[
\chi^2=\sum_{i=1}^n\frac{(y_i-\mu_i)^2}{Var(\mu_i)}
\]</span></p>
<h1 id="二项式模型">二项式模型</h1>
<h1 id="多项式模型">多项式模型</h1>
<h1 id="计数模型">计数模型</h1>
<h1 id="负二项式模型">负二项式模型</h1>
<h1 id="排序模型">排序模型</h1>
<h1 id="gamma模型">gamma模型</h1>
]]></content>
  </entry>
  <entry>
    <title>线性回归模型</title>
    <url>/2023/01/28/11.%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B/</url>
    <content><![CDATA[<p>​</p>
<span id="more"></span>
<h1 id="模型">模型</h1>
<p>线性回归模型的基本形式如下： <span class="math display">\[
\begin{aligned}
\hat y|\vec
x=\beta_0+\beta_1x_1+&amp;\beta_2x_2+...+\beta_px_p+\epsilon\\
=\vec x^T\vec \beta&amp;+\epsilon\\
\vec \beta \in R^{p+1}&amp;,\vec x\in R^{p+1}
\end{aligned}
\]</span></p>
<h1 id="假定">假定</h1>
<p>线性回归模型是广义线性模型的一个特例，当连接函数<span
class="math inline">\(g(\cdot)\)</span>选择<span
class="math inline">\(identity\quad
link\)</span>，且分布族选择正态分布时，广义线性模型退化为线性回归模型。类似广义线性模型，线性回归模型的假定有：</p>
<ol type="1">
<li>线性假定：在给定<span class="math inline">\(\vec
x\)</span>后，随机变量的条件均值为自变量的线性函数，或者说每个自变量对<span
class="math inline">\(y\)</span>的边际效应为常数<span
class="math inline">\(\beta\)</span>；</li>
<li>严格外生性：<span
class="math inline">\(E(\epsilon_i|X)=0\)</span>，就是说给定数据矩阵<span
class="math inline">\(X\)</span>之后，扰动项<span
class="math inline">\(\epsilon_i\)</span>的条件期望为0，也即<span
class="math inline">\(\epsilon_i\)</span>均值独立于所有解释变量的观测数据，而不仅仅是同一观测数据<span
class="math inline">\(\vec x_i\)</span>的解释变量。这意味着<span
class="math inline">\(\epsilon\)</span>与所有个体的解释变量都不相关。此假定能保证OLS是一个无偏估计。</li>
<li>不存在严格多重共线性；</li>
<li>球形扰动项：即扰动项具有零均值、等方差、不相关。此假定能保证OLS是方差最小的无偏估计。</li>
</ol>
<h1 id="参数估计">参数估计</h1>
<p><span class="math display">\[
\hat {\vec\beta}=(X^TX)^{-1}X^T\vec y
\]</span> # 性质</p>
<h2 id="估计量的性质">估计量的性质</h2>
<ol type="1">
<li>线性性：估计量是<span
class="math inline">\(y\)</span>的线性函数；</li>
<li>无偏性：估计量是真实值的无偏估计；</li>
<li>有效性：方差最小，即</li>
</ol>
<p><span class="math display">\[
\begin{aligned}
{Var(\hat{\vec \beta})}={Var((X^TX)^{-1}X^T\vec y)}\\
=Var((X^TX)^{-1}X^T(X\vec \beta+\epsilon))
\end{aligned}
\]</span></p>
<p>因<span class="math inline">\(\vec\beta\)</span>为常数，因此 <span
class="math display">\[
\begin{aligned}
=&amp;Var((X^TX)^{-1}X^T\epsilon)\\
=&amp;(X^TX)^{-1}X^TVar(\epsilon)X(X^TX)^{-1}
\end{aligned}
\]</span> 当假定球星扰动项时，即<span
class="math inline">\(Var(\epsilon)=\sigma^2I\)</span>： <span
class="math display">\[
\begin{aligned}
=&amp;(X^TX)^{-1}X^T\sigma^2 IX(X^TX)^{-1}\\
=&amp;(X^TX)^{-1}\sigma^2
\end{aligned}
\]</span> <span
class="math inline">\(\sigma^2\)</span>常用扰动项方差，常用误差均方<span
class="math inline">\(s^2\)</span>来估计。因为<span
class="math inline">\(e_i\)</span>的均值为0，因此<span
class="math inline">\(\sigma^2\)</span>的无偏估计为： <span
class="math display">\[
\hat \sigma^2=\vec e^T\vec e/(n-p-1)
\]</span></p>
<ol start="4" type="1">
<li>当残差服从正态分布时，估计量也将服从正态分布。</li>
</ol>
<h2 id="ols的正交性">OLS的正交性</h2>
<p>残差向量<span class="math inline">\(\vec e\)</span>与拟合值向量<span
class="math inline">\(\hat {\vec y}\)</span>，常数1向量<span
class="math inline">\(\vec 1\)</span>以及自变量向量<span
class="math inline">\(\vec x\)</span>正交，即内积为0。</p>
<h2 id="平方和分解">平方和分解</h2>
<p>有常数项时，总离差平方和可以分解为模型的回归平方和和残差平方和：
<span class="math display">\[
\begin{aligned}
&amp;\sum_{i=1}^n(y_i-\bar y)^2\\
&amp;=\sum_{i=1}^n(y_i-\hat y_i+\hat y_i-\bar y)^2\\
&amp;=\sum_{i=1}^n[(y_i-\hat y_i)^2+2(y_i-\hat y_i)(\hat y_i-\bar
y)+(\hat y_i-\bar y)^2]\\
&amp;=\sum_{i=1}^n[e_i^2+2e_i(\hat y_i-\bar y)+(\hat y_i-\bar y)^2]\\
&amp;=\sum_{i=1}^ne_i^2+\sum_{i=1}^n(\hat y_i-\bar
y)^2+2\sum_{i=1}^ne_i\hat y_i-2\bar y\sum_{i=1}^ne_i
\end{aligned}
\]</span> 由于<span class="math inline">\(\vec e\)</span>与<span
class="math inline">\(\vec x,\vec y,\vec 1\)</span>均正交，因此<span
class="math inline">\(\sum_{i=1}^ne_i\hat
y_i=0,\sum_{i=1}^ne_i=0\)</span>，因此有： <span class="math display">\[
\sum_{i=1}^n(y_i-\bar y)^2=\sum_{i=1}^ne_i^2+\sum_{i=1}^n(\hat y_i-\bar
y)^2
\]</span>
<strong>这个分解公式只有回归方程中有常数项才成立，</strong>因为此时才能保证<span
class="math inline">\(\sum_{i=1}^ne_i=0\)</span>成立。</p>
<h2 id="遗漏变量与偏回归">遗漏变量与偏回归</h2>
<p>将设计矩阵<span class="math inline">\(X\)</span>分为两部分，即 <span
class="math display">\[
X=\begin{bmatrix}
X_1&amp;X_2
\end{bmatrix}
\]</span> 同时也将回归系数也分为两部分： <span class="math display">\[
\vec \beta=\begin{bmatrix}
\vec \beta_1\\
\vec \beta_2
\end{bmatrix}
\]</span> 这样线性回归模型的正规方程<span class="math inline">\(X^TX\vec
\beta=X^T\vec y\)</span>可以写为： <span class="math display">\[
\begin{bmatrix}
X_1^T\\
X_2^T
\end{bmatrix}\begin{bmatrix}
X_1&amp;X_2
\end{bmatrix}\begin{bmatrix}
\vec \beta_1\\
\vec \beta_s
\end{bmatrix}=\begin{bmatrix}
X_1^T\\
X_2^T
\end{bmatrix}\vec y\\
\begin{bmatrix}
X_1^TX_1&amp;X_1^TX_2\\
X_2^TX_1&amp;X_2^TX_2
\end{bmatrix}\begin{bmatrix}
\vec \beta_1\\
\vec \beta_2
\end{bmatrix}=\begin{bmatrix}
X_1^T\vec y\\
X_2^T\vec y
\end{bmatrix}\\
\begin{bmatrix}
X_1^TX_1\vec \beta_1+X_1^TX_2\vec \beta_2\\
X_2^TX_1\vec \beta_1+X_2^TX_2\vec \beta_2
\end{bmatrix}=\begin{bmatrix}
X_1^T\vec y\\
X_2^T\vec y
\end{bmatrix}
\]</span> 解第一个方程可得： <span class="math display">\[
\vec \beta_1=(X_1^TX_1)^{-1}X_1^T\vec y-(X_1^TX_1)^{-1}X_1^TX_2\vec
\beta_2
\]</span></p>
<p>分析这个式子可以发现两点：</p>
<ul>
<li>第一项<span class="math inline">\((X_1^TX_1)^{-1}X_1^T\vec
y\)</span>实际是以<span
class="math inline">\(X_1\)</span>为设计矩阵，<span
class="math inline">\(\vec
y\)</span>为响应变量的模型的回归系数，我们令这个系数估计值为<span
class="math inline">\(\vec \beta_{\vec y|X_1}\)</span>；</li>
<li>第二项前半部分<span
class="math inline">\((X_1^TX_1)^{-1}X_1^TX_2\)</span>实际上是以<span
class="math inline">\(X_1\)</span>为设计矩阵，<span
class="math inline">\(X_2\)</span>为响应变量的模型的回归系数，我们令这个系数的估计值为<span
class="math inline">\(\vec \beta_{X_2|X_1}\)</span>；</li>
</ul>
<p>因此上面的式子可以转换为： <span class="math display">\[
\vec \beta_{\vec y|X_1}=\vec\beta_1+\vec \beta_{X_2|X_1}\vec \beta_2
\]</span> 这就得到了当遗漏设计阵<span
class="math inline">\(X_2\)</span>时，参数估计值的表达式。分析这个表达式容易知道，<strong>当<span
class="math inline">\(\vec y\)</span>对<span
class="math inline">\(X_2\)</span>的回归系数<span
class="math inline">\(\vec \beta_2\)</span>，与<span
class="math inline">\(X_2\)</span>对<span
class="math inline">\(X_1\)</span>回归系数<span
class="math inline">\(\vec \beta_{X_2|X_1}\)</span>同号时，遗漏<span
class="math inline">\(X_2\)</span>对应的变量时将导致高估<span
class="math inline">\(X_1\)</span>对应的参数，反之则会低估，另外当<span
class="math inline">\(X_2\)</span>对应的变量与<span
class="math inline">\(X_1\)</span>对应的变量无关时，<span
class="math inline">\(\vec
\beta_{X_2|X_1}=0\)</span>，此时不会高估也不会低估。也就是说，如果因变量对遗漏变量的回归系数与遗漏变量对模型中自变量的回归系数同号，则会高估模型中自变量的回归系数。</strong></p>
<p>将第一个方程解的<span class="math inline">\(\vec
\beta_1\)</span>带入第二个方程可得： <span class="math display">\[
\begin{aligned}
X_2^TX_1[(X_1^TX_1)^{-1}X_1^T\vec y-(X_1^TX_1)^{-1}X_1^TX_2\vec
\beta_2]+X_2^TX_2\vec \beta_2=X_2^T\vec y\\
X_2^TX_1(X_1^TX_1)^{-1}X_1^T\vec y-X_2^TX_1(X_1^TX_1)^{-1}X_1^TX_2\vec
\beta_2+X_2^TX_2\vec \beta_2=X_2^T\vec y\\
X_2^T(I-X_1(X_1^TX_1)^{-1}X_1^T)X_2\vec
\beta_2=X_2^T(I-X_1(X_1^TX_1)^{-1}X_1^T)\vec y
\end{aligned}
\]</span> 令<span
class="math inline">\(H_1=X_1(X_1^TX_1)^{-1}X_1^T,M_1=I-H_1\)</span>，因此有：
<span class="math display">\[
\vec \beta_2=[X_2^TM_1X_2]^{-1}X_2^TM_1\vec y
\]</span> 容易发现<span
class="math inline">\(M_1\)</span>是一个幂等矩阵，即<span
class="math inline">\(M_1=M_1^TM_1\)</span>，因此有： <span
class="math display">\[
\vec \beta_2=[X_2^TM_1^TM_1X_2]^{-1}X_2^TM_1^TM_1\vec y\\
\]</span> <span class="math inline">\(M_1\vec y\)</span>实际上是以<span
class="math inline">\(\vec y\)</span>为因变量，以<span
class="math inline">\(X_1\)</span>为设计矩阵建立模型的残差向量<span
class="math inline">\(\vec e_{\vec y|X_1}\)</span>，<span
class="math inline">\(M_1X_2\)</span>实际上是以<span
class="math inline">\(X_2\)</span>为因变量以<span
class="math inline">\(X_1\)</span>为设计阵建立模型的残差向量<span
class="math inline">\(\vec e_{X_2|X_1}\)</span>，因此进一步有： <span
class="math display">\[
\vec \beta_2=[\vec e_{X_2|X_1}^T\vec e_{X_2|X_1}]^{-1}\vec
e_{X_2|X_1}^T\vec e_{\vec y|X_1}
\]</span> <strong>这表明<span class="math inline">\(\vec
\beta_2\)</span>实际上是残差<span class="math inline">\(\vec e_{\vec
y|X_1}\)</span>对<span class="math inline">\(\vec
e_{X_2|X_1}\)</span>的回归系数。</strong></p>
<p>上面式子表明：<strong>如有三个变量<span
class="math inline">\(x,y,z\)</span>，<span
class="math inline">\(y\)</span>对<span
class="math inline">\(x\)</span>回归的偏回归系数实际上等于从<span
class="math inline">\(y\)</span>中滤去<span
class="math inline">\(z\)</span>的影响（<span
class="math inline">\(y\)</span>对<span
class="math inline">\(z\)</span>做回归）得到残差，对从<span
class="math inline">\(x\)</span>中滤去<span
class="math inline">\(z\)</span>的影响（<span
class="math inline">\(x\)</span>对<span
class="math inline">\(z\)</span>做回归）得到残差做回归时的回归系数。</strong></p>
<h1 id="假设检验">假设检验</h1>
<h2 id="f检验">F检验</h2>
<p>常见的<span class="math inline">\(H_0\)</span>假设为： <span
class="math display">\[
H_0:\beta_1=\beta_2=...=\beta_p=0
\]</span> 利用总离差平方和分解公式： <span class="math display">\[
\begin{aligned}
\sum_{i=1}^n(y_i-\bar y)^2&amp;=\sum_{i=1}^n(y_i-\hat
y_i)^2+\sum_{i=1}^n(\hat y_i-\bar y)^2\\
=SSE+SSR
\end{aligned}
\]</span> 可以证明，下面的统计量服从<span
class="math inline">\(F\)</span>分布： <span class="math display">\[
F=\frac{SSR/p}{SSE/(n-p-1)}
\]</span></p>
<h2 id="t检验">t检验</h2>
<p>主要检验单个系数，例如<span class="math inline">\(\beta_j,\quad
j=1,2,...p\)</span>，如： <span class="math display">\[
H_0:\beta_j=0
\]</span> 从<span class="math inline">\(\hat{\vec
\beta}\)</span>的协方差<span
class="math inline">\(\sigma^2(X^TX)^{-1}\)</span>中提取<span
class="math inline">\(\hat\beta_{j}\)</span>的方差，利用统计量： <span
class="math display">\[
t_j=\frac{\hat\beta_j}{\hat\sigma\sqrt{c_{jj}}}
\]</span> <span class="math inline">\(c_{jj}\)</span>为<span
class="math inline">\((X^TX)^{-1}\)</span>中第<span
class="math inline">\(j\)</span>行<span
class="math inline">\(j\)</span>列元素。</p>
<h2 id="偏决定系数与偏f检验">偏决定系数与偏F检验</h2>
<p>偏决定系数测量的是在回归方程中包含若干个自变量时，再引入某一个新的自变量时残差平方和的减少量。例如，记<span
class="math inline">\(SSE(x_2)\)</span>是模型中只有自变量<span
class="math inline">\(x_2\)</span>时的残差平方和，<span
class="math inline">\(SSE(x_1,x_2)\)</span>是模型中有自变量<span
class="math inline">\(x_1,x_2\)</span>时的残差平方和，因此在模型中已有<span
class="math inline">\(x_2\)</span>时，<span
class="math inline">\(x_1\)</span>和<span
class="math inline">\(y\)</span>的偏决定系数为： <span
class="math display">\[
\frac{SSE(x_2)-SSE(x_1,x_2)}{SSE(x_2)}
\]</span> 反之亦然。</p>
<p>不失一般性，令<span class="math inline">\(y\)</span>对自变量<span
class="math inline">\(x_1,x_2,...,x_p\)</span>的残差平方和为<span
class="math inline">\(SSE\)</span>，回归平方和为<span
class="math inline">\(SSR\)</span>，在模型中剔除自变量<span
class="math inline">\(x_j\)</span>时候，用<span
class="math inline">\(y\)</span>对剩下的<span
class="math inline">\(p-1\)</span>个自变量做回归，记所得模型的残差平方和为<span
class="math inline">\(SSE(j)\)</span>，回归平方和为<span
class="math inline">\(SSR(j)\)</span>，则自变量<span
class="math inline">\(x_j\)</span>对回归的贡献为<span
class="math inline">\(\Delta=SSR-SSR(j)\)</span>，称为<span
class="math inline">\(x_j\)</span>的偏回归平方和。可构造如下偏F统计量：
<span class="math display">\[
F_j=\frac{\Delta/1}{SSE/(n-p-1)}
\]</span> 此统计量实际上与<span
class="math inline">\(t\)</span>检验是等价的。</p>
<h2 id="偏相关系数">偏相关系数</h2>
<p>当其他变量固定之后，给定的任意两个变量之间的相关系数，叫偏相关系数。偏相关系数等于偏决定系数的平方根，也等于其他变量对两个变量做回归，得到两组残差值，计算两组残差之间的相关系数。</p>
<h1 id="拟合优度">拟合优度</h1>
<h2 id="决定系数r2">决定系数<span
class="math inline">\(R^2\)</span></h2>
<p>用决定系数<span class="math inline">\(R^2\)</span>来度量拟合优度，即
<span class="math display">\[
R^2=\frac{\sum_{i=1}^n(\hat y_i-\bar y)^2}{\sum_{i=1}^n(y_i-\bar
y)^2}=1-\frac{\sum_{i=1}^ne_i^2}{\sum_{i=1}^n(y_i-\bar y)^2}
\]</span> 在有常数项时，<span
class="math inline">\(R^2\)</span>等于拟合值<span
class="math inline">\(\hat y_i\)</span>和<span
class="math inline">\(y_i\)</span>之间相关系数的平方。决定系数的缺点在于：即便增加无效的预测变量也能使得决定系数增加，因此又有了调整决定系数。</p>
<h2 id="调整决定系数r_a2">调整决定系数<span
class="math inline">\(R_a^2\)</span></h2>
<p>调整<span
class="math inline">\(R^2_a\)</span>，主要考虑了解释变量个数： <span
class="math display">\[
R^2_a=1-\frac{\sum_{i=1}^ne_i^2/(n-p-1)}{\sum_{i=1}^n(y_i-\bar
y)^2/(n-1)}
\]</span> 分子自由度为<span
class="math inline">\(n-p\)</span>是因为有<span
class="math inline">\(p+1\)</span>个正规方程。</p>
<h2 id="复相关系数">复相关系数</h2>
<p>为决定系数<span
class="math inline">\(R^2\)</span>的开方，实际上等于拟合值<span
class="math inline">\(\hat y_i\)</span>和<span
class="math inline">\(y_i\)</span>之间相关系数。</p>
<h2 id="无常数项的回归">无常数项的回归</h2>
<p>无常数项回归即消去<span class="math inline">\(\beta_0\)</span>：
<span class="math display">\[
\hat y=\beta_1x_1+\beta_2x_2+...+\beta_px_p+\epsilon
\]</span> 无常数项回归必然经过原点。此时残差向量<span
class="math inline">\(\vec e\)</span>与拟合值向量<span
class="math inline">\(\hat {\vec y}\)</span>，自变量向量<span
class="math inline">\(\vec
x\)</span>正交，即内积为0，<strong>但是不一定与常数1向量<span
class="math inline">\(\vec 1\)</span>正交。</strong></p>
<h1 id="模型选择">模型选择</h1>
<h2 id="全子集回归">全子集回归</h2>
<p>设共有<span
class="math inline">\(p\)</span>个自变量（这里的自变量不包括常数项），每个自变量都有进入或者不进入模型的可能，那么<span
class="math inline">\(y\)</span>关于这些自变量的所有可能的回归模型有<span
class="math inline">\(2^p\)</span>个。当所有自变量都不进入模型，此时模型中只有一个常数项，如果不考虑这个模型，则可能的回归模型有<span
class="math inline">\(2^p-1\)</span>个。</p>
<p>从这<span
class="math inline">\(2^p\)</span>个模型中选择一个最优的模型，就称为全子集回归。如何选择呢，这就需要一个标准，常见的标准有：</p>
<h3 id="调整r2_a">调整<span class="math inline">\(R^2_a\)</span></h3>
<p><span class="math display">\[
R^2_a=1-\frac{\sum_{i=1}^ne_i^2/(n-p-1)}{\sum_{i=1}^n(y_i-\bar
y)^2/(n-1)}
\]</span></p>
<p><span class="math inline">\(R^2_a\)</span>越大模型越好。</p>
<h3 id="hat-sigma2"><span class="math inline">\(\hat
\sigma^2\)</span></h3>
<p><span class="math display">\[
\hat \sigma^2=\frac{\vec e^T\vec e}{n-p-1}
\]</span></p>
<p><span class="math inline">\(\hat \sigma^2\)</span>是<span
class="math inline">\(\sigma^2\)</span>的无偏估计，实际上<span
class="math inline">\(\hat \sigma^2\)</span>和调整<span
class="math inline">\(R_a^2\)</span>是等价的。<span
class="math inline">\(\hat \sigma^2\)</span>越小，模型越好。</p>
<h3 id="赤池信息准则aic">赤池信息准则AIC</h3>
<p>AIC等于-2倍对数似然函数值加上参数个数的2倍： <span
class="math display">\[
AIC=-2lnL+2p
\]</span> AIC越小，模型越好。</p>
<h3 id="c_p统计量"><span class="math inline">\(C_p\)</span>统计量</h3>
<p><span class="math display">\[
C_p=\frac{SSE}{\hat \sigma^2}+2p-n
\]</span></p>
<p><span class="math inline">\(SSE\)</span>是当前模型的误差平方和，<span
class="math inline">\(\hat\sigma^2\)</span>是所有自变量均纳入时模型的均方误差。<span
class="math inline">\(C_p\)</span>统计量越小模型越好。</p>
<h2 id="逐步回归">逐步回归</h2>
<h3 id="前进法">前进法</h3>
<p>前进法的思想是变量由少到多，每次增加一个，直到没有可引入的变量为止。具体做法是先将<span
class="math inline">\(p\)</span>个自变量，分别对因变量<span
class="math inline">\(y\)</span>建立<span
class="math inline">\(p\)</span>个一元线性回归方程，选择<span
class="math inline">\(p\)</span>值显著且最小（也即偏<span
class="math inline">\(F\)</span>值最大）的那个自变量先进入模型，设为<span
class="math inline">\(x_i\)</span>。然后在已经引入<span
class="math inline">\(x_i\)</span>的基础上，依次引入其他<span
class="math inline">\(p-1\)</span>个自变量，建立<span
class="math inline">\(p-1\)</span>个包含两个自变量的回归模型，从中选择<span
class="math inline">\(p\)</span>值最小且显著（偏<span
class="math inline">\(F\)</span>值最大）的那个变量进入方程，依次类推，直到模型外变量均不显著为止。</p>
<p>前进法有明显的不足：某个自变量刚引入时可能是显著的，但是当引入其他自变量后，它变得不显著了，但是前进法不会将其进行剔除。即一旦引入方程的自变量，将是“终身制“。即前进法只能”进“。</p>
<h3 id="后退法">后退法</h3>
<p>后退法与前进法相反，首先用全部<span
class="math inline">\(p\)</span>个自变量建立一个回归方程，然后在这<span
class="math inline">\(p\)</span>个变量中选择一个最不重要的变量，将它从方程中剔除。选择的标准是<span
class="math inline">\(p\)</span>值最大（对应的偏<span
class="math inline">\(F\)</span>值最小）的自变量。借着从剩下的<span
class="math inline">\(p-1\)</span>个自变量重新建立回归方程，重复这一过程，直到没有可剔除的自变量为止。</p>
<p>后退法也有不足：一是一开始把所有自变量引入方程，导致计算量很大。再就是一旦某个自变量被剔除，就再也没有机会进入回归方程。即后退法只能”出“。</p>
<h3 id="逐步法">逐步法</h3>
<p>逐步法的基本思想是有进有出。具体做法是将变量一个一个引入，当每引入一个自变量后，会对已入选的变量逐个检验，如果不再显著，会将其剔除。同样，剔除一个变量之后，会再次对已入选的变量逐个检验，如有有变量不再显著，会将其剔除。检验显著性仍然使用偏F检验或对应的<span
class="math inline">\(p\)</span>值。这个过程反复进行，直到既无显著自变量进入方程，也无不显著自变量从回归方程中剔除。</p>
<p>逐步法需要注意的是：引入自变量的显著性水平<span
class="math inline">\(\alpha_{in}\)</span>需要小于剔除自变量的显著性水平<span
class="math inline">\(\alpha_{out}\)</span>，否则可能出现”死循环“。</p>
<h1 id="回归诊断">回归诊断</h1>
<h2 id="模型适用条件判断">模型适用条件判断</h2>
<h3 id="异方差检验方差齐性">异方差（检验方差齐性）</h3>
<h4 id="图示法">图示法</h4>
<p>画标准化残差或学生化残差与拟合值<span class="math inline">\(\hat
y_i\)</span>的散点图，或者与某一个解释变量<span
class="math inline">\(x_{ik}\)</span>的散点图。如果等方差，残差应该不会随<span
class="math inline">\(\hat y_i\)</span>或<span
class="math inline">\(x_{ik}\)</span>发生变化。</p>
<h4 id="假设检验法">假设检验法</h4>
<h3 id="自相关检验独立性">自相关（检验独立性）</h3>
<h4 id="图示法-1">图示法</h4>
<p>画残差与其滞后之间的散点图，或者对残差绘制自相关图或计算偏自相关系数。</p>
<h4 id="假设检验法-1">假设检验法</h4>
<h3 id="正态性">正态性</h3>
<h4 id="图示法-2">图示法</h4>
<p>考察普通残差<span
class="math inline">\(e_i\)</span>是否服从正态分布，可以画直方图、qq图等。</p>
<h4 id="假设检验法-2">假设检验法</h4>
<p>使用W检验等。</p>
<h2 id="异常点与强影响点判断">异常点与强影响点判断</h2>
<p>异常点主要包括杠杆点和离群点。X空间异常的点称为杠杆点，Y空间异常的点称为离群点。<strong>强影响点</strong>(influential
points)
<strong>是指对模型有较大影响的点，模型中包含该点与不包含该点会使求得的回归系数相差很大</strong>。如果某点既是离群点又是高杠杆点，则该点很有可能是强影响点。</p>
<h3 id="杠杆点">杠杆点</h3>
<p>线性回归的预测值为： <span class="math display">\[
\begin{aligned}
\hat {\vec y}=X\hat{\vec \beta}\\
=X(X^TX)^{-1}X^T\vec y\\
=H\vec y
\end{aligned}
\]</span> 矩阵<span
class="math inline">\(H=X(X^TX)^{-1}X^T\)</span>是一个投影矩阵，也称为帽子矩阵（给<span
class="math inline">\(\vec y\)</span>戴了一顶帽子），作用是将<span
class="math inline">\(\vec y\)</span>投影到<span
class="math inline">\(X\)</span>的列空间。帽子矩阵是一个<span
class="math inline">\(n*n\)</span>的对称矩阵，矩阵的主对角线上的元素（共有n个）称为帽子统计量。帽子矩阵的迹，也即帽子统计量的和为<span
class="math inline">\(p+1\)</span>，证明其实很简单，因为<span
class="math inline">\(tr(AB)=tr(BA)\)</span>，因此： <span
class="math display">\[
\begin{aligned}
tr(X(X^TX)^{-1}X^T)&amp;=tr(X^TX(X^TX)^{-1})\\
&amp;=tr(I_{p+1})\\
&amp;=p+1
\end{aligned}
\]</span> 因此帽子统计量的平均值等于 <span class="math display">\[
\frac{p+1}{n}
\]</span>
如果某条观测的帽子统计量是帽子均值的2或3倍，即可视为高杠杆点。</p>
<h3 id="离群点">离群点</h3>
<p>离群点主要通过残差<span
class="math inline">\(e_i,i=1,2,...,n\)</span>来判断。定义残差向量<span
class="math inline">\(\vec e=[e_1,e_2,...,e_i,...,e_n]^T\)</span>，则：
<span class="math display">\[
\begin{aligned}
\vec e=\vec y-\hat{\vec y}\\
=\vec y-H\vec y\\
=(I-H)\vec y
\end{aligned}
\]</span> 残差向量的协方差矩阵为： <span class="math display">\[
\begin{aligned}
Cov(\vec e,\vec e)&amp;=Cov((I-H)\vec y,(I-H)\vec y)\\
&amp;=(I-H)Cov(\vec y,\vec y)(I-H)^{T}\\
&amp;=(I-H)\sigma^2I(I-H)^T\\
&amp;=\sigma^2[II^T-IH^T-HI+HH^T]
\end{aligned}
\]</span> 因为<span
class="math inline">\(H\)</span>是对称矩阵，也是幂等矩阵，因此有： <span
class="math display">\[
Cov(\vec e,\vec e)=\sigma^2(I-H)
\]</span> 因此可知每个残差<span
class="math inline">\(e_i\)</span>的方差为<span
class="math inline">\((1-h_{ii})\sigma^2\)</span>，<span
class="math inline">\(h_{ii}\)</span>为第<span
class="math inline">\(i\)</span>条观测数据的帽子统计量。</p>
<p>根据上述结论，可以定义下面5类残差：</p>
<h4 id="未标准化残差">未标准化残差</h4>
<p>也称为原始残差，<span class="math inline">\(e_i=y_i-\hat
y_i\)</span>。由于原始残差的方差与帽子统计量有关，各不相同，为了便于进行比较，提出了标准化残差和学生化残差。</p>
<h4 id="标准化残差">标准化残差</h4>
<p>将未标准化残差进行标准化，即先减去均值（有截距项时，残差的均值为0），再除以标准差，即<span
class="math inline">\(\frac{e_i}{\hat
\sigma}\)</span>，标准化残差使得残差具备可比性，一般认为标准化残差绝对值大于3为离群点。标准化残差因为没有解决残差之间方差不等的问题，为此又有了学生化残差。</p>
<h4 id="学生化残差">学生化残差</h4>
<p>公式为<span class="math inline">\(\frac{e_i}{\hat
\sigma(1-h_{ii})}\)</span>，<span
class="math inline">\(h_{ii}\)</span>为第<span
class="math inline">\(i\)</span>条数据的帽子统计量。学生化残差相比较标准化残差进一步解决了方差不等问题，因此更适合做残差比较。不过当<span
class="math inline">\(y\)</span>本身存在异常时，会将回归线拉向自身，会减弱残差的判断作用，为此又有了剔除残差。</p>
<h4 id="剔除残差">剔除残差</h4>
<p>剔除该条记录后，拟合新的模型。新模型输入该条记录的自变量得到预测值，得到的未标准化残差。</p>
<h4 id="学生化剔除残差">学生化剔除残差</h4>
<p>剔除残差进行学生化转换。显然学生化剔除残差更能判断离群点。</p>
<p>如果学生化残差绝对值<span class="math inline">\(\ge
2\)</span>，可以认为是一个可疑点；如果学生化残差绝对值<span
class="math inline">\(\ge 3\)</span>，基本可以认定是一个离群点。</p>
<h3 id="强影响点">强影响点</h3>
<h4 id="cook距离">Cook距离</h4>
<p>将学生化残差与帽子统计量求积： <span class="math display">\[
\frac{e_i^2}{(1-h_{ii})^2\hat\sigma^2}\frac{h_{ii}}{\frac{p+1}{n}}=n\frac{e_i^2}{(1-h_{ii})^2\hat\sigma^2}\frac{h_{ii}}{p+1}
\]</span> <span
class="math inline">\(n\)</span>为所有观测所共有，忽略后即为Cook距离：
<span class="math display">\[
D_i=\frac{e_i^2}{(1-h_{ii})^2\hat\sigma^2}\frac{h_{ii}}{p+1}
\]</span>
直观上看Cook距离综合了y空间的离群点和x空间的杠杆点。Cook距离判断强影响点的标准比较复杂，一般认为Cook距离大于1，可以认为是强影响点。</p>
<h4 id="dfbeta">DfBeta</h4>
<p>剔除第<span
class="math inline">\(i\)</span>条记录后，拟合新的模型。比较剔除前和剔除后，<strong>某一个</strong>回归系数估计值的变化（即求差值），变化较大表示强影响点。</p>
<h4 id="标准化dfbeta">标准化DfBeta</h4>
<p>即对每条观测某一个回归系数的DfBeta进行标准化。</p>
<h4 id="dffit">DfFit</h4>
<p>剔除第<span
class="math inline">\(i\)</span>条记录后，拟合新的模型。比较剔除前和提出后，第<span
class="math inline">\(i\)</span>条记录拟合值的变化（即求差值），变化较大表示强影响点。</p>
<h4 id="标准化dffit">标准化DfFit</h4>
<p>即对每条观测的DfFit进行标准化。</p>
<h2 id="共线性判断">共线性判断</h2>
<p>在近似多重共线性情况下，OLS仍然是最佳线性无偏估计，即仍然是最小方差的估计，但是这并不意味着方差的绝对值很小，实际上，由于多重共线性，OLS估计量的方差会很大，使得系数估计很不稳定，数据矩阵X轻微变动就可能导致参数估计值发生巨大变化，甚至方向与预期相反，此时<span
class="math inline">\(t\)</span>检验也往往不会通过。</p>
<h3 id="何时需要处理共线性">何时需要处理共线性</h3>
<ul>
<li>如果模型只是用来进行预测，则不必关注多重共线性；</li>
<li>如果关注具体的回归系数，但不关心的自变量出现了严重多重共线性，也不必关注；</li>
<li>如果关注具体的回归系数，且关心的自变量存在严重多重共线性，则需要进行处理。</li>
</ul>
<h3 id="如何判断共线性">如何判断共线性</h3>
<h4 id="方差膨胀因子">方差膨胀因子</h4>
<p>将第<span class="math inline">\(k\)</span>个自变量<span
class="math inline">\(x_k\)</span>对其余自变量做一个线性回归，可以计算出该回归方程的决定系数<span
class="math inline">\(R_k^2\)</span>，可以证明<span
class="math inline">\(x_k\)</span>的系数<span class="math inline">\(\hat
\beta_k\)</span>的方差为： <span class="math display">\[
Var(\hat \beta_k)=\frac{\sigma^2}{(1-R_k^2)S_k}
\]</span> <span class="math inline">\(S_k\)</span>为<span
class="math inline">\(x_k\)</span>的离差平方和，<span
class="math inline">\(\sigma^2\)</span>为扰动项的方差。<span
class="math inline">\(x_k\)</span>的方差膨胀因子VIF定义为： <span
class="math display">\[
VIF_k=\frac{1}{1-R_k^2}
\]</span> 因此 <span class="math display">\[
Var(\hat \beta_k)=VIF_k\frac{\sigma^2}{S_k}
\]</span> 从这个式子可以看出来方差膨胀因子的含义，因为<span
class="math inline">\(\sigma^2/S_k\)</span>已经固定，<span
class="math inline">\(VIF_k\)</span>实际上就是使<span
class="math inline">\(\hat \beta_k\)</span>的方差变为<span
class="math inline">\(\frac{\sigma^2}{S_k}\)</span>的倍数。另外VIF的分母部分，也就是<span
class="math inline">\(1-R_k^2\)</span>称为容忍度。</p>
<p>计算每个自变量的方差膨胀因子，一般认为方差膨胀因子大于10，认为该自变量与其他自变量之间存在严重的多重共线性。</p>
<h4 id="特征值">特征值</h4>
<p>数据矩阵<span class="math inline">\(X\)</span>标准化之后，对<span
class="math inline">\(X^TX\)</span>做特征值分解，求得特征值和特征向量。如果有<span
class="math inline">\(r,r\le
p\)</span>个特征值近似为0，以其对应的特征向量为系数的<span
class="math inline">\(X\)</span>的线性组合将近似为0，这意味着<span
class="math inline">\(X\)</span>中有<span
class="math inline">\(r\)</span>个变量与其他变量有严重共线关系。</p>
<h4 id="条件数">条件数</h4>
<p><span class="math inline">\(X^TX\)</span>（<span
class="math inline">\(X\)</span>提前标准化）为<span
class="math inline">\(p*p\)</span>的方阵，有<span
class="math inline">\(p\)</span>个特征值，其中最大特征值除以最小特征值再开方定义为矩阵<span
class="math inline">\(X^TX\)</span>的条件数，条件数较大（例如大于30）时认为有严重的多重共线性。</p>
<h3 id="如何处理多重共线性">如何处理多重共线性</h3>
<ul>
<li>增大样本容量</li>
<li>剔除严重共线性的变量</li>
<li><strong>将严重共线性的变量进行标准化</strong></li>
<li>修改模型的设定</li>
<li>使用其他模型：主成分回归、岭回归等。</li>
</ul>
<p>注意：多项式回归中容易出现多重共线性，例如<span
class="math inline">\(x\)</span>和<span
class="math inline">\(x^2\)</span>的相关性就比较强，此时解决办法是将<span
class="math inline">\(x\)</span>做标准化，并引入标准化变量的平方。</p>
]]></content>
      <categories>
        <category>数理统计</category>
      </categories>
      <tags>
        <tag>回归</tag>
      </tags>
  </entry>
  <entry>
    <title>广义线性模型总论</title>
    <url>/2023/03/08/13.%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/</url>
    <content><![CDATA[<p>​</p>
<span id="more"></span>
<h1 id="模型">模型</h1>
<p>广义线性模型有三个主要组成部分：</p>
<ol type="1">
<li>给定<span
class="math inline">\(X_1=x_1,...,X_p=x_p\)</span>后，随机变量<span
class="math inline">\(y\)</span>的条件分布源于指数分布族；</li>
</ol>
<p><span class="math display">\[
\begin{aligned}
y|(X_1=x_2,X_2=x_2,...,X_p=x_p)\sim exp(\theta,\phi,a,b,c)
\end{aligned}
\]</span></p>
<p><span
class="math inline">\(exp(\theta,\phi,a,b,c)\)</span>表示指数分布族，后面我们会详细介绍指数分布族。</p>
<ol start="2" type="1">
<li>给定<span
class="math inline">\(X_1=x_1,...,X_p=x_p\)</span>后，随机变量<span
class="math inline">\(y\)</span>的条件均值用连接函数<span
class="math inline">\(g\)</span>进行转换，转换后为<span
class="math inline">\(\eta\)</span>，要求连接函数<span
class="math inline">\(g\)</span>单调可微；</li>
</ol>
<p><span class="math display">\[
\begin{aligned}
g(E(y|X_1=x_1,...,X_p=x_p))=\eta
\end{aligned}
\]</span></p>
<ol start="3" type="1">
<li>自变量对<span class="math inline">\(\eta\)</span>进行回归；</li>
</ol>
<p><span class="math display">\[
\begin{aligned}
\eta = \beta_0+\beta_1x_1+...+\beta_px_p=\vec x^T\vec \beta\\
\vec x\in R^{p+1},\vec \beta\in R^{p+1}
\end{aligned}
\]</span></p>
<p>因此，广义线性模型的基本形式其实为： <span class="math display">\[
\begin{aligned}
g(y|(X_1=x_1,...,X_p=x_p)=\vec x^T\vec \beta
\end{aligned}
\]</span> 或者可以写为： <span class="math display">\[
\begin{aligned}
y|(X_1=x_1,...,X_p=x_p)=g^{-1}(\vec x^T\vec \beta)
\end{aligned}
\]</span> 这里的<span
class="math inline">\(g^{-1}\)</span>为连接函数<span
class="math inline">\(g\)</span>的反函数，实际上可以看作是神经网络中的激活函数，下面会详细讨论如何选择连接函数或激活函数。</p>
<h1 id="指数分布族">指数分布族</h1>
<p>上面提到广义线性模型中随机变量<span
class="math inline">\(y\)</span>的条件分布来源于指数分布族，下面我们给出指数分布族的概率密度函数：
<span class="math display">\[
\begin{aligned}
f(y;\theta,\phi)=exp(\frac{y\theta-b(\theta)}{a(\phi)}+c(y,\phi))
\end{aligned}
\]</span> 这里的<span
class="math inline">\(a(.),b(.),c(.)\)</span>都是特定函数，随不同的分布有不同的形式。具体而言，<span
class="math inline">\(\theta\)</span>为自然参数，它和分布的均值<span
class="math inline">\(\mu\)</span>有密切关系，往往是我们感兴趣的参数。<span
class="math inline">\(\phi\)</span>为分散参数，它和分布的方差有关。<span
class="math inline">\(a(\phi)\)</span>是<span
class="math inline">\(\phi\)</span>的函数，称为分散函数，该函数的函数形式并不重要，本质上它就是代表了分散参数<span
class="math inline">\(\phi\)</span>，如果每个样本有相同的方差，常会直接令<span
class="math inline">\(a(\phi)=\phi\)</span>（如线性回归模型等），如果每个样本有不同的方差，则往往定义<span
class="math inline">\(a(\phi)\)</span>为： <span class="math display">\[
a(\phi)_i=\frac{\phi}{w_i}
\]</span> 这里的<span
class="math inline">\(w_i\)</span>为每个样本的权重，称为先验权重，当所有样本有相同的权重时，就等价于<span
class="math inline">\(a(\phi)=\phi\)</span>。<span
class="math inline">\(\phi\)</span>则是每个样本都是相同的。</p>
<p>对于广义线性模型，其期望<span
class="math inline">\(\mu\)</span>和方差<span
class="math inline">\(\sigma^2\)</span>与<span
class="math inline">\(b(\theta)\)</span>和<span
class="math inline">\(a(\phi)\)</span>有关： <span
class="math display">\[
\begin{aligned}
\mu = E(y)=b\prime(\theta)\\
\sigma^2 = Var(y)=b\prime\prime(\theta)a(\phi)
\end{aligned}
\]</span></p>
<p>可以看到自然参数<span
class="math inline">\(\theta\)</span>与期望<span
class="math inline">\(\mu\)</span>密切相关，同时也可以看到，方差<span
class="math inline">\(\sigma^2\)</span>由两部分构成：一部分是分散函数<span
class="math inline">\(a(\phi)\)</span>，另一部分是<span
class="math inline">\(b\prime\prime(\theta)\)</span>。</p>
<p><span class="math inline">\(b\prime\prime(\theta)\)</span>是<span
class="math inline">\(b(\theta)\)</span>关于<span
class="math inline">\(\theta\)</span>的二阶导数，它要么是一个常数，要么是一个关于参数<span
class="math inline">\(\theta\)</span>的函数。对于后者，因为<span
class="math inline">\(\theta\)</span>与<span
class="math inline">\(\mu\)</span>相关，这提示<span
class="math display">\[b\prime\prime(\theta)\]</span>可以写成<span
class="math inline">\(\mu\)</span>的函数，定义这个函数为方差函数<span
class="math inline">\(v(\mu)\)</span>： <span class="math display">\[
b\prime\prime(\theta)=v(\mu)
\]</span> 由于<span
class="math inline">\(b\prime\prime(\theta)\)</span>有两种情况，因此：</p>
<ul>
<li>当<span
class="math inline">\(b\prime\prime(\theta)\)</span>为常数，方差函数<span
class="math inline">\(v(\mu)\)</span>为常数，此时分布的方差<span
class="math inline">\(\sigma^2=a(\phi)\)</span>，这意味着方差与均值无关。；</li>
<li>当<span
class="math inline">\(b\prime\prime(\theta)\)</span>为关于均值<span
class="math inline">\(\mu\)</span>的函数，此时分布的方差<span
class="math inline">\(\sigma^2=a(\phi)v(\mu)\)</span>，这意味着方差与均值有关。</li>
</ul>
<p>后面会看到，在正态分布中<span
class="math inline">\(b\prime\prime(\theta)=1\)</span>，所以均值和方差是不相关的，但是对于其他分布则不成立，也就是说正态分布是一个特例。</p>
<p>几种常见分布的方差函数</p>
<table>
<thead>
<tr class="header">
<th>分布</th>
<th>方差函数</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Gaussian</td>
<td>1</td>
</tr>
<tr class="even">
<td>Bernoulli</td>
<td><span class="math inline">\(\mu(1-\mu)\)</span></td>
</tr>
<tr class="odd">
<td>Binomial(k)</td>
<td><span class="math inline">\(\mu(1-\mu/k)\)</span></td>
</tr>
<tr class="even">
<td>Poisson</td>
<td><span class="math inline">\(\mu\)</span></td>
</tr>
<tr class="odd">
<td>Gamma</td>
<td><span class="math inline">\(\mu^2\)</span></td>
</tr>
<tr class="even">
<td>Inverse Gaussian</td>
<td><span class="math inline">\(\mu^3\)</span></td>
</tr>
<tr class="odd">
<td>Negative binomial(α)</td>
<td><span class="math inline">\(\mu+\alpha\mu^3\)</span></td>
</tr>
<tr class="even">
<td>Power(k)</td>
<td><span class="math inline">\(\mu^k\)</span></td>
</tr>
</tbody>
</table>
<p>下面介绍几种常用的源于指数分布族的分布。</p>
<h2 id="正态分布">正态分布</h2>
<p><span class="math display">\[
f(y)=\frac{1}{\sqrt{2\pi}\sigma}exp(-\frac{(y-\mu)^2}{2\sigma^2})\\
=exp(ln(\frac{1}{\sqrt{2\pi}\sigma})-\frac{y^2-2y\mu+\mu^2}{2\sigma^2})\\
=exp(\frac{y\mu-\frac{1}{2}\mu^2}{\sigma^2}+ln(\frac{1}{\sqrt{2\pi}\sigma})-\frac{y^2}{2\sigma^2})
\]</span></p>
<p>易知 <span class="math display">\[
\theta=\mu\\
b(\theta)=\frac{1}{2}\mu^2\\
\phi=\sigma^2\\
a(\phi)=\sigma^2\\
b\prime(\theta)=\mu\\
b\prime\prime(\theta)a(\phi)=\sigma^2
\]</span>
因此对于正态分布来说，均值和方差是不相关的。特别的，对于异方差模型。
<span class="math display">\[
a(\phi)_i=\sigma^2/w_i
\]</span> ## 伯努利分布</p>
<p><span class="math display">\[
f(y)=p^y(1-p)^{1-y}\\
=exp(ylnp+(1-y)ln(1-p))\\
=exp(yln(\frac{p}{1-p})+ln(1-p))
\]</span></p>
<p>易知： <span class="math display">\[
\theta=ln(\frac{p}{1-p})\\
b(\theta)=-ln(1-p)=ln(1+e^\theta)\\
a(\phi)=1\\
p=\frac{e^\theta}{1+e^\theta}\\
\mu=b\prime(\theta)=\frac{e^\theta}{1+e^\theta}=p\\
\sigma^2=b\prime\prime(\theta)a(\phi)=p(1-p)=\mu(1-\mu)
\]</span></p>
<p>因此，对于伯努利分布，分散参数为1，或者说没有分散参数。</p>
<h2 id="多项分布">多项分布</h2>
<h2 id="泊松分布">泊松分布</h2>
<p><span class="math display">\[
f(y)=\frac{\mu^yexp(-\mu)}{y!}\\
=exp(yln\mu-\mu-lny!)
\]</span></p>
<p>因此： <span class="math display">\[
\theta=ln\mu\\
b(\theta)=\mu=exp(\theta)\\
a(\phi)=1
\]</span></p>
<h1 id="连接函数选择">连接函数选择</h1>
<h2 id="规范连接函数">规范连接函数</h2>
<p>我们来考虑一下连接函数<span
class="math inline">\(g\)</span>的选择。理论上连接函数的选择可以有很多种，实际上为了方便，在广义线性模型中常常选择规范连接函数（canonical
link
function）。通常来说，我们习惯对一个分布中的均数进行建模，根据上面的推导，我们已经知道：
<span class="math display">\[
\mu=b\prime(\theta)=g^{-1}(\eta)
\]</span> 如果我们令： <span class="math display">\[
\theta=\eta=\vec x^T\vec \beta
\]</span> 那么我们可以推导出： <span class="math display">\[
b\prime(.)=g^{-1}(.)
\]</span> 或者： <span class="math display">\[
g(.)=(b\prime)^{-1}(.)
\]</span> 这意味着连接函数<span
class="math inline">\(g\)</span>可以选择<span
class="math inline">\(b\)</span>函数一阶导数的反函数，或者说激活函数<span
class="math inline">\(g^{-1}\)</span>可以选择<span
class="math inline">\(b\)</span>函数的一阶导数，这时候的连接函数<span
class="math inline">\(g\)</span>称为规范连接函数。</p>
<p>可以看到，如果<span
class="math inline">\(g\)</span>选择规范连接函数，<span
class="math inline">\(g\)</span>其实和<span
class="math inline">\(b\prime\)</span>函数互为反函数，互反函数有一个性质就是导数得乘积等于1，即：
<span class="math display">\[
g\prime(\mu)b\prime\prime(\theta)=1
\]</span> 这一性质在下面参数估计推导中会有运用。</p>
<p>在选择经典连接函数的情况下，广义线性模型可以用一行进行总结： <span
class="math display">\[
y|(X_1=x_2,X_2=x_2,...,X_p=x_p)\sim exp(\eta,\phi,a,b,c)
\]</span> 注意这里用<span class="math inline">\(\eta\)</span>替换为<span
class="math inline">\(\theta\)</span>。</p>
<h2 id="常用分布的连接函数">常用分布的连接函数</h2>
<h3 id="正态分布-1">正态分布</h3>
<p>根据以上结论可知对于正态分布，有： <span class="math display">\[
b\prime(\theta)=g(\mu)=\mu
\]</span> 此时的连接函数其实为恒等连接，即<span
class="math inline">\(g(\mu)=\mu\)</span>。</p>
<h3 id="伯努利分布">伯努利分布</h3>
<p>对于伯努利分布，有： <span class="math display">\[
\mu=b\prime(\theta)=\frac{e^\theta}{1+e^\theta}=\frac{1}{1+e^{-\eta}}=\frac{1}{1+e^{-X\vec
\beta}}\\
g(\mu)=ln(\frac{\mu}{1-\mu})=\eta=X\vec \beta
\]</span>
即连接函数为logit函数，激活函数为sigmoid函数，此时为logistic回归。</p>
<h3 id="多项分布-1">多项分布</h3>
<h3 id="泊松分布-1">泊松分布</h3>
<p>对于泊松分布： <span class="math display">\[
\mu=b\prime(\theta)=exp(\theta)=exp(\eta)=exp(X\vec \beta)\\
g(\mu)=ln(\mu)=\eta=X\vec \beta
\]</span> 即连接函数为log函数，激活函数为指数函数，此时为泊松回归。</p>
<h1 id="参数估计">参数估计</h1>
<h2 id="似然函数">似然函数</h2>
<p>由于指数分布族的存在，所有广义线性模型都可以在一个统一的框架下进行参数估计，参数估计方法仍然是我们熟悉的最大似然估计法。</p>
<p>考虑数据集<span class="math inline">\(\{\{\vec
x_i,y_i\}\}_{i=1}^n\)</span>，应用规范连接函数，我们有： <span
class="math display">\[
\begin{aligned}
y_i|(X_{1}=x_{i1},...X_{p}=x_{ip})&amp;\sim exp(\theta_i,\phi,a,b,c)\\
i=&amp;1,2,...,n
\end{aligned}
\]</span> 其中： <span class="math display">\[
\begin{aligned}
\theta_i =\eta_i= \vec x_i^T\vec \beta,&amp; \quad \vec \beta\in
R^{p+1}\\
\mu_i=E(y_i|(X_{1}=x_{i1},...&amp;X_{p}=x_{ip}))=b\prime(\theta_i)=g^{-1}(\eta_i)
\end{aligned}
\]</span> 这样就可以写出似然函数为： <span class="math display">\[
L(\vec
\beta)=\prod_{i=1}^n[\frac{y_i\theta_i-b(\theta_i)}{a_i(\phi)}+c(y_i,\phi)]
\]</span> 对数似然函数为： <span class="math display">\[
lnL(\vec
\beta)=\sum_{i=1}^n[\frac{y_i\theta_i-b(\theta_i)}{a_i(\phi)}+c(y_i,\phi)]
\]</span> ## 优化算法</p>
<h3 id="牛顿法">牛顿法</h3>
<p>我们对<span class="math inline">\(\vec \beta\)</span>求偏导数得：
<span class="math display">\[
\begin{aligned}
\frac{\partial lnL(\vec \beta)}{\partial \vec
\beta}=\sum_{i=1}^n[\frac{\partial lnL(\vec \beta)}{\partial
\theta_i}\frac{\partial \theta_i}{\partial \vec \beta}]\\
=\sum_{i=1}^n[\frac{y_i-b\prime(\theta_i)}{a_i(\phi)}\vec x_i]
\end{aligned}
\]</span> 令一阶导数为<span
class="math inline">\(Z\)</span>，下面继续求二阶导数： <span
class="math display">\[
\begin{aligned}
\frac{\partial ^2lnL(\vec \beta)}{\partial \vec \beta\partial \vec
\beta^T}=\sum_{i=1}^n\frac{\partial Z}{\partial \theta_i}(\frac{\partial
\theta_i}{\partial\vec \beta})\\
=\sum_{i=1}^n[-\frac{b\prime\prime(\theta_i)}{a_i(\phi)}\vec x_i\vec
x_i^T]\\
=-\sum_{i=1}^n\frac{b\prime\prime(\theta_i)}{a_i(\phi)}\vec x_i\vec
x_i^T
\end{aligned}
\]</span></p>
<p>我们将以上推导的结果写成矩阵形式。因为<span
class="math inline">\(Var(y_i)=a_i(\phi)b\prime\prime(\theta_i),g\prime(\mu_i)b\prime\prime(\theta_i)=1\)</span>，可得<span
class="math inline">\(\frac{1}{a_i(\phi)}=\frac{1}{Var(y_i)g\prime(\mu_i)}\)</span>。我们令<span
class="math inline">\(A=diag(\frac{1}{a_i(\phi)})\)</span>，即： <span
class="math display">\[
\begin{aligned}
A=diag(\frac{1}{a_i(\phi)})=\begin{bmatrix}
\frac{1}{Var(y_1)g\prime(\mu_1)}&amp;\cdot\cdot\cdot&amp;0&amp;\cdot\cdot\cdot&amp;0\\
\cdot\cdot\cdot&amp;\cdot\cdot\cdot&amp;\cdot\cdot\cdot&amp;\cdot\cdot\cdot&amp;\cdot\cdot\cdot\\
0&amp;\cdot\cdot\cdot&amp;\frac{1}{Var(y_i)g\prime(\mu_i)}&amp;\cdot\cdot\cdot&amp;0\\
\cdot\cdot\cdot&amp;\cdot\cdot\cdot&amp;\cdot\cdot\cdot&amp;\cdot\cdot\cdot&amp;\cdot\cdot\cdot\\
0&amp;\cdot\cdot\cdot&amp;0&amp;\cdot\cdot\cdot&amp;\frac{1}{Var(y_n)g\prime(\mu_n)}\\
\end{bmatrix}
\end{aligned}
\]</span> 同时令<span
class="math inline">\(\frac{b\prime\prime(\theta_i)}{a_i(\phi)}=\frac{1}{Var(y_i)[g\prime(\mu_i)]^2}\)</span>以及<span
class="math inline">\(W=diag(\frac{b\prime\prime(\theta_i)}{a_i(\phi)})\)</span>，即：
<span class="math display">\[
\begin{aligned}
W=diag(\frac{b\prime\prime(\theta_i)}{a_i(\phi)})=\begin{bmatrix}
\frac{1}{Var(y_1)[g\prime(\mu_1)]^2}&amp;\cdot\cdot\cdot&amp;0&amp;\cdot\cdot\cdot&amp;0\\
\cdot\cdot\cdot&amp;\cdot\cdot\cdot&amp;\cdot\cdot\cdot&amp;\cdot\cdot\cdot&amp;\cdot\cdot\cdot\\
0&amp;\cdot\cdot\cdot&amp;\frac{1}{Var(y_i)[g\prime(\mu_i)]^2}&amp;\cdot\cdot\cdot&amp;0\\
\cdot\cdot\cdot&amp;\cdot\cdot\cdot&amp;\cdot\cdot\cdot&amp;\cdot\cdot\cdot&amp;\cdot\cdot\cdot\\
0&amp;\cdot\cdot\cdot&amp;0&amp;\cdot\cdot\cdot&amp;\frac{1}{Var(y_n)[g\prime(\mu_n)]^2}\\
\end{bmatrix}
\end{aligned}
\]</span></p>
<p>则对数似然函数的梯度为： <span class="math display">\[
\frac{\partial ln L(\vec \beta)}{\partial \vec \beta}=X^TA(\vec y-\vec
\mu)
\]</span> 对数似然函数的Hessian为： <span class="math display">\[
\frac{\partial^2 ln L(\vec \beta)}{\partial \vec \beta\partial \vec
\beta^T}=-X^TWX
\]</span></p>
<p>然后就可以用牛顿法求解了，第<span
class="math inline">\(k+1\)</span>轮牛顿迭代公式为： <span
class="math display">\[
\hat{\vec \beta}_{k+1}=\hat{\vec \beta}_k+(X^TW_kX)^{-1}X^TA_k(\vec
y-\vec \mu_k)
\]</span></p>
<h3 id="迭代再加权最小二乘法">迭代再加权最小二乘法</h3>
<p>当使用规范连接函数时，牛顿法实际上与迭代再加权最小二乘法等价，下面我们简单说明一下。我们对牛顿迭代公式稍作变化得：
<span class="math display">\[
\hat{\vec\beta}_{k+1}=(X^TW_kX)^{-1}(X^TW_kX\vec \beta_k+X^TA_k(\vec
y-\vec \mu_k))
\]</span> 注意到矩阵<span class="math inline">\(W\)</span>和矩阵<span
class="math inline">\(A\)</span>存在关系<span
class="math inline">\(A=Wg\prime(\vec
\mu)\)</span>，因此上述公式可以进一步化简为： <span
class="math display">\[
\begin{aligned}
\hat{\vec \beta}_{k+1}=&amp;(X^TW_kX)^{-1}X^TW_k(X\hat{\vec
\beta}_k+g\prime(\vec \mu_k)(\vec y-\vec \mu_k))\\
\hat{\vec \beta}_{k+1}=&amp;(X^TW_kX)^{-1}X^TW_k\vec z_k
\end{aligned}
\]</span> 这里的<span class="math inline">\(\vec z_k=X\hat{\vec
\beta}_k+g\prime(\vec \mu_k)(\vec y-\vec
\mu_k)\)</span>，在很多文献中<span class="math inline">\(\vec
z_k\)</span>被称为调整后的因变量或者工作变量。这样以来，牛顿迭代过程就可以看作是迭代再加权最小二乘法。</p>
<p>在迭代再加权最小二乘法中，第<span
class="math inline">\(k\)</span>轮我们使用：</p>
<ul>
<li>当前<span class="math inline">\(\vec \beta\)</span>的估计值<span
class="math inline">\(\hat{\vec \beta}_k\)</span>去计算新的权重矩阵<span
class="math inline">\(W_k\)</span>和新的工作向量<span
class="math inline">\(\vec z_k\)</span>；</li>
<li>然后以<span
class="math inline">\(W_k\)</span>为权重，用设计矩阵<span
class="math inline">\(X\)</span>对<span class="math inline">\(\vec
z_k\)</span>进行回归，从而得到<span class="math inline">\(\hat{\vec
\beta}_{k+1}\)</span></li>
</ul>
<p>使用迭代再加权最小二乘法进行参数迭代有两点好处，一个是利于编程，因为对于牛顿法不同分布有不同的迭代公式，而迭代再加权最小二乘法则有统一的公式，再者类似普通回归方便我们导出与模型诊断有关的统计量。</p>
<h1 id="假设检验">假设检验</h1>
<p>在线性回归中，参数的推断是精确推断，这是因为正态分布、线性和最小二乘法良好的性质。但是对于广义线性模型而言，参数的推断往往不是精确的，而是近似的。这是因为我们很难找出广义线性模型下统计量的精确分布，只知道大样本下的渐进分布，而这些推断都是基于这些渐进分布进行的。</p>
<p>根据似然理论，我们知道大样本下参数估计值<span
class="math inline">\(\hat {\vec \beta}\)</span>的渐进分布为： <span
class="math display">\[
\hat{\vec \beta}\mathop{\rightarrow}^dN(\vec \beta,(I(\vec \beta))^{-1})
\]</span> 这里的<span class="math inline">\(I(\vec
\beta)\)</span>为Fisher信息矩阵，由于<span class="math inline">\(\vec
\beta\)</span>是未知的，因此<span class="math inline">\(I(\vec
\beta)\)</span>是未知的，常用<span class="math inline">\(I(\hat{\vec
\beta})\)</span>来估计，而<span class="math inline">\(I(\hat{\vec
\beta})\)</span>等于样本Hessian矩阵取负： <span class="math display">\[
I(\hat{\vec \beta})=-\frac{\partial^2 ln L(\vec \beta)}{\partial \vec
\beta\partial \vec \beta^T}=X^TWX
\]</span> 有了<span class="math inline">\(\hat{\vec
\beta}\)</span>和<span class="math inline">\(I^{-1}(\vec
\beta)\)</span>的估计，接下来就可以使用Wald检验、似然比检验和LM检验了，同时也可以构造执行区间。</p>
<h1 id="拟合优度">拟合优度</h1>
<p>广义线性模型中，评价拟合优度的有两类统计量，一个deviance统计量，一类是Pearson卡方统计量。</p>
<h2 id="饱和模型">饱和模型</h2>
<p>引入deviance概念之前，还需要先介绍一下饱和模型的概念。饱和模型是一个完美拟合数据的模型。在这个意义上看，完美模型对第<span
class="math inline">\(i\)</span>个观测的预测值<span
class="math inline">\(\hat y_i\)</span>等于真实值<span
class="math inline">\(y_i\)</span>。前面提到对数似然函数等于： <span
class="math display">\[
lnL(\vec
\beta)=\sum_{i=1}^n[\frac{y_i\theta_i-b(\theta_i)}{a_i(\phi)}+c(y_i,\phi)]
\]</span> 计算饱和模型其实就是用<span
class="math inline">\(y_i\)</span>替换公式中的<span
class="math inline">\(\mu_i\)</span>，如果用的是规范连接函数，这等于设置<span
class="math inline">\(\theta_i=g(y_i)\)</span>。记饱和模型的对数似然函数为<span
class="math inline">\(L_s\)</span>，则： <span class="math display">\[
L_s=\sum_{i=1}^n[\frac{y_ig(y_i)-b(g(y_i))}{a_i(\phi)}+c(y_i,\phi)]
\]</span></p>
<h2 id="residual-deviance">residual deviance</h2>
<p><code>deviance</code>又称为<code>residual deviance</code>，是广义线性模型中的一个关键概念。<code>deviance</code>定义了当前拟合模型的对数似然函数值<span
class="math inline">\(L(\hat{\vec \beta})\)</span>与饱和模型<span
class="math inline">\(L_s\)</span>之间的差异，公式为： <span
class="math display">\[
D=-2(L(\hat {\vec \beta})-L_s)\phi
\]</span> <span class="math inline">\(L(\hat {\vec
\beta})\)</span>值永远小于等于<span
class="math inline">\(L_s\)</span>，因此<code>deviance</code>总是大于等于0，仅当当前模型拟合完美时为0。</p>
<p>如果使用规范连接函数，<code>deviance</code>还可以被定义为： <span
class="math display">\[
\begin{aligned}
D=-\frac{2}{a(\phi)}\sum_{i=1}^n[y_i\hat \theta_i-b(\hat
\theta_i)-y_ig(y_i)+b(g(y_i))]\phi\\
=\frac{2}{a(\phi)}\sum_{i=1}^n[y_i(g(y_i)-\hat
\theta_i)-b(g(y_i))+b(\hat \theta_i)]\phi
\end{aligned}
\]</span> 在大多数情况下，<span
class="math inline">\(a(\phi)\)</span>正比于<span
class="math inline">\(\phi\)</span>，因此偏差不依赖于<span
class="math inline">\(\phi\)</span>。</p>
<p><code>deviance</code>实际上是线性回归模型中残差平方和<code>SSE</code>的推广。为了证明这一点，我们考虑线性回归模型。已知线性回归模型中<span
class="math inline">\(\phi=\sigma^2,\theta=\mu=\eta,a(\phi)=\phi,b(\theta)=\frac{\mu^2}{2}\)</span>，因此：
<span class="math display">\[
\begin{aligned}
D=&amp;\frac{2}{\sigma^2}\sum_{i=1}^n[y_i(y_i-\hat\eta_i)-\frac{y_i^2}{2}+\frac{\hat
\eta_i^2}{2}]\sigma^2\\
=&amp;\sum_{i=1}^n[2y_i^2-2y_i\hat \eta_i-y_i^2+\hat \eta_i^2]\\
=&amp;\sum_{i=1}^n(y_i-\hat \eta_i)^2==SSE
\end{aligned}
\]</span></p>
<h2 id="null-deviance">null deviance</h2>
<p>除了<code>residual deviance</code>，还有<code>null deviance</code>，它的公式为：
<span class="math display">\[
D_0=-2(L(\hat {\vec \beta_0})-L_s)\phi
\]</span> <span class="math inline">\(L(\hat {\vec
\beta_0})\)</span>表示只有截距项的模型的对数似然函数值。<code>null residual</code>实际上是线性回归模型中总平方和<code>SST</code>的推广：
<span class="math display">\[
D_0=\sum_{i=1}^n(y_i-\hat\eta_i)^2
\]</span> 此时的<span class="math inline">\(\hat\eta_i=\hat \beta_0=\bar
y\)</span>，因此<span class="math inline">\(D_0=SST\)</span>。</p>
<h2 id="伪决定系数">伪决定系数</h2>
<p>类比线性回归，我们对决定系数<span
class="math inline">\(R^2\)</span>进行推广： <span
class="math display">\[
R^2=1-\frac{D}{D_0}
\]</span> 广义线性模型的<span
class="math inline">\(R^2\)</span>又与线性回归中的决定系数有些不同，一方面它并不是模型解释方差的占比，而是测量了当前模型与完美模型的接近程度；另一方面，它与相关系数无关。正因为如此，有些资料又将这个<span
class="math inline">\(R^2\)</span>称为伪<span
class="math inline">\(R^2\)</span>。</p>
<h2 id="scaled-deviance">scaled deviance</h2>
<p>还有一个与<code>deviance</code>有关的度量指标是<code>scaled deviance</code>：
<span class="math display">\[
D^{*}=\frac{D}{\phi}=-2[L(\hat{\vec \beta})-L_s]
\]</span> 如果<span
class="math inline">\(\phi\)</span>为1，则<code>deviance</code>和<code>scaled deviance</code>是一致的（例如二项或泊松回归）。<code>scaled deviance</code>有一个重要的性质：
<span class="math display">\[
D^{*}\mathop{\rightarrow}^d\chi^2(n-p-1)
\]</span> 注意以上是渐进分布，不过在线性回归的情景下，<span
class="math inline">\(D^{*}=\frac{1}{\sigma^2}SSE\sim
\chi^2(n-p-1)\)</span>，此时是精确分布。</p>
<p>在<span class="math inline">\(\phi\)</span>未知的情况下，由于<span
class="math inline">\(D^{*}\mathop{\rightarrow}^d\chi^2(n-p-1)\)</span>，这实际上为我们提供了一种估计<span
class="math inline">\(\phi\)</span>的方法。因为<span
class="math inline">\(E(\chi^2(n-p-1))=n-p-1\)</span>，因此<span
class="math inline">\(\phi\)</span>的估计为： <span
class="math display">\[
\hat \phi = \frac{-2[L(\hat{\vec \beta})-L_s]}{n-p-1}
\]</span> 在线性回归模型中<span class="math inline">\(\hat \phi=\hat
\sigma^2\)</span>，这和预期是一致的。</p>
<p><code>scaled deviance</code>还有更重要的作用，就是可用于对广义线性模型的系数集进行假设检验。</p>
<p>假定我们现在有一个模型<span
class="math inline">\(M_2\)</span>，有<span
class="math inline">\(p_2\)</span>个自变量（不包括截距），另有一个模型<span
class="math inline">\(M_1\)</span>，自变量个数为<span
class="math inline">\(p_1\)</span>，同时有<span
class="math inline">\(p_1\lt p_2\)</span>，且<span
class="math inline">\(M_1\)</span>的自变量是<span
class="math inline">\(M_2\)</span>的子集。换句话说，<span
class="math inline">\(M_1\)</span>嵌套于<span
class="math inline">\(M_2\)</span>中。然后我们可以做一个假设检验，检验的零假设为：<span
class="math inline">\(M_2\)</span>中多余的自变量系数同时为0。例如，<span
class="math inline">\(M_1\)</span>模型的系数为<span
class="math inline">\(\beta_0,\beta_1,...,\beta_{p_{1}}\)</span>，<span
class="math inline">\(M_2\)</span>模型的系数为<span
class="math inline">\(\beta_0,\beta_1,...,\beta_{p_{1}},\beta_{p_{1}+1},...,\beta_{p_{2}}\)</span>，零假设为：
<span class="math display">\[
H_0:\beta_{p_1+1}=...=\beta_{p_2}=0
\]</span> 我们可以构造如下统计量进行检验： <span class="math display">\[
D_{p1}^*-D_{p2}^*\mathop{\rightarrow}^d\chi^2(p_2-p_1)
\]</span> 如果<span class="math inline">\(H_0\)</span>成立，则<span
class="math inline">\(D_{p1}^*-D_{p2}\)</span>应该比较小，反之，我们有理由拒绝<span
class="math inline">\(H_0\)</span>。</p>
<p><code>scaled deviance</code>很显然移除了<span
class="math inline">\(\phi\)</span>项，但是仍然依赖于<span
class="math inline">\(\phi\)</span>，因为似然函数中仍然包含<span
class="math inline">\(\phi\)</span>，因此在不知道<span
class="math inline">\(\phi\)</span>时是无法计算<span
class="math inline">\(D^*\)</span>的，这使得上面式子应用受到了限制。幸运的是如果构造如下统计量，就可以消掉<span
class="math inline">\(\phi\)</span>了： <span class="math display">\[
F=\frac{(D^*_{p_1}-D^*_{p_2})/(p_2-p_1)}{D^*_{p_2}/(n-p_2-1)}=\frac{(D_{p_1}-D_{p_2})/(p_2-p_1)}{D_{p_2}/(n-p_2-1)}\mathop{\rightarrow}^d
F(p_2-p_1,n-p_2-1)
\]</span> 注意到当<span
class="math inline">\(p_1=0,p_2=p\)</span>时，这个检验实际上是线性回归中<span
class="math inline">\(F\)</span>检验的扩展。</p>
<p>借助<code>deviance</code>来对模型进行的分析称为<code>analysis of deviance</code>。在<span
class="math inline">\(R\)</span>中<code>deviance</code>的计算和相关的检验都可以通过<code>anova</code>函数进行。</p>
<h1 id="模型选择">模型选择</h1>
<h1 id="模型诊断">模型诊断</h1>
<p>上面已经提到，广义线性模型是建立在一些概率假设之上的，这要求应该对模型参数<span
class="math inline">\(\vec \beta\)</span>和<span
class="math inline">\(\phi\)</span>进行合理的推断。</p>
<p>总的来说，如果我们应用了规范连接函数，我们假定数据是从下面分布中生成的：
<span class="math display">\[
y|(X_1=x_1,...,X_p=x_p)\sim exp(\eta,\phi,a,b,c)
\]</span> 通过这种方式： <span class="math display">\[
\mu=E(y|X_1=x_1,...,X_p=x_p)=g^{-1}(\eta)\\
\eta=\vec x^T\vec \beta
\]</span>
对于具有规范链接函数的逻辑回归和泊松回归，一般模型采用如下形式(独立性是隐含的)：
<span class="math display">\[
y|(X_1=x_1,...,X_p=x_p)\sim Ber(logistic(\eta))\\
y|(X_1=x_1,...,X_p=x_p)\sim Pois(e^{\eta})
\]</span> 上面式子背后的假定是：</p>
<ol type="1">
<li>线性：期望值转换后与自变量之间是线性的；</li>
<li>因变量<span
class="math inline">\(y\)</span>的分布是对应的指数族分布；</li>
<li>独立性</li>
</ol>
<p>这些假设的背后还应该注意几点：</p>
<ol type="1">
<li>等方差在那里？等方差只是某些指数分布族所特定的，这些分布族中<span
class="math inline">\(\theta\)</span>并不影响方差，例如正态分布。然而对于二项分布和泊松分布而言，他们本身就是异方差的。</li>
<li>误差项在那里？误差不是建立线性模型的基础，而只是一个与最小二乘相关的有用概念。</li>
<li>没有提到<span
class="math inline">\(X_1,...,X_p\)</span>的分布，他们可以是确定的，也可以是随机的，可以是离散的，也可以是连续的。另外<span
class="math inline">\(X_1,...,X_p\)</span>并不要求相互独立。</li>
</ol>
<p>检验上述假定要比线性回归复杂的多，原因在于因变量<span
class="math inline">\(y\)</span>的异质性和异方差性，这使得残差<span
class="math inline">\(y_i-\hat
y_i\)</span>的检验变得复杂。而我们第一步要做的就是构造这个残差<span
class="math inline">\(\epsilon_i\)</span>。</p>
<h2 id="deviance-residuals"><strong>deviance residuals</strong></h2>
<p><code>deviance residuals</code>是线性模型中残差<span
class="math inline">\(\epsilon_i=y_i-\hat
y_i\)</span>的推广，它们是参考<code>residuals deviance</code>和<code>SSE</code>之间的类比关系来设计的。<code>residuals deviance</code>可以被表达为下面的<span
class="math inline">\(d_i\)</span>之和： <span class="math display">\[
D=\sum_{i=1}^nd_i
\]</span> 对于线性模型，<span class="math inline">\(d_i=\hat
\epsilon_i^2\)</span>，因为<span
class="math inline">\(D=SSE\)</span>。基于这些，我们可以定义<code>deviance residuals</code>为<span
class="math inline">\(\hat \epsilon^D_i\)</span>： <span
class="math display">\[
\hat \epsilon_i^D=sign(y_i-\mu_i)\sqrt{d_i},\quad i=1,2,...,n
\]</span> 这就是广义线性模型对线性回归中残差<span
class="math inline">\(\hat
\epsilon_i\)</span>的推广。根据前面介绍的<span
class="math inline">\(D^*\mathop{\rightarrow}^d
\chi^2(n-p-1)\)</span>，我们可以导出<span class="math inline">\(\hat
\epsilon_i^D\)</span>为渐进正态分布。<code>deviance residuals</code>还可以标准化得到标准化<code>deviance residuals</code>：
<span class="math display">\[
\hat \epsilon_i^{D_s}=\frac{\hat
\epsilon_i^D}{\sqrt{\hat\phi(1-h_{ii})}}
\]</span> <span class="math inline">\(h_{ii}\)</span>为帽子矩阵<span
class="math inline">\(W^{\frac{1}{2}}X(X^TWX)^{-1}X^TW^{\frac{1}{2}}\)</span>第<span
class="math inline">\(i\)</span>行第<span
class="math inline">\(i\)</span>列的取值。</p>
<p><code>deviance residuals</code>是广义线性模型诊断的关键，当我们提<code>residual</code>时，我们一般指的是<code>deviance residuals</code>（还有其他的residual定义），这也是R中<code>residuals</code>返回的残差。</p>
<p><code>deviance residuals</code>与线性回归中的残差还有一些有趣的联系：</p>
<ol type="1">
<li>类似线性回归，如果使用规范链接函数，<span
class="math inline">\(\sum_{i=1}^n\hat \epsilon_i^D=0\)</span>;</li>
<li>尺度参数的估计可以看作<span class="math inline">\(\hat
\phi_D=\frac{\sum_{i=1}^n(\hat
\epsilon_i^D)^2}{n-p-1}\)</span>，这和线性回归中也是类似的；</li>
<li>因此<span class="math inline">\(\hat \phi_D\)</span>实际上是<span
class="math inline">\(\hat
\epsilon_1^D,...,\hat\epsilon_n^D\)</span>的样本方差，这表明<span
class="math inline">\(\phi\)</span>是总体<code>deviance residuals</code>的渐近方差，即<span
class="math inline">\(Var(\epsilon^D)\approx \phi\)</span>;</li>
</ol>
<h2 id="response-residual">response residual</h2>
<p><code>response residual</code>的定义和线性回归中残差定义是类似的：
<span class="math display">\[
y_i-\mu_i
\]</span>
<code>response residual</code>的缺点是方差不是常数，因而很少使用。</p>
<blockquote>
<p>在R中<code>residuals(glmfit,type='response')</code>返回<code>response residual</code>。</p>
</blockquote>
<h2 id="partial-residual">partial residual</h2>
<p><code>partial residual</code>定义为： <span class="math display">\[
z_i-\hat \eta_i
\]</span>
<code>partial residual</code>可以用来评价某个自变量的非线性关系。</p>
<blockquote>
<p>在R中<code>residuals(glmfit,type='working')</code>或<code>residuals(glmfit,type='partial')</code>返回<code>partial residual</code>。</p>
</blockquote>
<h2 id="线性">线性</h2>
<p>随机变量<span
class="math inline">\(y\)</span>的期望的转换值与自变量<span
class="math inline">\(x_1,…,x_p\)</span>之间的线性关系是广义线性模型的基石。如果这个假设不成立，那么我们从分析中得出的所有结论都被怀疑是有缺陷的。因此，这是一个关键的假设。那么如何检验它呢？</p>
<h3 id="如何检验">如何检验</h3>
<h4 id="deviance-residuals-vs-eta">deviance residuals VS <span
class="math inline">\(\eta\)</span></h4>
<p>我们可以画出标准化的<code>deviance residuals</code>和拟合值的散点图，对于广义线性模型而言，这是<span
class="math inline">\(\hat \eta_i\)</span>和<span
class="math inline">\(\hat \epsilon_i^D\)</span>的散点图，注意不是<span
class="math inline">\(\hat y_i\)</span>和<span
class="math inline">\(\hat
\epsilon_i^D\)</span>的散点图。在线性假定之下，<span
class="math inline">\(\hat \eta_i\)</span>和<span
class="math inline">\(\hat
\epsilon_i^D\)</span>之间应该没有什么趋势。如果观察到非线性，则最好用偏残差图来寻找非线性关系。</p>
<h4 id="partial-residuals-plot">partial residuals plot</h4>
<h3 id="如何解决">如何解决</h3>
<p>对有问题的自变量进行非线性转换，或者添加交互作用项可能有帮助。另外，考虑对<span
class="math inline">\(y\)</span>做一个非线性转换也可能是有用的。</p>
<h2 id="因变量分布">因变量分布</h2>
<p><code>deviance residuals</code>的近似正态性允许我们对因变量分布进行检验。好消息是，我们可以做到这一点，而不必专用工具。坏消息是，我们必须在不精确性方面付出重要的代价，因为我们采用了渐近分布。这种渐近收敛的速度和有效性在很大程度上取决于几个方面：因变量的分布、样本量和自变量的分布。</p>
<h3 id="如何检验-1">如何检验</h3>
<p>QQ图允许我们检查标准化残差是否遵循<span
class="math inline">\(N(0,1)\)</span>。在正确的因变量分布下，我们期望这些点与对角线对齐。通常情况下，即使在正态数据下，在极端情况下也会偏离对角线，而不是在中心，尽管如果数据是非正态的，这些偏离会更加明显。不幸的是，即使模型完全正确，也有可能严重偏离正态。原因很简单，<code>deviance residuals</code>明显是非正态的，这在逻辑回归中经常发生。</p>
<h3 id="如何检验-2">如何检验</h3>
<p>修正分布假设并不容易，需要考虑更灵活的模型。一种可能是通过转换<span
class="math inline">\(y\)</span>，当然代价是对转换后的因变量建模，而不是对<span
class="math inline">\(y\)</span>建模。</p>
<h2 id="独立性">独立性</h2>
<p>独立性也是一个关键的假设:它保证了我们拥有的关于<span
class="math inline">\(y\)</span>和<span
class="math inline">\(x1,…,xp\)</span>与n个观测值之间关系的信息量是最大的。</p>
<p>残差中是否存在自相关可以用残差的连续图来检验。在不相关条件下，我们期望序列没有残差的跟踪，这是序列正相关的标志。负序列相关可以通过残差的小-大或正-负系统交替的形式来识别。这可以通过延迟更好地进行探索。</p>
<p>在线性模型中，如果数据中存在依赖关系，一旦收集到这些数据，就不能做什么。如果存在序列依赖性，则响应的分化可能导致独立的观测。</p>
<h2 id="多重共线性">多重共线性</h2>
<p>多重共线性也可以出现在广义线性模型中。尽管自变量对因变量是非线性影响，但自变量的转换值是线性的。因此，如果两个或多个预测因子之间高度相关，则模型的拟合将受到影响，因为每个预测因子的单独线性效应将难以与其他相关预测因子分离。</p>
<p>检测多重共线性的一种有效方法是检验每个自变量的VIF。这种情况与线性回归完全相同。</p>
<p>正态分布 <span class="math display">\[
\begin{aligned}
f(y)=\frac{1}{\sqrt{2\pi}\sigma}exp(-\frac{1}{2\sigma^2}(y-\mu)^2)\\
=exp(ln(\frac{1}{\sqrt{2\pi}\sigma})-\frac{1}{2\sigma^2}(y^2-2y\mu+\mu^2))\\
=exp(\frac{y\mu-\frac{1}{2}\mu^2}{\sigma^2}-\frac{y^2}{2\sigma^2}+ln(\frac{1}{\sqrt{2\pi}\sigma}))
\end{aligned}
\]</span> 因此： <span class="math display">\[
\begin{aligned}
b(\theta)=\frac{1}{2}\mu^2\\
\theta=\mu\\
a(\phi)=\sigma^2\\
g(\mu)=\eta=\theta=\mu=X\vec \beta\\
Var(y)=\sigma^2\\
g\prime(\mu)=1
\end{aligned}
\]</span> 进一步可得<span class="math inline">\(A\)</span>矩阵和<span
class="math inline">\(W\)</span>矩阵为： <span class="math display">\[
\begin{aligned}
A=diag(\frac{1}{Var(y_i)g\prime(\mu_i)})=diag(\frac{1}{\sigma^2_i})=\begin{bmatrix}
\frac{1}{\sigma^2}&amp;\cdot\cdot\cdot&amp;0&amp;\cdot\cdot\cdot&amp;0\\
\cdot\cdot\cdot&amp;\cdot\cdot\cdot&amp;\cdot\cdot\cdot&amp;\cdot\cdot\cdot&amp;\cdot\cdot\cdot\\
0&amp;\cdot\cdot\cdot&amp;\frac{1}{\sigma^2}&amp;\cdot\cdot\cdot&amp;0\\
\cdot\cdot\cdot&amp;\cdot\cdot\cdot&amp;\cdot\cdot\cdot&amp;\cdot\cdot\cdot&amp;\cdot\cdot\cdot\\
0&amp;\cdot\cdot\cdot&amp;0&amp;\cdot\cdot\cdot&amp;\frac{1}{\sigma^2}\\
\end{bmatrix}\\
W=diag(\frac{1}{Var(y_i)(g\prime(\mu_i))^2})=diag(\frac{1}{\sigma^2_i})=\begin{bmatrix}
\frac{1}{\sigma^2}&amp;\cdot\cdot\cdot&amp;0&amp;\cdot\cdot\cdot&amp;0\\
\cdot\cdot\cdot&amp;\cdot\cdot\cdot&amp;\cdot\cdot\cdot&amp;\cdot\cdot\cdot&amp;\cdot\cdot\cdot\\
0&amp;\cdot\cdot\cdot&amp;\frac{1}{\sigma^2}&amp;\cdot\cdot\cdot&amp;0\\
\cdot\cdot\cdot&amp;\cdot\cdot\cdot&amp;\cdot\cdot\cdot&amp;\cdot\cdot\cdot&amp;\cdot\cdot\cdot\\
0&amp;\cdot\cdot\cdot&amp;0&amp;\cdot\cdot\cdot&amp;\frac{1}{\sigma^2}\\
\end{bmatrix}
\end{aligned}
\]</span> 梯度以及解为： <span class="math display">\[
\begin{aligned}
X^TA(\vec y-\vec \mu)=X^T\frac{1}{\sigma^2}I(\vec y-X\vec \beta)=0\\
\hat {\vec \beta}=(X^TX)^TX^T\vec y
\end{aligned}
\]</span> Hessian以及<span class="math inline">\(\hat {\vec
\beta}\)</span>的渐进方差为： <span class="math display">\[
\begin{aligned}
-\frac{1}{\sigma^2}X^TX\\
[\frac{1}{\sigma^2}X^TX]^{-1}
\end{aligned}
\]</span> 伯努利分布 <span class="math display">\[
\begin{aligned}
f(y)=p^y(1-p)^{1-y}\\
=exp(ln(p^y(1-p)^{1-y}))\\
=exp(ylnp+(1-y)ln(1-p))\\
=exp(ylnp-yln(1-p)+ln(1-p))\\
=exp(yln\frac{p}{1-p}+ln(1-p))
\end{aligned}
\]</span> 因此： <span class="math display">\[
\begin{aligned}
\theta= ln\frac{p}{1-p}\\
b\prime(\theta)
\end{aligned}
\]</span></p>
<p><span class="math display">\[
\frac{e^{-\lambda}\lambda^y}{y!}=\frac{e^{-y}y^y}{y!}
\]</span></p>
]]></content>
      <categories>
        <category>数理统计</category>
      </categories>
      <tags>
        <tag>回归</tag>
        <tag>分类</tag>
      </tags>
  </entry>
  <entry>
    <title>分类算法</title>
    <url>/2022/12/25/2.%E5%88%86%E7%B1%BB%E7%AE%97%E6%B3%95/</url>
    <content><![CDATA[<p>​</p>
<span id="more"></span>
<h1 id="逻辑回归">逻辑回归</h1>
<h2 id="符号">符号</h2>
<p><span class="math inline">\(X\)</span>：数据矩阵，<span
class="math inline">\(X\in R^{n*p}\)</span></p>
<p><span class="math inline">\(\vec
y\)</span>：标签向量（列向量），<span class="math inline">\(\vec y\in
R^n\)</span></p>
<p><span class="math inline">\(\hat y\)</span>：标签的预测值</p>
<p><span class="math inline">\(y\)</span>：标签的真实值</p>
<p><span class="math inline">\(\vec
w\)</span>：参数向量（列向量），<span class="math inline">\(\vec w\in
R^p\)</span></p>
<p><span class="math inline">\(\vec
x\)</span>：一条观测数据的特征向量（列向量），<span
class="math inline">\(\vec x\in R^p\)</span></p>
<p><span class="math inline">\(l\)</span>：一条数据的损失</p>
<p><span class="math inline">\(L\)</span>：总损失</p>
<p><span class="math inline">\(\nabla_{\vec
w}L\)</span>：损失函数关于<span class="math inline">\(\vec
w\)</span>的梯度</p>
<p><span class="math inline">\(\nabla_{\vec
w}^2L\)</span>：损失函数关于<span class="math inline">\(\vec
w\)</span>的hessian矩阵</p>
<p><span
class="math inline">\(\odot\)</span>：向量或矩阵对应元素相乘</p>
<p><span class="math inline">\(diag(a)\)</span>：用<span
class="math inline">\(a\)</span>来生成一个对角矩阵，即用<span
class="math inline">\(a\)</span>填充对角矩阵的主对角线。</p>
<h2 id="模型">模型</h2>
<p><span class="math display">\[
\hat y=\frac{1}{1+e^{-\vec x^T\vec w}}=sigmoid(\vec x^T\vec w)
\]</span></p>
<h2 id="损失">损失</h2>
<p><span class="math display">\[
l=-[yln\hat y+(1-y)ln(1-\hat y)]\\
=-[yln\frac{1}{1+e^{-\vec x^T\vec w}}+(1-y)ln(1-\frac{1}{1+e^{-\vec
x^T\vec w}})]\\
=-[yln\frac{e^{\vec x^T\vec w}}{e^{\vec x^T\vec
w}+1}+(1-y)ln\frac{1}{e^{\vec x^T\vec w}+1}]\\
=-[ylne^{\vec x^T\vec w}-yln(e^{\vec x^T\vec w}+1)+yln(e^{\vec x^T\vec
w}+1)-ln(e^{\vec x^T\vec w}+1)]\\
=-y\vec x^T\vec w+ln(e^{\vec x^T\vec w}+1)
\]</span></p>
<p><span class="math display">\[
L=-\vec y^TX\vec w+\vec 1^Tln(e^{X\vec w}+\vec 1)
\]</span></p>
<h2 id="优化">优化</h2>
<h3 id="梯度">梯度</h3>
<p>求微分 <span class="math display">\[
dL=-\vec y^TXd\vec w+\vec 1^Td(ln(e^{X\vec w}+\vec 1))\\
=-\vec y^TXd\vec w+\vec 1^T[\frac{1}{e^{X\vec w}+\vec 1}\odot d(e^{X\vec
w}+\vec 1)]\\
=-\vec y^TXd\vec w+[\frac{1}{e^{X\vec w}+\vec 1}]^Td(e^{X\vec w})\\
=-\vec y^TXd\vec w+[\frac{1}{e^{X\vec w}+\vec 1}]^T[e^{X\vec w}\odot
Xd\vec w]
\]</span> <span class="math display">\[
=tr(-\vec y^TXd\vec w+[\frac{1}{e^{X\vec w}+\vec 1}]^T[e^{X\vec w}\odot
Xd\vec w])\\
=tr(-\vec y^TXd\vec w+[\frac{1}{e^{X\vec w}+\vec 1}\odot e^{X\vec
w}]^TXd\vec w)\\
=tr([\frac{e^{X\vec w}}{e^{X\vec w}+\vec 1}-\vec y]^TXd\vec w)\\
=tr([X^T(\frac{e^{X\vec w}}{e^{X\vec w}+\vec 1})]^Td\vec w)
\]</span></p>
<p>梯度： <span class="math display">\[
\nabla_{\vec w}L=X^T(\frac{e^{X\vec w}}{e^{X\vec w}+\vec 1}-\vec y)\\
=X^T(sigmoid(X\vec w)-\vec y)
\]</span></p>
<h3 id="hessian矩阵">hessian矩阵</h3>
<p>微分： <span class="math display">\[
d\nabla_{\vec w}L=X^Td(sigmoid(X\vec w))\\
=X^T[sigmoid\prime(X\vec w)\odot Xd\vec w]\\
=X^Tdiag(sigmoid\prime(X\vec w))Xd\vec w\\=[X^Tdiag(sigmoid\prime(X\vec
w))X]^Td\vec w
\]</span> 所以Hessian为： <span class="math display">\[
\nabla_{\vec w}^2L=X^Tdiag(sigmoid\prime(X\vec w))X\\
=X^Tdiag(sigmoid(X\vec w)\odot(\vec 1-sigmoid(X\vec w)))X
\]</span></p>
<p>容易看出，当<span
class="math inline">\(X\)</span>满秩时hessian矩阵正定，故可用迭代下降算法求解参数<span
class="math inline">\(\vec w\)</span>。</p>
<h1 id="softmax回归">softmax回归</h1>
<p>主要用来做多分类，假设有<span
class="math inline">\(k\)</span>类。</p>
<h2 id="符号-1">符号</h2>
<p><span class="math inline">\(\vec
y\)</span>：标签向量（列向量）的真实值（经过one-hot编码），<span
class="math inline">\(\vec y\in R^k\)</span></p>
<p><span class="math inline">\(\hat{\vec
y}\)</span>：标签的预测值（列向量），<span
class="math inline">\(\hat{\vec y}\in R^k\)</span></p>
<p><span class="math inline">\(W\)</span>：参数矩阵，<span
class="math inline">\(W\in R^{k*p}\)</span>，每一行对应一个类别</p>
<p><span class="math inline">\(\vec
x\)</span>：一条观测数据的特征向量（列向量），<span
class="math inline">\(\vec x\in R^p\)</span></p>
<p><span class="math inline">\(l\)</span>：一条数据的损失</p>
<p><span
class="math inline">\(\odot\)</span>：向量或矩阵对应元素相乘</p>
<p><span class="math inline">\(\otimes\)</span>：Kronecker积</p>
<p><span class="math inline">\(diag(a)\)</span>：用<span
class="math inline">\(a\)</span>来生成一个对角矩阵，即用<span
class="math inline">\(a\)</span>填充对角矩阵的主对角线。</p>
<h2 id="模型-1">模型</h2>
<p><span class="math display">\[
\hat{\vec y}=softmax(W\vec x)=\frac{e^{W\vec x}}{\vec 1^Te^{W\vec x}}
\]</span></p>
<h2 id="损失-1">损失</h2>
<p><span class="math display">\[
l=-\vec y^Tln\hat {\vec y}\\
=-\vec y^Tln\frac{e^{W\vec x}}{\vec 1^Te^{W\vec x}}\\
=-\vec y^T(lne^{W\vec x}-\vec 1ln(\vec 1^Te^{W\vec x}))\\
=-\vec y^TW\vec x+ln(\vec 1^Te^{W\vec x})
\]</span></p>
<h2 id="优化-1">优化</h2>
<h3 id="梯度-1">梯度</h3>
<p>微分： <span class="math display">\[
dl=-\vec y^T (dW)\vec x+\frac{1}{\vec 1^Te^{W\vec x}}\odot d(\vec
1^Te^{W\vec x})\\
=-\vec y^T(dW)\vec x+\frac{1}{\vec 1^Te^{W\vec x}}\vec 1^T de^{W\vec
x}\\
=-\vec y^T(dW)\vec x+\frac{1}{\vec 1^Te^{W\vec x}}(\vec 1^T(e^{W\vec
x}\odot d(W)\vec x))\\
=-\vec y^T(dW)\vec x+\frac{1}{\vec 1^Te^{W\vec x}}[e^{W\vec
x}]^{T}(dW)\vec x\\
=(\frac{1}{\vec 1^Te^{W\vec x}}[e^{W\vec x}]^T-\vec y^T)(dW)\vec x\\
=tr(\vec x(\frac{[e^{W\vec x]^T}}{\vec 1^Te^{W\vec x}}-\vec y^T)dW)\\
=tr([(\frac{e^{W\vec x}}{\vec 1^Te^{W\vec x}}-\vec y)\vec x^T]^TdW)
\]</span> 梯度： <span class="math display">\[
\nabla_{W}l=(\frac{e^{W\vec x}}{\vec 1^T e^{W\vec x}}-\vec y)\vec
x^T=(softmax(W\vec x)-\vec y)\vec x^T
\]</span></p>
<h3 id="hessian矩阵-1">hessian矩阵</h3>
<p>令： <span class="math display">\[
\vec z=W\vec x
\]</span></p>
<p>求梯度微分： <span class="math display">\[
d\nabla_Wl=(\frac{e^{\vec z}\odot d\vec z}{\vec 1^Te^{\vec
z}}-\frac{e^{\vec z}(\vec 1^T(e^{\vec z}\odot d\vec z))}{(\vec 1^T
e^{\vec z})^2})\vec x^T\\
=(\frac{diag(e^{\vec z})}{\vec 1^Te^{\vec z}}-\frac{e^{\vec z}[e^{\vec
z}]^T}{(\vec 1^Te^{\vec z})^2})(d\vec z) \vec x^T\\
=(diag(softmax(\vec z))-softmax(\vec z)[softmax(\vec z)]^T)(d\vec z)\vec
x^T\\
=(diag(softmax(W\vec x))-softmax(W\vec x)[softmax(W\vec x)]^T)(dW)\vec
x\vec x^T
\]</span> hessian矩阵为： <span class="math display">\[
\nabla_W^2l=(\vec x\vec x^T)\otimes (diag(softmax(W\vec
x))-softmax(W\vec x)[softmax(W\vec x)]^T)
\]</span> 同样用迭代下降算法求解参数<span
class="math inline">\(W\)</span>。</p>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>分类</tag>
      </tags>
  </entry>
  <entry>
    <title>无约束优化算法</title>
    <url>/2022/12/26/3%E6%97%A0%E7%BA%A6%E6%9D%9F%E4%BC%98%E5%8C%96%E6%96%B9%E6%B3%95/</url>
    <content><![CDATA[<p>​</p>
<span id="more"></span>
<h1 id="迭代下降算法">迭代下降算法</h1>
<ul>
<li>setp0：给损失函数<span class="math inline">\(L(\vec
w)\)</span>的参数一个初始值<span class="math inline">\(\vec
w_k,k=0\)</span>;</li>
<li>step1：判断损失函数<span class="math inline">\(L(\vec
w_k)\)</span>是否满足收敛条件，如果不满足则进入step2；</li>
<li>step2：在<span class="math inline">\(\vec
w_k\)</span>处寻找能使损失函数<span class="math inline">\(L(\vec
w_k)\)</span>减小的方向<span class="math inline">\(d_k\)</span>；</li>
<li>step3：选择步长<span class="math inline">\(\lambda_k\gt
0\)</span>，要求<span class="math inline">\(L(\vec w_k+\lambda_kd_k)\lt
L(\vec w_k)\)</span>；</li>
<li>step4：令<span class="math inline">\(\vec w_{k+1}=\vec
w_k+\lambda_kd_k,k=k+1\)</span>，返回step1。</li>
</ul>
<p>收敛条件：<span class="math inline">\(||\nabla L(\vec w_k)||_2\lt
\epsilon,\epsilon\rightarrow 0\)</span>。</p>
<h1 id="梯度下降法">梯度下降法</h1>
<p>本质是用了泰勒一阶展开 <span class="math display">\[
L(\vec w_{k+1})\approx L(\vec w_k)+\nabla L(\vec w_k)^T(\vec
w_{k+1}-\vec w_k)\\
\rightarrow\\
\vec w_{k+1}-\vec w_k=-\lambda_k\nabla L(\vec w_k)\\
\vec w_{k+1}=\vec w_k-\lambda_k \nabla L(\vec w_k)
\]</span></p>
<h1 id="牛顿法">牛顿法</h1>
<p>本质是用了泰勒二阶展开 <span class="math display">\[
L(\vec w_{k+1})\approx L(\vec w_k)+\nabla L(\vec w_k)^T(\vec
w_{k+1}-\vec w_k)+\frac{1}{2}(\vec w_{k+1}-\vec w_{k})^TH(\vec w_k)(\vec
w_{k+1}-\vec w_k)\\
\nabla L(\vec w_{k+1})=\nabla L(\vec w_k)+H(\vec w_{k})(\vec
w_{k+1}-\vec w_k)=0\\
\rightarrow\\
\vec w_{k+1}=\vec w_k-H(\vec w_{k})^{-1}\nabla L(\vec w_k)
\]</span> <span class="math inline">\(H(\vec w_k)\)</span>为<span
class="math inline">\(\vec
w_k\)</span>处的hessian矩阵，要求为正定矩阵。此时： <span
class="math display">\[
L(\vec w_{k+1})\approx L(\vec w_k)-\nabla L(\vec w_k)^TH(\vec
w_k)^{-1}\nabla L(\vec w_k)+\frac{1}{2}(-H(\vec w_k)^{-1}\nabla L(\vec
w_k))^TH(\vec w_k)(-H(\vec w_k)^{-1}\nabla L(\vec w_k))\\
\approx L(\vec w_k)-\nabla L(\vec w_k)^TH(\vec w_k)^{-1}\nabla L(\vec
w_k)+\frac{1}{2}\nabla L(\vec w_k)^TH(\vec w_k)^{-1}\nabla L(\vec w_k)\\
\approx L(\vec w_k)-\frac{1}{2}\nabla L(\vec w_k)^TH(\vec
w_k)^{-1}\nabla L(\vec w_k)
\]</span> 可见只要<span class="math inline">\(H(\vec
w_k)\)</span>正定，必然有： <span class="math display">\[
L(\vec w_{k+1})\lt L(\vec w_k)
\]</span></p>
<h1 id="拟牛顿法">拟牛顿法</h1>
<p>基本思想是在满足拟牛顿方程的条件下，用一个正定矩阵来近似<span
class="math inline">\(H(\vec w_k)\)</span>，以减小计算量。</p>
<p>用拉格朗日中值定理： <span class="math display">\[
\nabla L(\vec w_k)-\nabla L(\vec w_{k-1})=\nabla^2L(\vec \xi) (\vec
w_k-\vec w_{k-1})
\]</span> 令<span class="math inline">\(B_k=\nabla^2 L(\vec \xi),\vec
y_{k-1}=\nabla L(\vec w_k)-\nabla L(\vec w_{k-1}),\vec \delta_{k-1}=\vec
w_k-\vec w_{k-1}\)</span>，解下面的拟牛顿方程： <span
class="math display">\[
\vec y_{k-1}=B_k\vec \delta_{k-1}
\]</span> 或令<span
class="math inline">\(G_k=B_k^{-1}\)</span>，解下面的拟牛顿方程： <span
class="math display">\[
G_k\vec y_{k-1}=\vec \delta_{k-1}
\]</span> 用解出的<span class="math inline">\(B_k\)</span>代替<span
class="math inline">\(H(\vec w_k)\)</span> <span class="math display">\[
\vec w_{k+1}=\vec w_k-B_k^{-1}\nabla L(\vec w_k)
\]</span> 或<span class="math inline">\(G_k\)</span>代替<span
class="math inline">\(H(\vec w_k)^{-1}\)</span>： <span
class="math display">\[
\vec w_{k+1}=\vec w_k-G_k\nabla L(\vec w_k)
\]</span> 初始化<span class="math inline">\(\vec
w_0\)</span>后，用牛顿迭代法计算<span class="math inline">\(\vec
w_1\)</span>，解拟牛顿方程算出<span
class="math inline">\(B_1\)</span>或<span
class="math inline">\(G_1\)</span>，后面用拟牛顿法迭代。</p>
<p>要求<span class="math inline">\(B_k\)</span>或<span
class="math inline">\(G_k\)</span>为正定矩阵，正定矩阵有未知数<span
class="math inline">\(\frac{p(p+1)}{2}\)</span>个，方程有p个，故解集不唯一。</p>
<h1 id="dfp法">DFP法</h1>
<p>用迭代法求解<span class="math inline">\(G_k\)</span>。 <span
class="math display">\[
G_{k}=G_{k-1}+a\vec u\vec u^T+b\vec v\vec v^T
\]</span></p>
<p>带入拟牛顿方程： <span class="math display">\[
G_{k-1}\vec y_{k-1}+a\vec u\vec u^T\vec y_{k-1}+b\vec v\vec v^T\vec
y_{k-1}=\vec\delta_{k-1}
\]</span> 令： <span class="math display">\[
G_{k-1}\vec y_{k-1}+a\vec u\vec u^T\vec y_{k-1}=0\\
b\vec v\vec v^T\vec y_{k-1}-\vec \delta_{k-1}=0\\
\vec u=G_{k-1}\vec y_{k-1}\\
\vec v=\vec \delta_{k-1}
\]</span> 得： <span class="math display">\[
a=-\frac{1}{\vec y_{k-1}^TG_{k-1}\vec y_{k-1}}\\
b=\frac{1}{\vec\delta_{k-1}^T\vec y_{k-1}}
\]</span> 因此： <span class="math display">\[
G_{k}=G_{k-1}-\frac{G_{k-1}\vec y_{k-1}\vec y_{k-1}^TG_{k-1}^T}{\vec
y_{k-1}^TG_{k-1}\vec y_{k-1}}+\frac{\vec \delta_{k-1}\vec
\delta_{k-1}^T}{\vec \delta_{k-1}^T\vec y_{k-1}}
\]</span></p>
<h1 id="bfgs法">BFGS法</h1>
<p>用迭代法求解<span class="math inline">\(B_k\)</span>。 <span
class="math display">\[
B_k=B_{k-1}+a\vec u\vec u^T+b\vec v\vec v^T
\]</span></p>
<p>带入拟牛顿方程： <span class="math display">\[
\vec y_{k-1}=B_{k-1}\vec \delta_{k-1}+a\vec u\vec u^T\vec
\delta_{k-1}+b\vec v\vec v^T\vec \delta_{k-1}
\]</span> 令： <span class="math display">\[
\vec y_{k-1}=b\vec v\vec v^T\vec \delta_{k-1}\\
B_{k-1}\vec \delta_{k-1}=-a\vec u\vec u^T\vec \delta_{k-1}\\
\vec v=\vec y_{k-1}\\
\vec u=B_{k-1}\vec \delta_{k-1}
\]</span> 可得： <span class="math display">\[
b=\frac{1}{\vec v^T\vec \delta_{k-1}}=\frac{1}{\vec y_{k-1}^T\vec
\delta_{k-1}}\\
a=-\frac{1}{\vec u^T\vec \delta_{k-1}}=\frac{1}{\vec
\delta_{k-1}^TB_{k-1}\vec \delta_{k-1}}
\]</span> 因此： <span class="math display">\[
B_{k}=B_{k-1}-\frac{B_{k-1}\vec
\delta_{k-1}\vec\delta_{k-1}^TB_{k-1}}{\vec \delta_{k-1}^TB_{k-1}\vec
\delta_{k-1}}+\frac{\vec y_{k-1}\vec y_{k-1}^T}{\vec y_{k-1}^T\vec
\delta_{k-1}}
\]</span></p>
<h1 id="broyden法">Broyden法</h1>
<p>结合了DFP和BFGS方法： <span class="math display">\[
G_k=\alpha G^{DFP}+(1-\alpha)G^{BFGS},0\le\alpha\le 1
\]</span> <span
class="math inline">\(G^{DFP}\)</span>为DFP算法计算的<span
class="math inline">\(G_k\)</span>，<span
class="math inline">\(G^{BFGS}\)</span>为BFGS算法计算的<span
class="math inline">\(B_k\)</span>取逆。</p>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>优化算法</tag>
      </tags>
  </entry>
  <entry>
    <title>决策树</title>
    <url>/2022/12/27/4.%E5%86%B3%E7%AD%96%E6%A0%91/</url>
    <content><![CDATA[<p>​</p>
<span id="more"></span>
<h1 id="id3算法">ID3算法</h1>
<p>主要用来做分类问题</p>
<h2 id="特征选择算法">特征选择算法</h2>
<p>使用信息增益最大来做特征选择： <span class="math display">\[
Gain=H(Y)-H(Y|X)\\
H(Y)=-\sum_{k=1}^Kp_k\mathop{log_2}p_k\\
H(Y|X)=\sum_{l=1}^Lp_lH(Y|X=x_l)
\]</span> <span class="math inline">\(K\)</span>：表示标签<span
class="math inline">\(Y\)</span>有<span
class="math inline">\(K\)</span>个类别；<span
class="math inline">\(p_k\)</span>：表示标签<span
class="math inline">\(Y\)</span>的第<span
class="math inline">\(k\)</span>个类别的取值概率，<span
class="math inline">\(k=1,2,...,K\)</span>；<span
class="math inline">\(L\)</span>：表示特征<span
class="math inline">\(X\)</span>有<span
class="math inline">\(L\)</span>个类别；<span
class="math inline">\(x_l\)</span>：表示特征<span
class="math inline">\(X\)</span>的第<span
class="math inline">\(l\)</span>种取值，<span
class="math inline">\(l=1,2,...,L\)</span>；<span
class="math inline">\(p_l\)</span>：表示特征<span
class="math inline">\(X\)</span>的第<span
class="math inline">\(l\)</span>个类别的取值概率。</p>
<h2 id="决策树生成">决策树生成</h2>
<p>从根节点开始，遍历特征计算每一个特征的信息增益，选择信息增益最大的特征作为根节点。由该根节点特征的不同取值建立子节点，对每个子节点递归调用上述方法创建决策树。终止条件：</p>
<ul>
<li>无需分类：每个子节点包含的样本，其标签都属于同一个类别；</li>
<li>无法分类：每个子节点所包含的样本，其每一个特征都只有一种取值，或者特征已经用完。</li>
</ul>
<h2 id="决策树剪枝">决策树剪枝</h2>
<p>ID3无剪枝算法</p>
<h2 id="预测">预测</h2>
<p>样本会落到ID3决策树的一个叶子节点上，计算叶子节点中每种类别的频率，将该样本预测为频率最高的那个类别。</p>
<h2 id="缺点">缺点</h2>
<ul>
<li>无剪枝算法</li>
<li>只能处理分类特征，且特征无法复用</li>
<li>无法处理缺失值</li>
<li>偏好类别较多的特征</li>
<li>容易达到局部最优而非全局最优，容易过拟合</li>
</ul>
<h1 id="c4.5算法">C4.5算法</h1>
<p>主要用来做分类问题</p>
<h2 id="特征选择算法-1">特征选择算法</h2>
<p>使用信息增益率最大来做特征选择： <span class="math display">\[
Gain\_Ratio=\frac{H(Y)-H(Y|X)}{H(X)}\\
H(Y)=-\sum_{k=1}^Kp_k\mathop{log_2}p_k\\
H(X)=-\sum_{l=1}^Lp_l\mathop{log_2}p_l\\
H(Y|X)=\sum_{l=1}^Lp_lH(Y|X=x_l)
\]</span></p>
<h3 id="处理连续变量">处理连续变量</h3>
<p>用贪心算法选择连续变量的最优切分点对连续变量进行二分。</p>
<h3 id="处理缺失值">处理缺失值</h3>
<h4 id="训练集有缺失值">训练集有缺失值</h4>
<ol type="1">
<li><p>如何计算信息增益率？用无缺失样本计算信息增益率，然后乘以无缺失样本的比例。</p></li>
<li><p>缺失值对应的样本划分到那个子节点？划分到所有子节点，并赋予权重。权重等于缺失样本划分前的权重（初始所有样本权重为1）乘以各个子节点包含的无缺失样本量占父节点包含的无缺失样本量的比例。这个权重会影响后面的信息增益率的计算。</p></li>
</ol>
<h4 id="测试预测集有缺失值">测试/预测集有缺失值</h4>
<p>将缺失样本划分到所有子节点，然后统计缺失样本属于各个标签类别的概率，将缺失样本判为概率最大的类别。</p>
<h2 id="决策树生成-1">决策树生成</h2>
<p>从根节点开始，遍历特征计算每一个特征的信息增益率，选择信息增益率最大的特征作为根节点。由该根节点特征的不同取值建立子节点，对每个子节点递归调用上述方法创建决策树。终止条件：</p>
<ul>
<li>无需分类：每个子节点包含的样本，其标签都属于同一个类别；</li>
<li>无法分类：每个子节点所包含的样本，其每一个分类特征都只有一个类别，或者分类特征已经用完，而每一个连续特征的信息增益率都小于阈值。</li>
</ul>
<h2 id="决策树剪枝-1">决策树剪枝</h2>
<h3 id="预剪枝">预剪枝</h3>
<ul>
<li>树达到一定深度</li>
<li>信息增益率小于指定阈值</li>
<li>叶子节点包含的样本量小于阈值</li>
<li>叶子节点本身包括的样本量大于阈值，但是划分后任意一个子节点（变为叶子节点）的样本量小于阈值。</li>
</ul>
<h3 id="后剪枝">后剪枝</h3>
<p>先生成决策树之后再剪枝</p>
<p><span class="math display">\[
L=\sum_{t=1}^{|T|}n_tH_t(Y)+\alpha|T|
\]</span></p>
<p><span class="math inline">\(T\)</span>：表示一棵决策树；<span
class="math inline">\(|T|\)</span>：表示决策树<span
class="math inline">\(T\)</span>有<span
class="math inline">\(|T|\)</span>个叶子节点；<span
class="math inline">\(n_t\)</span>：表示决策树第<span
class="math inline">\(t\)</span>个叶子节点上的样本量，<span
class="math inline">\(t=1,2,...,|T|\)</span>；<span
class="math inline">\(H_t(Y)\)</span>：表示决策树第<span
class="math inline">\(t\)</span>个叶子节点上的标签<span
class="math inline">\(Y\)</span>的信息熵；<span
class="math inline">\(\alpha\)</span>为正则化参数；<span
class="math inline">\(L\)</span>表示损失。如果非叶子节点替换为叶子节点能使<span
class="math inline">\(L\)</span>减小，则进行替换，即剪枝。</p>
<h2 id="预测-1">预测</h2>
<p>样本会落到C4.5决策树的一个叶子节点上，计算叶子节点中每种类别的频率，将该样本预测为频率最高的那个类别。</p>
<h2 id="缺点-1">缺点</h2>
<ul>
<li>只能处理分类问题，不能处理回归问题</li>
<li>容易陷入局部最优，而非全局最优，容易过拟合</li>
<li>需要大量对数运算，计算成本较高</li>
</ul>
<h1 id="cart算法">CART算法</h1>
<p>可以用于分类，也可以用于回归。</p>
<h2 id="特征选择算法-2">特征选择算法</h2>
<h3 id="分类问题">分类问题</h3>
<p>分类问题使用基尼指数最小来做特征选择： <span class="math display">\[
Gini\_index(Y|X)=\sum_{l=1}^Lp_lGini(Y|X=x_l)\\
Gini(Y)=\sum_{k=1}^Kp_k(1-p_k)\\
\]</span></p>
<h3 id="回归问题">回归问题</h3>
<p>回归问题使用平方误差最小来做特征选择： <span class="math display">\[
SSE=\sum_{i=1}^n(y_i-\bar y)^2
\]</span> <span
class="math inline">\(n\)</span>为子节点的样本量。这个算法会让一个节点产生的两个子节点，各自计算平方误差和，然后加总，取最小值时对应的特征和切分点。</p>
<h3 id="处理连续变量-1">处理连续变量</h3>
<p>用贪心算法选择连续变量的最优切分点对连续变量进行二分。</p>
<h3 id="处理分类变量">处理分类变量</h3>
<p>同样采用二分的方法，将分类特征分为某个类别和不属于某个类别。分类特征有多个类别时，在树生成过程中这个特征也可以复用。</p>
<h3 id="处理缺失值-1">处理缺失值</h3>
<ul>
<li>如何计算gini值或平方误差？采用类似C4.5的算法，用无缺失样本计算，最后乘以无缺失样本的比例</li>
<li>缺失样本该划分到左节点还是右节点呢？基于代理特征，来决定缺失样本应该分到左节点还是右节点。代理特征即下一个对分裂最有用的特征，且其在当前样本未缺失。</li>
</ul>
<h2 id="决策树生成-2">决策树生成</h2>
<p>从根节点开始，遍历特征计算每一个特征的最小gini指数（分类问题）或者平方误差（回归问题），选择gini指数或平方误差最小的特征作为根节点。由该根节点二分建立子节点，对每个子节点递归调用上述方法创建决策树，直到满足终止条件。</p>
<h2 id="决策树剪枝-2">决策树剪枝</h2>
<h3 id="预剪枝-1">预剪枝</h3>
<ul>
<li>树达到一定深度</li>
<li>基尼指数/平方误差和不满足指定阈值</li>
<li>叶子节点包含的样本量小于阈值</li>
<li>叶子节点本身包括的样本量大于阈值，但是划分后任意一个子节点（变为叶子节点）的样本量小于阈值。</li>
</ul>
<h3 id="后剪枝-1">后剪枝</h3>
<p>定义损失函数为： <span class="math display">\[
L=C(T)+\alpha|T|
\]</span> <span
class="math inline">\(C(T)\)</span>为训练集的预测误差，对于分类问题为基尼指数，对于回归问题为平方误差。<span
class="math inline">\(\alpha\)</span>为正则化参数，<span
class="math inline">\(T\)</span>为一颗决策树，<span
class="math inline">\(|T|\)</span>为决策树<span
class="math inline">\(T\)</span>包含的叶子节点个数。</p>
<p>剪枝时，会从训练好的完整树开始，不断调整<span
class="math inline">\(\alpha\)</span>，来选择最优的树，一直剪枝到只有一个根节点的树，形成一个子树序列，然后用一个验证集使用交叉验证的方法从中选出最优的树。</p>
<h2 id="预测-2">预测</h2>
<p>样本会落到CART决策树的一个叶子节点上。对于分类问题，同样计算叶子节点中每种类别的频率，将该样本预测为频率最高的那个类别。对于回归问题，取叶子节点上包含的训练样本的平均值作为预测值。</p>
<h2 id="缺点-2">缺点</h2>
<ul>
<li>容易陷入局部最优，而非全局最优，容易过拟合。</li>
</ul>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>回归</tag>
        <tag>分类</tag>
      </tags>
  </entry>
  <entry>
    <title>集成学习</title>
    <url>/2022/12/28/5.%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/</url>
    <content><![CDATA[<p>​</p>
<span id="more"></span>
<h1 id="boosting">Boosting</h1>
<h2 id="adaboost">AdaBoost</h2>
<p>下一轮模型训练时会提高上一轮错误分类样本的权重。</p>
<h3 id="模型">模型</h3>
<p><span class="math inline">\(K\)</span>个基学习器构成的加法模型：
<span class="math display">\[
\hat y^K=sign(z^K)\\
z^K=\sum_{k=1}^K\alpha_kf_k(\vec x)\\
f_k(\vec x)\rightarrow \{-1,+1\},\alpha_k&gt;0
\]</span></p>
<p><span class="math inline">\(sign(a)\)</span>为符号函数，<span
class="math inline">\(a&gt;0\)</span>则<span
class="math inline">\(sign(a)\)</span>返回1，<span
class="math inline">\(a=0\)</span>则返回0，否则返回-1，<span
class="math inline">\(\alpha_k\)</span>为模型权重，要求每个基学习器<span
class="math inline">\(f_k(\vec x)\)</span>的输出为-1或1。</p>
<h3 id="损失">损失</h3>
<p>用指数函数<span class="math inline">\(y=e^{-\lambda
x}\)</span>来度量损失。第<span
class="math inline">\(t\)</span>轮的损失： <span class="math display">\[
L=\sum_{i=1}^ne^{-z_i^ty_i}\\
=\sum_{i=1}^ne^{-(z_i^{t-1}+\alpha_tf_t(\vec x_i))y_i}\\
=\sum_{i=1}^ne^{-z_i^{t-1}y_i}e^{-\alpha_tf_t(\vec x_i)y_i}
\]</span> 要寻找第<span class="math inline">\(t\)</span>轮时最优的<span
class="math inline">\(\alpha_t\)</span>和<span
class="math inline">\(f_t(\vec x)\)</span>，令： <span
class="math display">\[
w_i^{t}=e^{-z_i^{t-1}y_i}
\]</span> 则： <span class="math display">\[
L=\sum_{i=1}^nw_i^te^{-\alpha_tf_t(\vec x_i)y_i}
\]</span></p>
<h3 id="优化">优化</h3>
<p>因为<span class="math inline">\(y\rightarrow \{-1,+1\},f_t(\vec
x)\rightarrow\{-1,+1\}\)</span>，因此： <span class="math display">\[
L=\sum_{i=1}^nw_i^t(e^{-\alpha_t}I(y_i=f_t(\vec
x_i))+e^{\alpha_t}I(f_t(\vec x_i)\ne y_i))\\
=\sum_{i=1}^nw_i^t(e^{-\alpha_t}(1-I(y_i\ne f_t(\vec
x_i)))+e^{\alpha_t}I(f_t(\vec x_i)\ne y_i))\\
=\sum_{i=1}^nw_i^t(e^{-\alpha_t}-e^{-\alpha_t}I(f_t(\vec x_i)\ne
y_i)+e^{\alpha_t}I(f_t(\vec x_i)\ne y_i))\\
=\sum_{i=1}^nw_i^t(e^{-\alpha_t}+(e^{\alpha_t}-e^{-\alpha_t})I(f_t(\vec
x_i)\ne y_i))\\
=e^{-\alpha_t}\sum_{i=1}^nw_i^t+(e^{\alpha_t}-e^{-\alpha_t})\sum_{i=1}^nw_i^tI(f_t(\vec
x_i)\ne y_i)\\
\]</span> 将上面式子除以规范化因子<span
class="math inline">\(\sum_{i=1}^nw_i^t\)</span>，此时<span
class="math inline">\(\frac{\sum_{i=1}^nw_i^tI(f_t(\vec x_i)\ne
y_i)}{\sum_{i=1}^nw_i^t}\)</span>刚好为分类器<span
class="math inline">\(f_t(\vec x)\)</span>的分类错误率，令其为<span
class="math inline">\(\epsilon_t\)</span>，因此： <span
class="math display">\[
L=e^{-\alpha_t}+(e^{\alpha_t}-e^{-\alpha_t})\epsilon_t
\]</span> 先求<span class="math inline">\(f_t(\vec
x)\)</span>。由于<span
class="math inline">\(\alpha_t\gt0\)</span>，故而<span
class="math inline">\(e^{\alpha_t}-e^{-\alpha_t}\gt0\)</span>，一次函数的斜率为正，当<span
class="math inline">\(\epsilon_t\)</span>最小时，损失达到最小，因此最优的<span
class="math inline">\(f_t(\vec x)\)</span>要求最小化： <span
class="math display">\[
\mathop{min}\sum_{i=1}^nw_i^tI(f_t(\vec x_i)\ne y_i)
\]</span> 也就是说，最优的<span class="math inline">\(f_t(\vec
x)\)</span>是在权重为<span
class="math inline">\(w^t\)</span>时使得分类错误率最小时的那个分类器。</p>
<p>再求<span class="math inline">\(\alpha_t\)</span>。对<span
class="math inline">\(\alpha_t\)</span>求偏导： <span
class="math display">\[
\frac{\partial L}{\partial
\alpha_t}=e^{-\alpha_t}(-1)+e^{\alpha_t}\epsilon_t-e^{-\alpha_t}(-1)\epsilon_t\\
=e^{\alpha_t}\epsilon_t+e^{-\alpha_t}\epsilon_t-e^{-\alpha_t}\\
\]</span> 令导数为0： <span class="math display">\[
e^{\alpha_t}\epsilon_t=e^{-\alpha_t}(1-\epsilon_t)\\
\alpha_t+ln\epsilon_t=-\alpha_t+ln(1-\epsilon_t)\\
2\alpha_t=ln(\frac{1-\epsilon_t}{\epsilon_t})\\
\alpha_t=\frac{1}{2}ln(\frac{1-\epsilon_t}{\epsilon_t})
\]</span> 回头再观察一下第<span
class="math inline">\(t\)</span>轮时的权重： <span
class="math display">\[
w_i^{t}=e^{-z_i^{t-1}y_i}\\
=e^{-(z_i^{t-2}+\alpha_{t-1}f_{t-1}(\vec x_i))y_i}\\
=w_i^{t-1}e^{-\alpha_{t-1}f_{t-1}(\vec x_i)y_i}
\]</span> 第<span class="math inline">\(t-1\)</span>轮<span
class="math inline">\(f_{t-1}(\vec x)\)</span>模型分对的样本，<span
class="math inline">\(t\)</span>轮时的权重变为<span
class="math inline">\(t-1\)</span>轮时权重的<span
class="math inline">\(e^{-\alpha_{t-1}}\lt1\)</span>倍，即减小了权重。第<span
class="math inline">\(t-1\)</span>轮模型<span
class="math inline">\(f_{t-1}(\vec x)\)</span>分错的样本，<span
class="math inline">\(t\)</span>轮时权重变为<span
class="math inline">\(t-1\)</span>轮时的<span
class="math inline">\(e^{\alpha_{t-1}}\gt
1\)</span>倍，即增大了权重。</p>
<h2 id="gbm">GBM</h2>
<p>下一轮模型拟合上一轮损失函数的负梯度。</p>
<h3 id="模型-1">模型</h3>
<p><span class="math display">\[
\hat y^K = \sum_{i=1}^Kf_k(\vec x)
\]</span></p>
<p>也可以给每一个<span class="math inline">\(f_k(\vec
x)\)</span>加一个权重。</p>
<h3 id="损失-1">损失</h3>
<p>根据学习任务（分类或回归），选择合适的损失。第<span
class="math inline">\(t\)</span>轮的损失为： <span
class="math display">\[
L=\sum_{i=1}^nl(y_i,\hat y_i^t)
\]</span></p>
<h3 id="优化-1">优化</h3>
<p>用到了泰勒一阶展开： <span class="math display">\[
L=\sum_{i=1}^nl(y_i,\hat y_i^{t-1}+f_t(\vec x_i))\\
\approx\sum_{i=1}^n[l(y_i,\hat y_i^{t-1})+l\prime(y_i,\hat
y_i^{t-1})f_t(\vec x_i)]
\]</span> 为使损失最小，可令： <span class="math display">\[
f_t(\vec x_i)=-l\prime(y_i,\hat y_i^{t-1})
\]</span> 也就是说GBM中，下一轮模型<span class="math inline">\(f_t(\vec
x_i)\)</span>只需要拟合损失函数在上一轮预测值处的负梯度即可。</p>
<h3 id="举例">举例</h3>
<p>下面举几个特例：</p>
<ol type="1">
<li>回归问题</li>
</ol>
<p>回归问题一般用平方损失： <span class="math display">\[
l(y,\hat y^{t-1})=\frac{1}{2}(y-\hat y^{t-1})^2
\]</span> 负梯度为： <span class="math display">\[
-\frac{\partial l(y,\hat y^{t-1})}{\partial \hat y^{t-1}}=y-\hat y^{t-1}
\]</span> 此时下一轮模型只需要拟合上一轮的残差即可。</p>
<ol start="2" type="1">
<li>分类问题</li>
</ol>
<p>分类问题可以用交叉熵损失。例如二分类问题，首先加法模型先经过sigmoid函数转换，然后转换值放到交叉熵损失中：
<span class="math display">\[
p=\frac{1}{1+e^{-\hat y^{t-1}}}
\]</span></p>
<p><span class="math display">\[
l(y,p)=-y\mathop{ln}p-(1-y)\mathop{ln}(1-p)
\]</span></p>
<p>求导： <span class="math display">\[
\frac{\partial l(y,p)}{\partial p}=\frac{p-y}{p(1-p)}
\]</span></p>
<p><span class="math display">\[
\frac{\partial p}{\partial \hat y^{t-1}}=p(1-p)
\]</span></p>
<p>负梯度为： <span class="math display">\[
\frac{\partial l(y,\hat y^{t-1})}{\partial \hat
y^{t-1}}=y-p=y-\frac{1}{1+e^{-\hat y^{t-1}}}
\]</span> 某种程度上也可以看作是拟合上一轮的残差。</p>
<h2 id="gbdt">GBDT</h2>
<p>其实就是GBM，只不过基学习器使用的是Cart树。</p>
<h2 id="xgboost">XGBoost</h2>
<p>比GBM更加深入，用到了泰勒二阶展开，也引入了正则化项。</p>
<h3 id="模型-2">模型</h3>
<p><span class="math display">\[
\hat y^K=\sum_{k=1}^Kf_k(\vec x)
\]</span></p>
<p>也可以给每一个<span class="math inline">\(f_k(\vec
x)\)</span>加一个权重。</p>
<h3 id="损失-2">损失</h3>
<p>第<span class="math inline">\(t\)</span>轮的损失 <span
class="math display">\[
L=\sum_{i=1}^nl(y_i,\hat y_i^t)+\sum_{k=1}^t\Omega(f_k)\\
\]</span></p>
<h3 id="优化-2">优化</h3>
<p><span class="math display">\[
L=\sum_{i=1}^nl(y_i,\hat y_i^{t-1}+f_t(\vec
x_i))+\sum_{k=1}^t\Omega(f_k)\\
\approx\sum_{i=1}^n[l(y_i,\hat y_i^{t-1})+g_if_t(\vec
x_i)+\frac{1}{2}h_if_t^2(\vec x_i)]+\sum_{k=1}^t\Omega(f_k)\\
g_i=\frac{\partial l(y_i,\hat y_i^{t-1})}{\partial \hat
y_i^{t-1}},h_i=\frac{\partial^2 l(y_i,\hat y_i^{t-1})}{\partial \hat
y_i^{t-1}\partial \hat y_i^{t-1}}\\
\rightarrow\\
L=\sum_{i=1}^n[g_if_t(\vec x_i)+\frac{1}{2}h_if_t^2(\vec
x_i)]+\Omega(f_t)
\]</span> ### 举例</p>
<p>以基学习器为cart树的回归任务为例。令树有<span
class="math inline">\(T\)</span>个叶子，每个叶子节点上取值为<span
class="math inline">\(w_j,j=1,2,...,j,...,T\)</span>，令： <span
class="math display">\[
\Omega (f_t)=\gamma
T+\frac{1}{2}\lambda\sum_{j=1}^Tw_j^2+\frac{1}{2}\alpha
\sum_{j=1}^T|w_j|
\]</span> 取： <span class="math display">\[
\Omega(f_t)=\gamma T+\frac{1}{2}\lambda \sum_{j=1}^Tw_j^2
\]</span> 损失： <span class="math display">\[
L=\sum_{j=1}^T[w_j\sum_{i\in I_j}g_i+\frac{1}{2}w_j^2\sum_{i\in
I_j}h_i]+\gamma T+\frac{1}{2}\lambda \sum_{j=1}^Tw_j^2\\
=\sum_{j=1}^T[w_j\sum_{i\in I_j}g_i+\frac{1}{2}w_j^2(\sum_{i\in
I_j}h_i+\lambda)]+\gamma T\\
=\sum_{j=1}^T[w_jG_j+\frac{1}{2}w_j^2(H_j+\lambda)]+\gamma T\\
\rightarrow\\
w_j=-\frac{G_j}{H_j+\lambda}\\
G_j=\sum_{i\in I_j}g_i,H_j=\sum_{i\in I_j}h_i
\]</span> 回代： <span class="math display">\[
L=\sum_{j=1}^T[-\frac{G_j^2}{H_j+\lambda}+\frac{1}{2}\frac{G_j^2}{(H_j+\lambda)^2}(H_j+\lambda)]+\gamma
T\\
=-\frac{1}{2}\sum_{j=1}^T\frac{G_j^2}{H_j+\lambda}+\gamma T
\]</span> 特征选择： <span class="math display">\[
gain=-\frac{1}{2}\frac{G^2}{H+\lambda}+\gamma-(-\frac{1}{2}\frac{G^2_l}{H_l+\lambda}+\gamma-\frac{1}{2}\frac{G_r^2}{H_r+\lambda}+\gamma)\\
=\frac{1}{2}(\frac{G_l^2}{H_l+\lambda}+\frac{G_r^2}{H_r+\lambda}-\frac{G^2}{H+\lambda})-\gamma\\
=\frac{1}{2}(\frac{G_l^2}{H_l+\lambda}+\frac{G_r^2}{H_r+\lambda}-\frac{(G_l+G_r)^2}{H_l+H_r+\lambda})-\gamma
\]</span></p>
<h1 id="bagging">Bagging</h1>
<p>全称是booststrap
aggregating。基本思想是通过bootstrap抽样方法从原始数据集中抽取多个训练集，每个训练集训练一个模型，最后对多个模型的结果进行组合。对于回归任务采用平均法，对于分类任务采用投票法。bootstrap抽样时，原始数据集中约有36.8%的数据不会被抽到，可用做测试数据来估计bagging模型的泛化误差。</p>
<h2 id="随机森林">随机森林</h2>
<p>是bagging的一个实例，只不过随机森林的基学习器为决策树，bagging模型可以任选基学习器。另外，随机森林引入了输入属性扰动，它允许决策树每个节点做特征选择前可以随机抽取一个特征子集，仅仅从这个子集中选择一个最优的特征。</p>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>回归</tag>
        <tag>分类</tag>
      </tags>
  </entry>
  <entry>
    <title>似然理论综述</title>
    <url>/2022/12/30/7.%E4%BC%BC%E7%84%B6%E7%90%86%E8%AE%BA%E7%BB%BC%E8%BF%B0/</url>
    <content><![CDATA[<p>​</p>
<span id="more"></span>
<h1 id="最大似然估计的基本思想">最大似然估计的基本思想</h1>
<p>记随机变量<span class="math inline">\(Y\)</span>的概率密度函数为<span
class="math inline">\(f(y_i;\vec w)\)</span>，其中<span
class="math inline">\(\vec w\)</span>为概率密度函数中的参数向量，<span
class="math inline">\(\vec w\in R^p\)</span>。现在<span
class="math inline">\(\vec
w\)</span>未知需要进行估计，可抽取包括n个观测数据的随机样本<span
class="math inline">\(\{y_1,y_2,...y_n\}\)</span>，用样本来估计参数<span
class="math inline">\(\vec w\)</span>。记这<span
class="math inline">\(n\)</span>个观测数据的联合密度为： <span
class="math display">\[
L(\vec w;y_1,y_2,...,y_n)=\prod_{i=1}^nf(y_i;\vec w)
\]</span> 在这个表达式中，<span
class="math inline">\(\{y_1,y_2,...y_n\}\)</span>已经通过抽样获得，是已知的，而<span
class="math inline">\(\vec
w\)</span>是未知的，因此可以将表达式看作给定<span
class="math inline">\(\{y_1,y_2,...y_n\}\)</span>后关于未知参数<span
class="math inline">\(\vec
w\)</span>的函数，这个函数被称为似然函数。由于参数<span
class="math inline">\(\vec
w\)</span>是未知的，因此似然函数衡量的是在不同的<span
class="math inline">\(\vec w\)</span>取值下出现当前随机样本<span
class="math inline">\(\{y_1,y_2,...y_n\}\)</span>的可能性。究竟哪个<span
class="math inline">\(\vec
w\)</span>导致了当前样本的出现呢？我们可以认为使当前样本出现可能性最大的那个<span
class="math inline">\(\vec w\)</span>就是最有可能的<span
class="math inline">\(\vec
w\)</span>。因此我们需要做的就是使可能性达到最大，即最大化上述的似然函数：
<span class="math display">\[
max \,L(\vec w;y_1,y_2,...,y_n)
\]</span>
这就是最大似然估计。由于似然函数是一个乘积的形式，不便进行运算，常对其取对数转换为求和的形式，因此最大似然最终往往转换为最大化对数似然，即：
<span class="math display">\[
max\, ln L(\vec w;y_1,y_2,...,y_n)
\]</span> 记最大似然估计的估计值为<span class="math inline">\(\hat {\vec
w}_{ML}\)</span>，常写为： <span class="math display">\[
\hat{\vec w}_{ML}=argmax\,lnL(\vec w;\vec y)
\]</span> 这里的<span class="math inline">\(argmax\)</span>表示使得<span
class="math inline">\(lnL(\vec w,\vec y)\)</span>取得最大时的<span
class="math inline">\(\vec
w\)</span>的取值。这里额外提一句，对数似然函数往往就是机器学习中的损失函数。</p>
<h1 id="最大似然估计的求解">最大似然估计的求解</h1>
<p>最大似然估计的求解并没有什么不同，我们对对数似然函数求一阶导数得到梯度向量<span
class="math inline">\(\vec u(\vec w)\)</span>： <span
class="math display">\[
\vec u(\vec w)=\frac{\partial lnL(\vec w;\vec y)}{\partial \vec w}
\]</span>
如果似然函数是凹函数，就可以令梯度向量为0，从而得到最大似然估计值<span
class="math inline">\(\hat{\vec
w}_{ML}\)</span>。有时候梯度向量是一个非线性函数，此时可以使用我们熟悉的迭代法求解。即先给<span
class="math inline">\(\vec
w\)</span>一个初始值，然后一轮一轮迭代，直到达到收敛条件。如果使用牛顿法，则第<span
class="math inline">\(k+1\)</span>轮牛顿迭代公式为： <span
class="math display">\[
\vec w_{k+1}=\vec w_k-H(\vec w_k)^{-1}\vec u(\vec w_k)
\]</span></p>
<p><span class="math inline">\(H(\vec w_k)\)</span>为<span
class="math inline">\(\vec
w_k\)</span>处的hessian矩阵，即对数似然函数的二阶导数。</p>
<h1 id="最大似然估计的性质">最大似然估计的性质</h1>
<p>最大似然估计之所以常用，在于其有良好的性质。在介绍这些性质之前，需要再补充两个概念：得分函数和信息矩阵。</p>
<h2 id="得分函数">得分函数</h2>
<p>对数似然函数的一阶导数即梯度向量<span class="math inline">\(\vec
u(\vec
w)\)</span>有另外的名称：得分函数或得分向量。它有下面几个特点：</p>
<ul>
<li>得分函数是一个随机向量。这是因为我们是使用样本来估计参数<span
class="math inline">\(\vec
w\)</span>的，我们得到的样本只是抽样后的一次具体实现值，在抽样前为随机向量，所以得分函数是随机向量；</li>
<li>得分函数是参数<span class="math inline">\(\vec
w\)</span>的函数，这一点无需多说；</li>
<li>既然得分函数是随机向量，自然有期望和协方差矩阵；</li>
<li>当参数<span class="math inline">\(\vec w\)</span>取真实值<span
class="math inline">\(\vec w_0\)</span>时，得分函数的期望为0；</li>
<li>得分函数的协方差矩阵为信息矩阵。</li>
</ul>
<h2 id="信息矩阵">信息矩阵</h2>
<p>得分函数的协方差矩阵为信息矩阵，如参数<span
class="math inline">\(\vec w\)</span>取真实值<span
class="math inline">\(\vec w_0\)</span>时，信息矩阵为： <span
class="math display">\[
var[\vec u(\vec w_0)]=E[(\vec u(\vec w_0)-\vec 0)(\vec u(\vec w_0)-\vec
0)^T]\\
=E[\vec u(\vec w_0)(\vec u(\vec w_0))^T]\\
=I(\vec w_0)
\]</span>
称这个矩阵为信息矩阵。信息矩阵有什么作用呢？后面我们就会看到，信息矩阵与最大似然估计值的渐进方差有关。信息矩阵由于做了期望运算，因此不再是随机矩阵。信息矩阵往往还等于对数似然函数二阶导数期望的相反数：
<span class="math display">\[
I(\vec w_0)=-E[H(\vec w_0)]
\]</span>
这两个公式都是理论计算公式，实际中我们还是需要用样本来估计信息矩阵。常用的计算信息矩阵的方法有两种：</p>
<h3 id="梯度向量外积法">梯度向量外积法</h3>
<p>先用牛顿迭代法得到最大似然估计值<span class="math inline">\(\vec
w_{ML}\)</span>，进一步得到<span class="math inline">\(\vec
w_{ML}\)</span>处的梯度向量，然后对梯度向量做外积，得到信息矩阵的估计值。</p>
<h3 id="观测信息矩阵法">观测信息矩阵法</h3>
<p>先用牛顿迭代法得到最大似然估计值<span class="math inline">\(\vec
w_{ML}\)</span>，进一步得到<span class="math inline">\(\vec
w_{ML}\)</span>处的hessian矩阵，对hessian矩阵直接取相反数得到信息矩阵的估计值。</p>
<h2 id="最大似然估计的性质-1">最大似然估计的性质</h2>
<p>有了上述两个概念为基础，下面介绍一下最大似然估计的几个性质：</p>
<h3 id="一致性">一致性</h3>
<p>一致性是指在大样本下，最大似然估计值<span
class="math inline">\(\hat{\vec w}_{ML}\)</span>依概率收敛到真实值<span
class="math inline">\(\vec w_0\)</span>，即： <span
class="math display">\[
\hat{\vec w}_{ML}\mathop{\rightarrow}^{p}\vec w_0
\]</span> 一致性告诉我们，随着样本量逐渐增大，最大似然估计值<span
class="math inline">\(\hat{\vec w}_{ML}\)</span>等于真实值<span
class="math inline">\(\vec w_0\)</span>的概率越来越接近1。</p>
<h3 id="渐进正态">渐进正态</h3>
<p>渐进正态是指在大样本下，最大似然估计值<span
class="math inline">\(\hat{\vec
w}_{ML}\)</span>依分布收敛于正态分布，即： <span class="math display">\[
\sqrt n(\hat{\vec w}_{ML}-\vec w_0)\mathop{\rightarrow}^dN(0,nI(\vec
w_0)^{-1})
\]</span> 这里的<span class="math inline">\(nI(\vec
w_0)^{-1}\)</span>是正态分布的方差协方差矩阵，称为最大似然估计的渐进方差。这个性质告诉我们，随着样本量增大，最大似然估计值<span
class="math inline">\(\hat{\vec
w}_{ML}\)</span>的概率分布越来越接近正态分布。这个性质也告诉我们最大似然估计值<span
class="math inline">\(\hat{\vec w}_{ML}\)</span>的渐进方差为： <span
class="math display">\[
I(\vec w_0)^{-1}
\]</span> 也就是说最大似然估计值<span class="math inline">\(\hat{\vec
w}_{ML}\)</span>的渐进方差为信息矩阵<span class="math inline">\(I(\vec
w_0)\)</span>的逆。</p>
<h3 id="有效性">有效性</h3>
<p>这个性质是指在大样本下，最大似然估计值<span
class="math inline">\(\hat{\vec
w}_{ML}\)</span>的方差达到了克莱默-劳下限。简而言之，可以理解为最大似然估计的方差是最小的。</p>
<h1 id="最大似然估计的假设检验">最大似然估计的假设检验</h1>
<p>在涉及统计推断时，会需要对参数估计值进行假设检验，这里顺带也回顾一下常见的几种假设检验方法。令无效假设为：
<span class="math display">\[
H_0:\hat{\vec w}_{ML}=\vec w_s
\]</span> 这里的<span class="math inline">\(\vec
w_s\)</span>是已知的，常见如<span class="math inline">\(\vec
0\)</span>。</p>
<h2 id="wald检验">wald检验</h2>
<p>根据上面介绍的最大似然估计的基本性质，我们已经知道在大样本下，最大似然估计值<span
class="math inline">\(\hat{\vec
w}_{ML}\)</span>近似服从均数为真实值<span class="math inline">\(\vec
w_0\)</span>，方差协方差矩阵为信息矩阵逆矩阵的正态分布： <span
class="math display">\[
\hat{\vec w}_{ML}\sim N(\vec w_0,I(\vec w_0)^{-1})
\]</span> <span class="math inline">\(I(\vec
w_0)^{-1}\)</span>通常难以获得，故采用<span
class="math inline">\(I(\hat{\vec w}_{ML})^{-1}\)</span>来估计。</p>
<p>wald检验的思路其实很简单，其实就是借助正态分布来做假设检验。因为<span
class="math inline">\(\vec w_s\)</span>是已知的常数向量，若<span
class="math inline">\(H_0\)</span>假设成立，则<span
class="math inline">\(\hat{\vec w}_{ML}-\vec
w_s\)</span>将同样近似服从正态分布。因此，可构造如下二次型统计量： <span
class="math display">\[
W =(\hat{\vec w}_{ML}-\vec w_s)^T[var(\hat{\vec w}_{ML})]^{-1}(\hat{\vec
w}_{ML}-\vec w_s)\\
=(\hat{\vec w}_{ML}-\vec w_s)^TI(\hat{\vec w}_{ML})(\hat{\vec
w}_{ML}-\vec w_s)
\]</span> 它将服从自由度为<span
class="math inline">\(p\)</span>的卡方分布，<span
class="math inline">\(p\)</span>为参数向量的维度。</p>
<h2 id="似然比检验">似然比检验</h2>
<p>似然比检验的基本思路为：将<span class="math inline">\(\hat{\vec
w}_{ML}\)</span>和<span class="math inline">\(\vec
w_s\)</span>分别带入对数似然函数公式求得对数似然函数值，若<span
class="math inline">\(H_0\)</span>成立，二者应该相差不大，若二者相差很大，则有理由拒绝<span
class="math inline">\(H_0\)</span>假设。为此构造如下统计量： <span
class="math display">\[
LR=2(lnL(\hat{\vec w}_{ML})-lnL(\vec w_s))
\]</span> 可以证明，该统计量同样服从自由度为<span
class="math inline">\(p\)</span>的卡方分布。</p>
<h2 id="得分检验">得分检验</h2>
<p>从得分检验名称可以看出，这个检验应该和得分向量有关，事实上确实如此。得分检验的基本思路如下：由于对数似然函数在估计值<span
class="math inline">\(\hat {\vec
w}_{ML}\)</span>处的得分函数为0，若<span
class="math inline">\(H_0\)</span>成立，对数似然函数在<span
class="math inline">\(\vec
w_s\)</span>处的得分函数也应该接近0。得分函数的<span
class="math inline">\(\vec w_s\)</span>的渐进方差可以用<span
class="math inline">\(I(\vec
w_s)\)</span>来估计，注意这里没有取逆，因此可以构造如下二次型统计量：
<span class="math display">\[
LM=\frac{\partial ln L(\vec w_s)}{\partial \vec w}^T[I(\vec
w_s)]^{-1}\frac{\partial ln L(\vec w_s)}{\partial \vec w}
\]</span> 可以证明，上述统计量同样服从自由度为<span
class="math inline">\(p\)</span>的卡方分布。</p>
<h1 id="总结">总结</h1>
<p>最后我们对本文内容做一下总结。似然函数的表达式在形式上等于联合密度函数，只不过是数据已知，参数未定，通过最大化对数似然来得到参数估计值。最大似然估计值的一阶导数为梯度向量，梯度向量为随机向量，协方差矩阵为信息矩阵。若密度函数设定是正确的，最大似然估计值在大样本下服从均数为0，协方差矩阵为信息矩阵逆矩阵的正态分布，信息矩阵常用样本数据对数似然函数在最大似然估计值处的hessian矩阵取负，或者梯度向量外积来估计。常用的基于最大似然的假设检验方法为wald检验、似然比检验和得分检验。</p>
]]></content>
      <categories>
        <category>数理统计</category>
      </categories>
      <tags>
        <tag>数理统计</tag>
      </tags>
  </entry>
  <entry>
    <title>生存分析</title>
    <url>/2022/12/29/6.%E7%94%9F%E5%AD%98%E5%88%86%E6%9E%90/</url>
    <content><![CDATA[<p>​</p>
<span id="more"></span>
<h1 id="常用函数">常用函数</h1>
<h2 id="分布函数">分布函数</h2>
<p><span class="math display">\[
F(t)=P(T\le t)
\]</span></p>
<h2 id="密度函数">密度函数</h2>
<p><span class="math display">\[
f(t)=F\prime(t)
\]</span></p>
<h2 id="生存函数">生存函数</h2>
<p><span class="math display">\[
S(t)=1-F(t)
\]</span></p>
<h2 id="风险函数">风险函数</h2>
<p><span class="math display">\[
h(t)=\frac{f(t)}{S(t)}
\]</span></p>
<h2 id="累积风险函数">累积风险函数</h2>
<p><span class="math display">\[
H(t)=\int_{0}^th(u)du=-lnS(t)
\]</span></p>
<h2 id="对数似然函数">对数似然函数</h2>
<p><span class="math display">\[
L=\prod_{i=1}^nf(t_i)^{y_i}S(t_i)^{1-y_i}\\
lnL=\sum_{i=1}^n[y_ilnf(t_i)+(1-y_i)lnS(t_i)]\\
=\sum_{i=1}^n[y_ilnh(t_i)+y_ilnS(t_i)+lnS(t_i)-y_ilnS(t_i)]\\
=\sum_{i=1}^n[y_ilnh(t_i)-H(t_i)]
\]</span></p>
<p><span class="math inline">\(y_i\)</span>标记了是否删失，0为删失。</p>
<h1 id="非参数方法">非参数方法</h1>
<h2 id="生存函数的kaplan-meier估计">生存函数的Kaplan-Meier估计</h2>
<p>记有<span
class="math inline">\(J\)</span>个生存时间的完整数据，且已经排好序：<span
class="math inline">\(t_1,t_2,...,t_j,...t_J\)</span>，<span
class="math inline">\([t_{j-1},t_j)\)</span>期间尚存人数为<span
class="math inline">\(n_j\)</span>，<span
class="math inline">\(t_j\)</span>时死亡人数为<span
class="math inline">\(d_j\)</span>，删失人数为<span
class="math inline">\(c_j\)</span>，尚存人数为<span
class="math inline">\(n_{j+1}\)</span>，有<span
class="math inline">\(n_j=d_j+c_j+n_{j+1}\)</span>。则Kaplan-Meier估计为：
<span class="math display">\[
\hat S(t)=\prod_{j:t_j\le t}\frac{n_j-d_j}{n_j}
\]</span></p>
<h2
id="累积风险函数的nelson-aalen估计">累积风险函数的Nelson-Aalen估计</h2>
<p>使用Nelson-Aalen估计量： <span class="math display">\[
\hat H(t)=\sum_{j:t_j\le t}\frac{d_j}{n_j}
\]</span> 风险函数估计为： <span class="math display">\[
\hat h(t_j)=\frac{d_j}{n_j}
\]</span> 风险函数的方差估计为： <span class="math display">\[
\hat {Var}(\hat
h(t_j))=\frac{\frac{d_j}{n_j}(1-\frac{d_j}{n_j})}{n_j}=\frac{d_j(n_j-d_j)}{n_j^3}
\]</span></p>
<h2
id="km估计量渐进方差的greenwood估计">KM估计量渐进方差的Greenwood估计</h2>
<p>已知： <span class="math display">\[
ln\hat S(t)=\sum_{j:t_j\le t}ln(1-\hat h(t_j))
\]</span> 令： <span class="math display">\[
g=ln(1-\hat h(t_j))
\]</span> 由delta法： <span class="math display">\[
\hat{Var}(ln\hat S(t))\sim \sum_{j:t_j\le t}\hat{Var(\hat
h(t_j))}[g\prime]^2\\
=\sum_{j:t_j\le t}\frac{d_j(n_j-d_j)}{n_j^3}[\frac{1}{1-\hat
{h}(t_j)}(-1)]^2\\
=\sum_{j:t_j\le t}\frac{d_j(n_j-d_j)}{n_j^3}\frac{n_j^2}{(n_j-d_j)^2}\\
=\sum_{j:t_j\le t}\frac{d_j}{n_j(n_j-d_j)}
\]</span> 又有delta法： <span class="math display">\[
\hat{Var}(ln\hat{S}(t))\sim \hat{Var}(\hat S(t))[\frac{1}{\hat S(t)}]^2
\]</span> 因此： <span class="math display">\[
\hat{Var}(\hat S(t))=[\hat S(t)]^2\hat{Var}(ln\hat S(t))\\
=[\hat S(t)]^2\sum_{j:t_j\le t}\frac{d_j}{n_j(n_j-d_j)}
\]</span></p>
<h2 id="log-rank检验">Log-rank检验</h2>
<p>思路：任意一组<span class="math inline">\(i(i=1,2)\)</span>在<span
class="math inline">\(t_j\)</span>时的死亡人数服从超几何分布<span
class="math inline">\(H(n_j,n_{ij},d_{j})\)</span>。<span
class="math inline">\(n_j\)</span>为<span
class="math inline">\([t_{j-1},t_j)\)</span>风险集中的总人数，<span
class="math inline">\(n_{ij}\)</span>为<span
class="math inline">\([t_{j-1},t_j)\)</span>时第<span
class="math inline">\(i\)</span>组风险集人数，<span
class="math inline">\(d_{j}\)</span>为<span
class="math inline">\(t_j\)</span>时死亡总人数。任取一组如<span
class="math inline">\(i=1\)</span>，超几何分布的期望和方差为： <span
class="math display">\[
E_{1j}=\frac{n_{1j}d_{j}}{n_j}\\
V_{1j}=\frac{n_{1j}d_{j}}{n_j}(1-\frac{d_{j}}{n_j})\frac{n_j-n_{1j}}{n_j-1}
\]</span> 检验统计量： <span class="math display">\[
Z=\frac{\sum_{j=1}^J(d_{1j}-E_{1j})}{\sqrt{\sum_{j=1}^JV_{1j}}}\mathop{\rightarrow}^dN(0,1)
\]</span>
wilcoxon检验、Tarone-Ware、和Peto等加权log-rank检验只是在log-rank检验中加入了不同权重。</p>
<h1 id="参数模型">参数模型</h1>
<h2 id="加速失效时间模型">加速失效时间模型</h2>
<h3 id="结构">结构</h3>
<p><span class="math display">\[
ln T = \vec x^T\vec w+\epsilon\\
T=e^{\vec x^T\vec w+\epsilon}\\
T=T_0e^{\vec x^T\vec w}
\]</span></p>
<p>意义：<span class="math inline">\(\vec x^T\vec
w\)</span>延长或缩短了基线生存时间<span
class="math inline">\(T_0\)</span>。</p>
<h3 id="常见模型">常见模型</h3>
<table>
<thead>
<tr class="header">
<th>模型</th>
<th>ε的分布</th>
<th>T的分布</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>指数回归</td>
<td>标准Ⅰ型极值分布</td>
<td>指数分布</td>
</tr>
<tr class="even">
<td>威布尔回归</td>
<td>Ⅰ型极值分布</td>
<td>威布尔分布</td>
</tr>
<tr class="odd">
<td>对数正态模型</td>
<td>正态分布</td>
<td>对数正态分布</td>
</tr>
<tr class="even">
<td>对数logistic模型</td>
<td>logistic分布</td>
<td>对数logistic分布</td>
</tr>
</tbody>
</table>
<h2 id="比例风险模型">比例风险模型</h2>
<h3 id="结构-1">结构</h3>
<p><span class="math display">\[
h(t)=h_0(t)e^{\vec x^T\vec w}
\]</span></p>
<p>意义：<span class="math inline">\(\vec x^T\vec
w\)</span>增大或缩小了基线风险<span
class="math inline">\(h_0(t)\)</span>，且影响程度不随时间发生变化。</p>
<h3 id="检验">检验</h3>
<p>用生存函数的对数负对数图 <span class="math display">\[
S(t)=e^{-H(t)}\\
=e^{-\int_0^th(u)du}\\
=e^{-\int_0^th_0(u)e^{\vec x^T\vec w}du}\\
=[e^{-\int_0^th_0(u)du}]^{e^{\vec x^T\vec w}}\\
=[S_0(t)]^{e^{\vec x^T\vec w}}
\]</span> 取负对数： <span class="math display">\[
-lnS(t)=-e^{\vec x^T\vec w}lnS_0(t)\gt0
\]</span> 再取对数： <span class="math display">\[
ln[-lnS(t)]=\vec x^T\vec w+ln[-lnS_0(t)]
\]</span>
因此，若比例风险假定成立，对某变量的不同取值画生存函数，生存函数将互相平行。</p>
<h3 id="修正">修正</h3>
<p>违背比例风险假定时的修正方案有：</p>
<p>方案一：引入随时间变化的变量 <span class="math display">\[
h(t)=h_0(t)e^{\vec x^T(t)\vec w}
\]</span> 方案二：引入随时间变化的参数 <span class="math display">\[
h(t)=h_0(t)e^{\vec x^T(\vec w+\vec\gamma^T\vec g(t))}
\]</span> <span class="math inline">\(g\)</span>为时间<span
class="math inline">\(t\)</span>的函数，如对数函数等。</p>
<h3 id="指数回归">指数回归</h3>
<p>指数分布为： <span class="math display">\[
F(t)=1-e^{-\lambda t},\lambda\gt 0\\
f(t)=\lambda e^{-\lambda t}\\
S(t)=e^{-\lambda t}\\
h(t)=\lambda\\
H(t)=\lambda t
\]</span> 指数回归为对<span
class="math inline">\(\lambda\)</span>进行回归： <span
class="math display">\[
\lambda = e^{\vec x^T\vec w}
\]</span> 对数似然函数为： <span class="math display">\[
lnL=\sum_{i=1}^n[y_ilne^{\vec x_i^T\vec w}-e^{\vec x_i^T\vec w}t_i]\\
=\sum_{i=1}^n[y_i\vec x_i^T\vec w-e^{\vec x_i^T\vec w}t_i]\\
=\vec y^TX\vec w-\vec t^Te^{X\vec w}
\]</span> 对数似然函数的微分： <span class="math display">\[
dlnL=\vec y^TXd\vec w-\vec t^Tde^{X\vec w}\\
=\vec y^TXd\vec w-\vec t^T(e^{X\vec w}\odot(Xd\vec w))\\
=tr(\vec y^TXd\vec w-[\vec t\odot e^{X\vec w}]^TXd\vec w)\\
=tr([\vec y-\vec t\odot e^{X\vec w}]^TXd\vec w)\\
=tr([X^T(\vec y-\vec t\odot e^{X\vec w})]^Td\vec w)
\]</span> <span
class="math inline">\(\odot\)</span>为向量或矩阵对应元素相乘，<span
class="math inline">\(tr\)</span>为计算矩阵的迹。梯度为： <span
class="math display">\[
\nabla_{\vec w}lnL=X^T(\vec y-\vec t\odot e^{X\vec w})
\]</span> 梯度微分： <span class="math display">\[
d\nabla_{\vec w}lnL=-X^Td(\vec t\odot e^{X\vec w})\\
=-X^T(\vec t\odot de^{X\vec w})\\
=-X^T(\vec t\odot(e^{X\vec w}\odot (Xd\vec w)))\\
=-X^T(\vec t\odot (diag(e^{X\vec w})Xd\vec w))\\
=-X^Tdiag(\vec t)diag(e^{X\vec w})Xd\vec w\\
-X^Tdiag(\vec t\odot e^{X\vec w})Xd\vec w\\
=-[X^Tdiag(\vec t\odot e^{X\vec w})X]^Td\vec w
\]</span> <span
class="math inline">\(diag\)</span>作用是生成一个对角矩阵。hessian矩阵为：
<span class="math display">\[
\nabla_{\vec w}^2lnL=-X^Tdiag(\vec t\odot e^{X\vec w})X
\]</span></p>
<p><span
class="math inline">\(X\)</span>满秩时，为凹函数，用牛顿迭代求解。</p>
<h3 id="威布尔回归">威布尔回归</h3>
<p>威布尔分布为： <span class="math display">\[
F(t)=1-e^{-\gamma t^p},\gamma&gt;0,p&gt;0\\
f(t)=\gamma pt^{p-1}e^{-\gamma t^p}\\
S(t)=e^{-\gamma t^p}\\
h(t)=\gamma pt^{p-1}\\
H(t)=\gamma t^p
\]</span> 威布尔回归是对<span
class="math inline">\(\gamma\)</span>做回归： <span
class="math display">\[
\gamma=e^{\vec x^T\vec w}\\
h(t)=e^{\vec x^T\vec w}pt^{p-1}\\
H(t)=e^{\vec x^T\vec w}t^p
\]</span> 似然函数为： <span class="math display">\[
L=\sum_{i=1}^n[y_i\mathop{ln}(e^{\vec x_i^T\vec w}pt_i^{p-1})-e^{\vec
x_i^T\vec w}t_i^p]\\
=\sum_{i=1}^n[y_i\vec x_i^T\vec w+y_ilnp+y_i(p-1)lnt_i)-e^{\vec
x_i^T\vec w}t_i^p]\\
=\vec y^TX\vec w+\vec y^T\vec 1lnp+\vec y^T(p-1)ln\vec t-[\vec
t^p]^Te^{X\vec w}
\]</span></p>
<h1 id="半参数模型">半参数模型</h1>
<h2 id="cox回归">Cox回归</h2>
<p>Cox回归也为比例风险模型： <span class="math display">\[
h(t)=h_0(t)e^{\vec x^T\vec w}
\]</span></p>
<h3 id="生存时间没有打结">生存时间没有“打结”</h3>
<p>似然函数为： <span class="math display">\[
L=\prod_{i=1}^nf(t_i)^{y_i}S(t_i)^{1-y_i}\\
=\prod_{i=1}^nh(t_i)^{y_i}S(t_i)^{y_i}S(t_i)^{1-y_i}\\
=\prod_{i=1}^nh(t_i)^{y_i}S(t_i)\\
=\prod_{i=1}^n\frac{h(t_i)^{y_i}}{\sum_{l\in
R(t_i)}h(t_i)^{y_i}}\sum_{l\in R(t_i)}h(t_i)^{y_i}S(t_i)\\
\]</span> Cox回归使用部分似然： <span class="math display">\[
PL=\prod_{i=1}^n\frac{h(t_i)^{y_i}}{\sum_{l\in R(t_i)}h(t_i)^{y_i}}\\
=\prod_{i=1}^n[\frac{h_0(t_i)e^{\vec x_i^T\vec w}}{\sum_{l\in
R(t_i)}h_0(t_i)e^{\vec x_l^T\vec w}}]^{y_i}\\
=\prod_{i=1}^n[\frac{e^{\vec x_i\vec w}}{\sum_{l\in R(t_i)}e^{\vec
x_l^T\vec w}}]^{y_i}
\]</span> <span class="math inline">\(R(t_i)\)</span>为<span
class="math inline">\(t_i\)</span>时的风险集。记没有打结的排好序的时间为<span
class="math inline">\(t_1,t_2,...t_j,...,t_J\)</span>，部分似然函数可以简化为：
<span class="math display">\[
PL=\prod_{j=1}^J\frac{e^{\vec x_j^T\vec w}}{\sum_{l\in R(t_j)}e^{\vec
x_l^T\vec w}}
\]</span></p>
<h3 id="生存时间有打结">生存时间有“打结”</h3>
<p>Exact方法：将打结的时间看作是有顺序的，假设<span
class="math inline">\(t_j\)</span>时死亡<span
class="math inline">\(d_j\)</span>人，则可能的死亡顺序有<span
class="math inline">\(d_j!\)</span>，计算每一种可能顺序的部分似然，然后加到一起作为<span
class="math inline">\(t_j\)</span>时的部分似然。</p>
<p>Breslow方法：对于<span
class="math inline">\(d_j!\)</span>种顺序，在计算部分似然时都使用同一个分母，即使用<span
class="math inline">\(t_j\)</span>时刻前所有处于风险的个体作为分母。</p>
<h3 id="估计基线生存函数">估计基线生存函数</h3>
<p><span class="math display">\[
S(t)
=S_0(t)^{e^{\vec x^T\vec w}}
\]</span></p>
<p>借鉴Nelson-Aalen估计量来估计基线累积风险函数： <span
class="math display">\[
\hat H_0(t)=\sum_{j:t_j\le t}\frac{d_j}{\sum_{l\in R(t_j)}e^{\vec
x_l^T\vec w}}
\]</span> 因此： <span class="math display">\[
\hat S_0(t)=e^{-\hat H_0(t)}\\
=e^{-\sum_{j:t_j\le t}\frac{d_j}{\sum_{l\in R(t_j)}e^{\vec x_l^T\vec
w}}}\\
=\prod_{j:t_j\le t}e^{-\frac{d_j}{\sum_{l\in R(t_j)}e^{\vec x_l^T\vec
w}}}
\]</span></p>
]]></content>
      <categories>
        <category>数理统计</category>
      </categories>
      <tags>
        <tag>回归</tag>
      </tags>
  </entry>
  <entry>
    <title>数理统计中的一些基本概念</title>
    <url>/2023/01/07/8.%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1%E4%B8%AD%E7%9A%84%E4%B8%80%E4%BA%9B%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5/</url>
    <content><![CDATA[<span id="more"></span>
<h1 id="随机变量的数字特征">随机变量的数字特征</h1>
<h2 id="矩">矩</h2>
<p>随机变量<span class="math inline">\(X\)</span>，任一可能取值为<span
class="math inline">\(x\)</span>，密度为<span
class="math inline">\(f(x)\)</span>，则随机变量的函数<span
class="math inline">\(g(x)\)</span>，<span
class="math inline">\(g(x)\)</span>的期望为矩： <span
class="math display">\[
E[g(X)]=\int g(x)f(x)dx
\]</span></p>
<p>当<span
class="math inline">\(g(x)=x\)</span>时，为一阶原点矩，就是期望，当<span
class="math inline">\(g(x)=x^2\)</span>时，就是二阶原点矩，当<span
class="math inline">\(g(x)=x^3\)</span>时，就是三阶原点矩。</p>
<p>当<span class="math inline">\(g(x)=(x-\bar
x)^2\)</span>时，就是二阶中心矩，就是方差，当<span
class="math inline">\(g(x)=(\frac{x-\bar
x}{\sigma})^3\)</span>时，就是三阶标准化矩，就是偏度，当<span
class="math inline">\(g(x)=(\frac{x-\bar
x}{\sigma})^4\)</span>时，就是四阶标准化矩，就是峰度。</p>
<h2 id="期望">期望</h2>
<h3 id="离散型定义">离散型定义</h3>
<p>随机变量<span class="math inline">\(X\)</span>有取值<span
class="math inline">\(x_1,x_2,...,x_k,...x_K\)</span>，每种取值概率分别为：<span
class="math inline">\(p_1,p_2,...,p_k,...,p_K\)</span>，则期望为： <span
class="math display">\[
E(X)=\sum_{k=1}^Kp_kx_k
\]</span></p>
<h3 id="连续型定义">连续型定义</h3>
<p>期望为： <span class="math display">\[
E(X)=\int xf(x)dx
\]</span></p>
<h3 id="性质">性质</h3>
<p>令<span class="math inline">\(X\)</span>和<span
class="math inline">\(Y\)</span>为随机变量，C为常数，有： <span
class="math display">\[
E(CX)=CE(X)\\
E(X+Y)=E(X)+E(Y)
\]</span> 当<span class="math inline">\(X\)</span>和<span
class="math inline">\(Y\)</span>相互独立时，有： <span
class="math display">\[
E(XY)=E(X)E(Y)
\]</span></p>
<h3 id="其他">其他</h3>
<p>期望为一阶原点矩<span
class="math inline">\(E(X)\)</span>，二阶原点矩为<span
class="math inline">\(E(X^2)\)</span>，三阶原点矩为<span
class="math inline">\(E(X^3)\)</span>，等等</p>
<h2 id="方差">方差</h2>
<h3 id="定义">定义</h3>
<p>随机变量的方差为： <span class="math display">\[
Var(X)=E[(X-E(X)]^2\\
=E[X^2]-[EX]^2
\]</span></p>
<h3 id="性质-1">性质</h3>
<p>令<span class="math inline">\(X\)</span>和<span
class="math inline">\(Y\)</span>为随机变量，C为常数，有： <span
class="math display">\[
Var(C+X)=Var(X)\\
Var(CX)=C^2Var(X)
\]</span> 如何理解<span
class="math inline">\(Var(CX)=C^2Var(X)\)</span>，<span
class="math inline">\(X\)</span>乘以<span
class="math inline">\(C\)</span>后，期望会变为<span
class="math inline">\(CX\)</span>，这样离差也将变为原来的<span
class="math inline">\(C\)</span>倍，离差再经过平方后，自然变为原来的<span
class="math inline">\(C^2\)</span>倍。</p>
<p>一般而言： <span class="math display">\[
Var(X±Y)=Var(X)+Var(Y)±2Cov(X,Y)
\]</span> 当<span class="math inline">\(X\)</span>和<span
class="math inline">\(Y\)</span>相互独立时： <span
class="math display">\[
Var(X±Y)=Var(X)+Var(Y)
\]</span></p>
<p>如何理解<span
class="math inline">\(Var(X±Y)=Var(X)+Var(Y)±2Cov(X,Y)\)</span>？考虑有衣服上有左右两个口袋，每个口袋里有一堆数字卡片，左口袋内的卡片代表随机变量<span
class="math inline">\(X\)</span>的可能取值，右口袋内的卡片代表随机变量<span
class="math inline">\(Y\)</span>的可能取值，现在随机从两个口袋中各拿出一个卡片。如果<span
class="math inline">\(X\)</span>和<span
class="math inline">\(Y\)</span>不相关，则无论从左口袋拿出的<span
class="math inline">\(X\)</span>是大还是小，都不会影响到从右口袋中拿出的<span
class="math inline">\(Y\)</span>的取值，此时二者和的方差为方差的和。而当<span
class="math inline">\(X\)</span>和<span
class="math inline">\(Y\)</span>正相关时，当从左口袋拿出的<span
class="math inline">\(X\)</span>是一个比较大的值（例如比期望值大），那么从右口袋拿出<span
class="math inline">\(Y\)</span>的值往往也比较大，此时二者相加能得到比二者无关时相加更大的值，从而导致和的分散程度增大，也就是方差增大，而二者相减能得到比二者无关时相减更小的值，从而导致差的分散程度减小，也就是方差减小。反之二者负相关时也是类似的。</p>
<h3 id="其他-1">其他</h3>
<p>方差为二阶中心距<span
class="math inline">\(E([X-E(X)])^2\)</span>，三阶中心矩为<span
class="math inline">\(E([X-E(X)]^3)\)</span>，四阶中心距为<span
class="math inline">\(E([X-E(X)]^4)\)</span>，等等。</p>
<h2 id="协方差">协方差</h2>
<h3 id="定义-1">定义</h3>
<p>随机变量<span class="math inline">\(X\)</span>与随机变量<span
class="math inline">\(Y\)</span>的协方差为： <span
class="math display">\[
Cov(X,Y)=E[(X-E(X))(Y-E(Y))]\\
=E(XY)-E(X)E(Y)
\]</span></p>
<h3 id="性质-2">性质</h3>
<p><span class="math display">\[
Cov(aX,bY)=abCov(X,Y)
\]</span></p>
<p><span class="math inline">\(a,b\)</span>为常数 <span
class="math display">\[
Cov(X,Y+Z)=Cov(X,Y)+Cov(X,Z)
\]</span></p>
<h2 id="相关系数">相关系数</h2>
<p>随机变量<span class="math inline">\(X\)</span>和随机变量<span
class="math inline">\(Y\)</span>的相关系数为 <span
class="math display">\[
\rho=Corr(X,Y)=\frac{Cov(X,Y)}{\sqrt{Var(X)Var(Y)}}
\]</span></p>
<h2 id="偏度和峰度">偏度和峰度</h2>
<p>偏度为三阶标准化矩 <span class="math display">\[
skewness=E([\frac{X-E(X)}{\sqrt {Var(X)}}]^3)
\]</span> 正态分布的偏度为0。</p>
<p>峰度为四阶标准化矩 <span class="math display">\[
kurtosis=E([\frac{X-E(X)}{\sqrt{Var(X)}}]^4)
\]</span> 正态分布的峰度为3。上面的量-3，为超额峰度。</p>
<h1 id="随机向量的数字特征">随机向量的数字特征</h1>
<p>随机向量<span class="math inline">\(\vec
X\)</span>是指每一个分量均为随机变量的向量。它的数字特征有期望和协方差矩阵。</p>
<p>协方差矩阵为： <span class="math display">\[
Var(\vec X)=Cov(\vec X)=E[(\vec X-E(\vec X))(\vec X-E(\vec X))^T]\\
=E(\vec X\vec X^T)-E(\vec X)[E(\vec X)]^T
\]</span> 性质： <span class="math display">\[
E(A\vec X)=AE(\vec X)\\
Var(A\vec X)=AVar(\vec X)A^T
\]</span> 这里的<span
class="math inline">\(A\)</span>为常数矩阵。另外随机向量的协方差矩阵一定为半正定矩阵，因为：
<span class="math display">\[
Var(\vec v^T\vec X)=\vec v^T\vec X\vec v\ge 0
\]</span></p>
<h1 id="正态分布的性质">正态分布的性质</h1>
<p>若<span class="math inline">\(X\sim
N(\mu,\sigma^2)\)</span>，则其线性组合也为正态分布，根据期望和方差性质可得<span
class="math inline">\(aX+b\sim N(a\mu+b,a^2\sigma^2)\)</span>；</p>
<p>若<span class="math inline">\(X\sim N(\mu_1,\sigma_1^2),Y\sim
N(\mu_2,\sigma_2^2)\)</span>，且<span
class="math inline">\(X\)</span>和<span
class="math inline">\(Y\)</span>相互独立，则<span
class="math inline">\(X+Y\)</span>以及<span
class="math inline">\(X-Y\)</span>都服从正态分布。</p>
<h1 id="常用抽样分布">常用抽样分布</h1>
<h2 id="chi2分布"><span class="math inline">\(\chi^2\)</span>分布</h2>
<p>若总体服从标准正态分布<span
class="math inline">\(N(0,1)\)</span>，从中随机抽取一个样本容量为<span
class="math inline">\(n\)</span>的样本<span
class="math inline">\(X_1,X_2,...,X_n\)</span>，则随机变量<span
class="math inline">\(Y\)</span>： <span class="math display">\[
Y=X_1^2+X_2^2+...+X_n^2
\]</span> 服从自由度为<span
class="math inline">\(n\)</span>的卡方分布，记为： <span
class="math display">\[
Y\sim \chi^2(n)
\]</span> 这里面要求<span
class="math inline">\(X_1,X_2,...,X_n\)</span>是相互独立的。</p>
<p><span class="math inline">\(\chi^2\)</span>分布有两个重要的性质：</p>
<ol type="1">
<li>可加性：两个相互独立的卡方分布相加后的统计量同样服从卡方分布，该分布的自由度为原来两个分布的自由度之和。</li>
<li>期望为n，方差为2n。</li>
</ol>
<h2 id="t分布">T分布</h2>
<p>若随机变量<span
class="math inline">\(X\)</span>服从标准正态分布，即<span
class="math inline">\(X\sim N(0,1)\)</span>，随机变量<span
class="math inline">\(Y\)</span>服从自由度为<span
class="math inline">\(n\)</span>的卡方分布，即<span
class="math inline">\(Y\sim \chi^2(n)\)</span>，且<span
class="math inline">\(X\)</span>和<span
class="math inline">\(Y\)</span>相互独立，则随机变量 <span
class="math display">\[
T=\frac{X}{\sqrt{\frac{Y}{n}}}\sim t(n)
\]</span> 服从自由度为<span
class="math inline">\(n\)</span>的t分布。</p>
<h2 id="f分布">F分布</h2>
<p>若随机变量<span class="math inline">\(X\)</span>服从自由度为<span
class="math inline">\(n\)</span>的卡方分布，即<span
class="math inline">\(X\sim \chi^2(n)\)</span>，随机变量<span
class="math inline">\(Y\)</span>服从自由度为<span
class="math inline">\(m\)</span>的卡方分布<span
class="math inline">\(Y\sim\chi^2(m)\)</span>，且<span
class="math inline">\(X\)</span>和<span
class="math inline">\(Y\)</span>相互独立，则随机变量 <span
class="math display">\[
F=\frac{X/n}{Y/m}
\]</span> 服从自由度为<span class="math inline">\((n,m)\)</span>的<span
class="math inline">\(F\)</span>分布，记为： <span
class="math display">\[
F\sim F(n,m)
\]</span></p>
<h2 id="样本均值的分布">样本均值的分布</h2>
<h3 id="正态总体">正态总体</h3>
<p>若总体服从正态分布<span
class="math inline">\(N(\mu,\sigma^2)\)</span>，则样本均值<span
class="math inline">\(\bar
X=\frac{1}{n}\sum_{i=1}^nX_i\)</span>服从<span
class="math inline">\(N(\mu,\frac{\sigma^2}{n})\)</span>的正态分布。
<span class="math display">\[
\frac{\bar X-\mu}{\sigma/\sqrt{n}}\sim N(0,1)
\]</span> <span class="math inline">\(\bar
X\)</span>中每一个随机变量都是正态分布，其和也是正态分布。只需要计算一下<span
class="math inline">\(\bar X\)</span>的期望和方差即可： <span
class="math display">\[
E(\bar
X)=E(\frac{1}{n}\sum_{i=1}^nX_i)\\=\frac{1}{n}\sum_{i=1}^nE(X_i)\\=\frac{n\mu}{n}\\=\mu\\
Var(\bar
X)=Var(\frac{1}{n}\sum_{i=1}^nX_i)\\=\frac{1}{n^2}Var(\sum_{i=1}^nX_i)\\=\frac{n\sigma^2}{n^2}\\=\frac{\sigma^2}{n}
\]</span></p>
<p>这个是精确分布，而非渐进分布。</p>
<h3 id="非正态总体">非正态总体</h3>
<p>根据中心极限定理，不管总体是什么分布，大样本下，样本均值的渐进分布都是正态分布。
<span class="math display">\[
\bar X\mathop{\rightarrow}^dN(\mu,\frac{\sigma^2}{n})
\]</span> <span class="math inline">\(\mu\)</span>和<span
class="math inline">\(\sigma^2\)</span>是总体分布的期望和方差。</p>
<h2 id="样本方差的分布">样本方差的分布</h2>
<h3 id="正态总体-1">正态总体</h3>
<p>基于卡方分布，可以导出样本方差的分布。若总体服从正态分布<span
class="math inline">\(N(\mu,\sigma^2)\)</span>，则下面统计量服从卡方分布
<span class="math display">\[
Y=\sum_{i=1}^n[\frac{X_i-\mu}{\sigma}]^2\\
=\frac{1}{\sigma^2}\sum_{i=1}^n[X_i-\mu]^2\\
=\frac{1}{\sigma^2}\sum_{i=1}^n[X_i-\bar X+\bar X-\mu]^2\\
=\frac{1}{\sigma^2}\sum_{i=1}^n[(X_i-\bar X)^2+2(X_i-\bar X)(\bar
X-\mu)+(\bar X-\mu)^2]\\
=\frac{1}{\sigma^2}\sum_{i=1}^n(X_i-\bar X)^2+\frac{2(\bar
X-\mu)}{\sigma^2}\sum_{i=1}^n(X_i-\bar
X)+\frac{1}{\sigma^2}\sum_{i=1}^n(\bar X-\mu)^2\\
=\frac{1}{\sigma^2}\sum_{i=1}^n(X_i-\bar X)^2+\frac{n}{\sigma^2}(\bar
X-\mu)^2\\
=\frac{(n-1)S^2}{\sigma^2}+\frac{n}{\sigma^2}(\bar X-\mu)^2\\
\sim \chi^2(n)
\]</span> 很显然，第二项是一个<span
class="math inline">\(\chi^2(1)\)</span>分布，可以证明第二项和第一项相互独立，根据卡方分布的可加性性质，因此：
<span class="math display">\[
\frac{(n-1)S^2}{\sigma^2}\sim \chi^2(n-1)
\]</span> 此结论只适用于总体是正态分布。</p>
<h2 id="样本频率的分布">样本频率的分布</h2>
<p>设随机变量<span class="math inline">\(X\)</span>服从伯努利分布<span
class="math inline">\(B(1,p)\)</span>，可以计算其期望和方差为： <span
class="math display">\[
E(X)=1×p+0×(1-p)=p\\
Var(X)=E(X^2)-(E(X))^2\\
=1^2×p+0^2×(1-p)-p^2\\
=p-p^2\\
=p(1-p)
\]</span> 若总体服从<span
class="math inline">\(B(1,p)\)</span>，则根据中心极限定理，在大样本下：
<span class="math display">\[
\hat p\mathop{\rightarrow}^d  N(p,\frac{p(1-p)}{n})
\]</span> <span class="math inline">\(\hat p\)</span>为样本频率。</p>
<p>若随机变量<span class="math inline">\(X\)</span>服从二项分布<span
class="math inline">\(B(n,p)\)</span>，若样本量较大，满足<span
class="math inline">\(np\gt 5,np(1-p)\gt 5\)</span>，<span
class="math inline">\(X\)</span>还将近似服从正态分布： <span
class="math display">\[
X\mathop{\rightarrow}^d N(np,np(1-p))
\]</span> 注意这里的的<span
class="math inline">\(X\)</span>是出现阳性结果的次数，不是样本均值（样本频率是样本均值），因此不能用中心极限定理。</p>
]]></content>
      <categories>
        <category>数理统计</category>
      </categories>
      <tags>
        <tag>数理统计</tag>
      </tags>
  </entry>
  <entry>
    <title>常用假设检验方法</title>
    <url>/2023/01/08/9.%E5%B8%B8%E7%94%A8%E5%81%87%E8%AE%BE%E6%A3%80%E9%AA%8C%E6%96%B9%E6%B3%95/</url>
    <content><![CDATA[<p>​</p>
<span id="more"></span>
<p>本文主要总结一下数理统计中常用的一些假设检验方法，涵盖了分布的检验、Z检验、似然比检验、单样本检验、两/多独立样本检验、配对样本检验和相关性检验等。本文的主要组织结构如下：</p>
<pre class="mermaid">graph LR;
假设检验-->分布的检验
假设检验-->单样本的检验
单样本的检验-->L1[连续变量的检验]
单样本的检验-->C1[分类变量的检验]
单样本的检验-->S1[有序变量的检验]
假设检验-->两/多独立样本的检验
两/多独立样本的检验-->L2[连续变量的检验]
两/多独立样本的检验-->C2[分类变量的检验]
两/多独立样本的检验-->S2[有序变量的检验]
假设检验-->配对样本的检验
配对样本的检验-->L3[连续变量的检验]
配对样本的检验-->C3[分类变量的检验]
配对样本的检验-->S3[有序变量的检验]
假设检验-->两变量的相关性检验
两变量的相关性检验-->L4[连续变量的检验]
两变量的相关性检验-->C4[分类变量的检验]
两变量的相关性检验-->S4[有序变量的检验]
假设检验-->一些重要的检验</pre>
<h1 id="分布的检验">分布的检验</h1>
<p>主要用来检验样本来自的总体，是否服从某个特定分布。</p>
<pre class="mermaid">graph LR;
分布的检验-->Pearson卡方拟合优度检验
分布的检验-->Kolmogorov检验
分布的检验-->正态性检验
正态性检验-->基于正态概率图的检验-->Shapiro-Wilk检验
正态性检验-->基于经验分布函数的检验
基于经验分布函数的检验-->Anderson-Darling检验
基于经验分布函数的检验-->Cramer-vonMises检验
基于经验分布函数的检验-->Jarque-Bera检验</pre>
<h2 id="pearson拟合优度chi2检验">Pearson拟合优度<span
class="math inline">\(\chi^2\)</span>检验</h2>
<h3 id="理论分布完全已知">理论分布完全已知</h3>
<p>如果理论分布以及分布中的参数都已知，这时候可以将数轴<span
class="math inline">\((-\infty,+\infty)\)</span>划分为m个区间，根据密度函数就可以计算数据落入各个区间的概率<span
class="math inline">\(p_1,p_2,...p_i,...,p_m\)</span>。记样本数据落入各个区间的频数分别为<span
class="math inline">\(n_1,n_2,...n_i,...,n_m\)</span>，那么如果<span
class="math inline">\(H_0\)</span>成立，则理论上落入各个区间的频数（理论频数）应该为<span
class="math inline">\(np_i,n=n_1+n_2+...+n_m\)</span>，此时统计量：
<span class="math display">\[
K=\sum_{i=1}^m\frac{(n_i-np_i)^2}{np_i}\mathop{\rightarrow}^d
\chi^2(m-1)
\]</span>
Pearson卡方检验要求分组后每组理论频数不应该小于5，如果小于5需要做连续型矫正。</p>
<blockquote>
<p>R中<code>chisq.test()</code>函数可以做Pearson卡方检验。其主要参数为：</p>
<figure class="highlight r"><table><tr><td class="code"><pre><span class="line">chisq.test<span class="punctuation">(</span>x<span class="punctuation">,</span> y <span class="operator">=</span> <span class="literal">NULL</span><span class="punctuation">,</span> correct <span class="operator">=</span> <span class="literal">TRUE</span><span class="punctuation">,</span></span><br><span class="line">           p <span class="operator">=</span> <span class="built_in">rep</span><span class="punctuation">(</span><span class="number">1</span><span class="operator">/</span><span class="built_in">length</span><span class="punctuation">(</span>x<span class="punctuation">)</span><span class="punctuation">,</span> <span class="built_in">length</span><span class="punctuation">(</span>x<span class="punctuation">)</span><span class="punctuation">)</span><span class="punctuation">,</span> rescale.p <span class="operator">=</span> <span class="literal">FALSE</span><span class="punctuation">,</span></span><br><span class="line">           simulate.p.value <span class="operator">=</span> <span class="literal">FALSE</span><span class="punctuation">,</span> B <span class="operator">=</span> <span class="number">2000</span><span class="punctuation">)</span></span><br></pre></td></tr></table></figure>
<ul>
<li><span
class="math inline">\(x\)</span>是由观测数据构成的向量或矩阵，如果是向量，一般直接将其转换为区间频数，从而与参数<span
class="math inline">\(p\)</span>相对应；</li>
<li><span class="math inline">\(y\)</span>是数据向量（当<span
class="math inline">\(x\)</span>为矩阵时，<span
class="math inline">\(y\)</span>无效）；</li>
<li><span
class="math inline">\(correct\)</span>是逻辑向量，表明是否进行连续性修正，<span
class="math inline">\(TRUE\)</span>时表示修正；</li>
<li><span
class="math inline">\(p\)</span>为原假设落在小区间的理论概率，缺省时表示均匀分布</li>
</ul>
</blockquote>
<h3
id="理论分布已知但是分布中参数未知">理论分布已知但是分布中参数未知</h3>
<p>若理论分布中存在未知参数，例如我们想检验样本对应的总体是否是正态分布，而正态分布有两个参数，我们并不知道这两个参数。此时可以先通过极大似然估计来估计出这些未知参数，然后再按照理论分布已知的<span
class="math inline">\(Pearson\quad \chi^2\)</span>
的方法进行检验。不同的是，<strong>此时检验统计量将渐进服从自由度为<span
class="math inline">\(m − 1 − r\)</span>的卡方分布，<span
class="math inline">\(r\)</span>为分布中未知参数的个数。</strong>因为估计这些未知参数消耗了<span
class="math inline">\(r\)</span>个自由度，因此最终自由度需要减去<span
class="math inline">\(r\)</span>。</p>
<h2 id="kolmogorov检验">Kolmogorov检验</h2>
<p>一般只用来对<strong>理论分布为连续型分布</strong>进行检验，该连续型分布为一维分布且<strong>不能有未知参数</strong>。在满足条件的情况下，一般认为Kolmogorov检验优于Pearson卡方拟合优度检验。该检验的检验统计量为：
<span class="math display">\[
D_n=\mathop{sup}_x|F_n(x)-F_0(x)|
\]</span> 这里的<span
class="math inline">\(sup\)</span>为上确界（类似最大值），<span
class="math inline">\(F_n(x)\)</span>为经验分布函数，<span
class="math inline">\(F_0(x)\)</span>为<span
class="math inline">\(H_0\)</span>假设中的理论分布函数。</p>
<p>当检验两个样本是否来源于同一个分布时，Kolmogorov检验推广为Kolmogorov-Smirnov检验，其检验统计量为：
<span class="math display">\[
D_{m,n}=\mathop{sup}_x|F_n(x)-G_m(y)|
\]</span> 这里的<span class="math inline">\(F_n(x)\)</span>和<span
class="math inline">\(G_m(y)\)</span>为两个样本对应的经验分布函数。</p>
<blockquote>
<p>R中<code>ks.test()</code>函数可以进行Kolmogorov-Smirnov检验。</p>
</blockquote>
<h2 id="正态性检验">正态性检验</h2>
<h3 id="图示法qq图">图示法（QQ图）</h3>
<p>将正态分布的分位数与数据的分位数画成散点图，如果数据来源于正态分布，则散点应该集中在45°线附近，这就是分位数-分位数图（quantile-quantile
plot，简称QQ图）。</p>
<h3 id="基于正态概率图的检验">基于正态概率图的检验</h3>
<h4
id="correlation-coefficient-with-normal-probability-plots">Correlation
Coefficient with Normal Probability Plots</h4>
<h4 id="shapiro-wilk-test">Shapiro-Wilk Test</h4>
<h3 id="基于经验分布函数的检验">基于经验分布函数的检验</h3>
<h4 id="anderson-darling-test">Anderson-Darling Test</h4>
<p>基于经验分布于理论分布之间差异平方的加权平均进行的检验。</p>
<h4 id="cramer-von-mises-test">Cramer-von Mises Test</h4>
<p>基于经验分布于理论分布之间差异平方的简单平均进行的检验。</p>
<h4 id="jarquebera-test">Jarque–Bera Test</h4>
<p>该检验主要基于样本的偏度和峰度来检验正态性。检验统计量为： <span
class="math display">\[
JB=\frac{n}{6}(S^2+\frac{(K-3)^2}{4})\mathop{\rightarrow}^d \chi(2)
\]</span> 这个检验的零假设是偏度为0以及超额峰度为0。这里的<span
class="math inline">\(S\)</span>是样本偏度，<span
class="math inline">\(K\)</span>是样本峰度，<span
class="math inline">\(K-3\)</span>就是超额峰度。其中： <span
class="math display">\[
\begin{aligned}
S=\frac{1}{n}\sum_{i=1}^n(\frac{x_i-\bar x}{\hat{\sigma}})^3\\
K=\frac{1}{n}\sum_{i=1}^n(\frac{x_i-\bar x}{\hat{\sigma}})^4
\end{aligned}
\]</span> 如果数据服从正态分布，<span
class="math inline">\(JB\)</span>统计量将渐进服从2个自由度的卡方分布，<span
class="math inline">\(JB\)</span>统计量收敛速度较慢，对样本容量要求较高。</p>
<h1 id="常用的一些假设检验方法">常用的一些假设检验方法</h1>
<p>除了上面介绍的几种分布的检验之外，数理统计中还有一些非常常用的假设检验方法，这些方法在下文中将会频繁出现，这里先进行一些整理。</p>
<pre class="mermaid">graph LR;
一些重要的检验-->Z检验
一些重要的检验-->样本率在大样本下的Z检验
一些重要的检验-->似然比检验</pre>
<h2 id="z检验">Z检验</h2>
<p>如果随机变量<span class="math inline">\(X\)</span>满足： <span
class="math display">\[
X\sim N(\mu,\sigma^2)
\]</span> 则统计量<span class="math inline">\(Z\)</span>： <span
class="math display">\[
Z=\frac{X-\mu}{\sigma}\sim N(0,1)
\]</span> 这就是<span class="math inline">\(Z\)</span>检验。<span
class="math inline">\(Z\)</span>统计量的平方服从： <span
class="math display">\[
\chi^2=Z^2\sim \chi^2(1)
\]</span> 所以有时候用<span
class="math inline">\(\chi^2\)</span>统计量代替<span
class="math inline">\(Z\)</span>统计量。</p>
<p>数理统计中，由于中心极限定理，很多统计量在大样本下都服从渐进正态分布，这使得大样本下都可以构造<span
class="math inline">\(Z\)</span>统计量进行<span
class="math inline">\(Z\)</span>检验，因此<span
class="math inline">\(Z\)</span>检验非常常用，例如<span
class="math inline">\(t\)</span>检验在大样本下就可以用<span
class="math inline">\(Z\)</span>检验来代替。</p>
<h2 id="率的z检验">率的Z检验</h2>
<p>这里介绍一下样本率在大样本下的<span
class="math inline">\(Z\)</span>检验。若随机变量<span
class="math inline">\(X\sim B(1,\pi)\)</span>，样本率<span
class="math inline">\(p\)</span>在大样本<span
class="math inline">\((n\pi\gt 5,n(1-\pi)\gt 5)\)</span>下有： <span
class="math display">\[
p\mathop{\rightarrow}^d N(\pi,\frac{\pi(1-\pi)}{n})
\]</span> 因此<span class="math inline">\(Z\)</span>统计量为： <span
class="math display">\[
Z=\frac{|p-\pi|-\frac{0.5}{n}}{\sqrt\frac{\pi(1-\pi)}{n}}\mathop{\rightarrow}^dN(0,1)
\]</span> 这里的<span
class="math inline">\(\frac{0.5}{n}\)</span>为连续性矫正系数。同时我们还知道大样本下，出现阳性个体的次数<span
class="math inline">\(Y\)</span>也是一个随机变量，它服从： <span
class="math display">\[
Y\mathop{\rightarrow}^d N(n\pi,n\pi(1-\pi))
\]</span> 此时<span class="math inline">\(Z\)</span>统计量为： <span
class="math display">\[
Z=\frac{|Y-n\pi|-0.5}{\sqrt {n\pi(1-\pi)}}\mathop{\rightarrow}^d N(0,1)
\]</span></p>
<h2 id="似然比检验">似然比检验</h2>
<p>对于分类数据，除了可以使用Pearson拟合优度<span
class="math inline">\(\chi^2\)</span>检验之外，还可以用似然比检验，通常需要假定数据服从伯努利分布或多项分布。</p>
<h3 id="理论分布完全已知-1">理论分布完全已知</h3>
<p>假定随机变量<span class="math inline">\(X\)</span>服从多项分布，<span
class="math inline">\(X\)</span>属于可以取<span
class="math inline">\(m\)</span>种类别，各种类别的取值概率分别为<span
class="math inline">\(\pi_1,\pi_2,...\pi_i,...,\pi_m\)</span>。现收集一部分数据，数据中属于各个类别的频数分别为<span
class="math inline">\(n_1,n_2,...n_i,...,n_m\)</span>，样本量为<span
class="math inline">\(n=n_1+n_2+...+n_m\)</span>，这样用样本来估计各个类别的概率为：
<span class="math display">\[
\frac{n_1}{n},\frac{n_2}{n},...,\frac{n_m}{n}
\]</span> 此时可以定义两个似然<span
class="math inline">\(L1\)</span>和<span
class="math inline">\(L2\)</span>： <span class="math display">\[
\begin{aligned}
L1=\prod_{i=1}^m\pi_i^{n_i}\\
L2=\prod_{i=1}^m(\frac{n_i}{n})^{n_i}
\end{aligned}
\]</span> 两个负对数似然为： <span class="math display">\[
\begin{aligned}
-lnL1=-\sum_{i=1}^mn_iln\pi_i\\
-lnL2=-\sum_{i=1}^mn_iln\frac{n_i}{n}
\end{aligned}
\]</span> 可以证明，如果样本和<span
class="math inline">\(X\)</span>的分布相同，则两个负对数似然函数之差的2倍服从：
<span class="math display">\[
-2ln\frac{L1}{L2}=-\sum_{i=1}^mn_iln\frac{\pi_i}{\frac{n_i}{n}}\mathop{\rightarrow}^d
\chi^2(m-1)
\]</span></p>
<h3
id="理论分布已知但是分布中参数未知-1">理论分布已知但是分布中参数未知</h3>
<p>若理论分布中存在未知参数，类似前面的<span
class="math inline">\(Pearson\quad \chi^2\)</span>
检验，我们需要先估计未知参数，然后做似然比检验，不过<strong>此时检验统计量将渐进服从自由度为<span
class="math inline">\(m − 1 − r\)</span>的卡方分布，<span
class="math inline">\(r\)</span>为分布中未知参数的个数。</strong>下面将要介绍的两组或多组分类变量的检验就是这种情况。</p>
<h1 id="单样本的假设检验">单样本的假设检验</h1>
<pre class="mermaid">graph LR;
单样本-->连续变量
连续变量--正态分布-->σ已知?
σ已知?--已知-->Z1[Z检验]
σ已知?--未知,且小样本-->t检验
σ已知?--未知,但大样本-->Z1
连续变量--非正态分布-->符号检验/wilcoxon秩和检验
单样本-->二分类变量--二项分布-->大样本?
大样本?--是-->Z检验
大样本?--否-->精确检验
单样本-->多分类变量--多项分布-->卡方拟合优度检验/似然比检验
单样本-->有序变量--多项分布-->卡方拟合优度检验/似然比检验</pre>
<h2 id="连续变量">连续变量</h2>
<p>对于单样本的连续变量，先检验数据分布，判断是否服从正态分布。如果服从正态分布，则均值是反应总体集中趋势的重要指标，可对均值进行假设检验。如果不服从正态分布，则样本中位数是反应总体集中趋势的重要指标，可对中位数进行假设检验。</p>
<h3 id="正态分布">正态分布</h3>
<p>若总体服从正态分布<span
class="math inline">\(N(\mu,\sigma^2)\)</span>，样本均值<span
class="math inline">\(\bar X\)</span>将服从： <span
class="math display">\[
\bar X\sim N(\mu,\frac{\sigma^2}{n})
\]</span></p>
<p><span
class="math inline">\(\mu\)</span>一般会给出，这时候检验的目标是样本对应的总体均值是否为<span
class="math inline">\(\mu\)</span>，需要根据总体分布中的参数<span
class="math inline">\(\sigma^2\)</span>是否已知来选择合适的假设检验方法。</p>
<h4 id="sigma2已知"><span
class="math inline">\(\sigma^2\)</span>已知</h4>
<p>若<span
class="math inline">\(\sigma^2\)</span>已知，就可以构造Z检验： <span
class="math display">\[
Z=\frac{\bar X-\mu}{\sigma/\sqrt{n}}\sim N(0,1)
\]</span></p>
<p>这是一个精确检验。</p>
<h4 id="sigma2未知且小样本"><span
class="math inline">\(\sigma^2\)</span>未知，且小样本</h4>
<p>虽然<span
class="math inline">\(\sigma^2\)</span>未知，但是我们知道样本方差服从<span
class="math inline">\(\frac{(n-1)S^2}{\sigma^2}\sim
\chi^2(n-1)\)</span>，且与样本均值<span class="math inline">\(\bar
X\)</span>独立，因此： <span class="math display">\[
\begin{align}
T=\frac{Z}{\sqrt{(n-1)S^2/(n-1)\sigma^2}}\\
=\frac{\sqrt{n}(\bar X-\mu)}{\sigma}\frac{\sigma}{S}\\
=\frac{\bar X-\mu}{S/\sqrt{n}}\sim t(n-1)
\end{align}
\]</span> 因此构造T统计量进行检验： <span class="math display">\[
T=\frac{\bar X-\mu}{S/\sqrt{n}}\sim t(n-1)
\]</span></p>
<p>这就是著名的<span
class="math inline">\(t\)</span>检验，这个检验也是精确检验。</p>
<blockquote>
<p>R中用<code>t.test()</code>做单样本的t检验。</p>
</blockquote>
<h4 id="sigma2未知但是大样本"><span
class="math inline">\(\sigma^2\)</span>未知，但是大样本</h4>
<p>由于<span
class="math inline">\(t\)</span>分布的渐进分布是正态分布，因此在大样本下还可以用Z检验来进行近似。即用<span
class="math inline">\(S\)</span>代替<span
class="math inline">\(\sigma\)</span>： <span class="math display">\[
Z=\frac{\bar X-\mu}{S/\sqrt{n}}\sim N(0,1)
\]</span></p>
<p>很显然，这是一个近似检验。</p>
<h3 id="非正态分布">非正态分布</h3>
<p>非正态分布时，我们主要检验样本对应的总体，其中位数是否等于某个确定值<span
class="math inline">\(M_0\)</span>。主要有符号检验和Wilcoxon符号秩检验两种。</p>
<h4 id="符号检验">符号检验</h4>
<p>思想：用样本数据减去<span
class="math inline">\(M_0\)</span>，如果<span
class="math inline">\(H_0\)</span>成立，差值中将有一半数据为负数，一半数据为正数，也就是说有50%的概率出现正号或者负号，记即出现正号的频率为<span
class="math inline">\(p\)</span>，它将服从<span
class="math inline">\(p\sim
B(1,\pi),\pi=0.5\)</span>。或者记正号出现的次数为<span
class="math inline">\(F\)</span>，它将服从<span
class="math inline">\(F\sim
B(n,\pi),\pi=0.5\)</span>。假定分布后，就可以用基于二项分布的精确检验，或者大样本下率的Z检验。</p>
<h5 id="精确检验">精确检验</h5>
<p>直接根据二项分布的分布律计算<span
class="math inline">\(p\)</span>值。</p>
<p>若<span class="math inline">\(F\gt \frac{n}{2}\)</span>，<span
class="math inline">\(p\)</span>值为 <span class="math display">\[
p=2×\sum_{k=F}^nC_{n}^F(0.5)^k(0.5)^{n-k}=2×\sum_{k=F}^nC_{n}^F(0.5)^n
\]</span> 若<span class="math inline">\(F\lt \frac{n}{2}\)</span>，<span
class="math inline">\(p\)</span>值为 <span class="math display">\[
p=2×\sum_{k=1}^FC_n^F(0.5)^k(0.5)^{n-k}=2\sum_{k=1}^FC_n^F(0.5)^n
\]</span> 若<span class="math inline">\(F=\frac{n}{2}\)</span>，<span
class="math inline">\(p=1\)</span></p>
<h5 id="近似检验">近似检验</h5>
<p>在大样本下<span class="math inline">\(n\pi\gt 5,n\pi(1-\pi)\gt
5\)</span>，出现正号的频率<span class="math inline">\(\hat
p\)</span>应该服从： <span class="math display">\[
p\mathop{\rightarrow}^d N(0.5,\frac{0.5×0.5}{n})
\]</span> 或者出现正号的次数<span
class="math inline">\(F\)</span>应该服从： <span class="math display">\[
F\mathop{\rightarrow}^d N(\frac{n}{2},\frac{n}{4})
\]</span></p>
<p>因此，检验统计量为： <span class="math display">\[
Z=\frac{|p-0.5|-\frac{0.5}{n}}{\sqrt{\frac{0.25}{n}}}
\]</span></p>
<p>或 <span class="math display">\[
Z=\frac{|F-n/2|-0.5}{\sqrt{n/4}}
\]</span> &gt;
R中使用<code>binom.test()</code>函数做单样本的符号检验。</p>
<h4 id="wilcoxon符号秩检验">Wilcoxon符号秩检验</h4>
<p>思想：在符号检验的基础上引入了秩和。用样本数据减去<span
class="math inline">\(M_0\)</span>，得到差值。记录差值的符号，若差值为0则舍去。对差值取绝对值，然后排序记录每个差值的秩次（若差值的绝对值相等取平均秩次）。记删除差值为0之后的样本数目为n，显然总的秩次和为<span
class="math inline">\(\frac{n(n+1)}{2}\)</span>，然后统计符号为正的秩次之和。若<span
class="math inline">\(H_0\)</span>成立，大样本下（<span
class="math inline">\(n\gt 50\)</span>）符号为正的秩次之和<span
class="math inline">\(T_+\)</span>近似服从： <span
class="math display">\[
T_+\mathop{\rightarrow}^d N(\frac{n(n+1)}{4},\frac{n(n+1)(2n+1)}{4})
\]</span> 因此，使用<span class="math inline">\(Z\)</span>检验为： <span
class="math display">\[
Z=\frac{T_+-\frac{n(n+1)}{4}-0.5}{\frac{n(n+1)(2n+1)}{4}}
\]</span>
0.5为连续性矫正。取差值之后，可能有多个差值绝对值相等，称为“结”，若“结”较多时，上述公式需要修正。</p>
<blockquote>
<p>R中用<code>wilcox.test()</code>做单样本的wilcoxon检验。</p>
</blockquote>
<h2 id="分类变量">分类变量</h2>
<p>如果是二分类变量，常常假定数据服从伯努利分布，问题转化为样本率的检验，可以使用二项分布的精确检验，也可以使用大样本下的<span
class="math inline">\(Z\)</span>检验。如果是多分类变量，常常假定数据服从多项分布，可以使用<span
class="math inline">\(Pearson\quad
\chi^2\)</span>拟合优度检验或者似然比检验。</p>
<h3 id="伯努利分布">伯努利分布</h3>
<p>假定数据服从伯努利分布，分布率的<span
class="math inline">\(\pi\)</span>一般会给出，可以进行精确检验或近似检验</p>
<h4 id="精确检验-1">精确检验</h4>
<p>直接根据二项分布的性质计算<span
class="math inline">\(p\)</span>值。记阳性样本出现的次数为<span
class="math inline">\(F\)</span>。</p>
<p>若<span class="math inline">\(F\gt \frac{n}{2}\)</span>，<span
class="math inline">\(p\)</span>值为 <span class="math display">\[
p=2×\sum_{k=F}^nC_{n}^F(\pi)^k(1-\pi)^{n-k}
\]</span> 若<span class="math inline">\(F\lt \frac{n}{2}\)</span>，<span
class="math inline">\(p\)</span>值为 <span class="math display">\[
p=2×\sum_{k=1}^FC_n^F(\pi)^k(1-\pi)^{n-k}
\]</span> 若<span class="math inline">\(F=\frac{n}{2}\)</span>，<span
class="math inline">\(p=1\)</span></p>
<h4 id="近似检验-1">近似检验</h4>
<p>在大样本下<span class="math inline">\(n\pi\gt 5,n\pi(1-\pi)\gt
5\)</span>，出现正号的频率<span
class="math inline">\(p\)</span>应该服从： <span class="math display">\[
p\sim N(\pi,\frac{\pi×(1-\pi)}{n})
\]</span> 或者出现正号的次数<span
class="math inline">\(F\)</span>应该服从： <span class="math display">\[
F\sim N(n\pi,n\pi(1-\pi))
\]</span></p>
<p>因此，检验统计量为： <span class="math display">\[
Z=\frac{|p-\pi|-\frac{0.5}{n}}{\sqrt{\frac{\pi(1-\pi)}{n}}}
\]</span></p>
<p>或 <span class="math display">\[
Z=\frac{|F-n\pi|-0.5}{n\pi(1-\pi)}
\]</span> 0.5为连续性修正</p>
<blockquote>
<p>R中使用<code>binom.test()</code>函数做单样本的符号检验。</p>
</blockquote>
<h3 id="多项分布">多项分布</h3>
<p>可以假定多分类变量服从多项分布，这时候一般会给出各个类别的概率，使用参数已知时的<span
class="math inline">\(Pearson\quad
\chi^2\)</span>拟合优度检验或者似然比检验。</p>
<h2 id="有序变量">有序变量</h2>
<p>有序变量同样可以看作服从多项分布，使用参数已知时的卡方拟合优度检验或似然比检验。</p>
<h1 id="两多独立样本的假设检验">两/多独立样本的假设检验</h1>
<pre class="mermaid">graph LR;
两独立样本-->a1[连续变量]
a1--均服从正态分布-->方差均已知?
方差均已知?--是-->Z检验
方差均已知?--否,但知道相等-->t检验
方差均已知?--否,且知道不相等-->S[Satterthwaite t检验]
a1--至少有一个不服从正态分布-->W1[Wilcoxon秩和检验]
两独立样本-->a2[分类变量]
a2[分类变量]--近似法-->卡方检验/似然比检验/优势比检验/MH检验/CMH检验
a2[分类变量]--精确法-->Fisher精确检验
两独立样本-->a3[有序变量]-->W1[Wilcoxon秩和检验]</pre>
<h2 id="连续变量-1">连续变量</h2>
<p>对于连续变量，这里只讨论两独立样本问题，多独立样本主要用方差分析和KW检验，后面会专门写一篇文章进行介绍。</p>
<h3 id="正态分布-1">正态分布</h3>
<p>两独立样本，如果均服从正态分布，且总体方差已知。根据正态分布的性质，独立样本的和也将服从正态分布，可构造<span
class="math inline">\(Z\)</span>统计量进行假设检验。即：总体1：<span
class="math inline">\(X\sim N(\mu_1,\sigma_1^2)\)</span>，总体2：<span
class="math inline">\(Y\sim
N(\mu_2,\sigma_2^2)\)</span>，且相互独立，则： <span
class="math display">\[
\begin{aligned}
\bar X\sim N(\mu_1,\sigma^2_1/n_1)\\
\bar Y\sim N(\mu_2,\sigma_2^2/n_2)
\end{aligned}
\]</span></p>
<p><span class="math display">\[
\begin{aligned}
E(\bar X-\bar Y)=E(\bar X)-E(\bar Y)=\mu_1-\mu_2\\
Var(\bar X-\bar Y)=Var(\bar X)+Var(\bar Y)-2Cov(\bar X,\bar Y)
\end{aligned}
\]</span></p>
<p>因为<span class="math inline">\(X\)</span>和<span
class="math inline">\(Y\)</span>独立，因此： <span
class="math display">\[
\begin{align}
Cov(\bar X,\bar Y)=0\\
Var(\bar X-\bar Y)=Var(\bar X)+Var(\bar Y)
\end{align}
\]</span> 且因为<span class="math inline">\(X\)</span>和<span
class="math inline">\(Y\)</span>均为正态总体，因此<span
class="math inline">\(\bar X-\bar Y\)</span>也服从正态分布： <span
class="math display">\[
\bar X-\bar Y\sim
N(\mu_1-\mu_2,\frac{\sigma_1^2}{n_1}+\frac{\sigma_2^2}{n_2})
\]</span>
若总体方差未知但样本方差齐，可以用样本的联合方差估计总体方差，构造<span
class="math inline">\(t\)</span>统计量进行检验。若总体方差未知且样本方差不齐，可用矫正的<span
class="math inline">\(t\)</span>检验。</p>
<h4 id="sigma_12和sigma_22均已知"><span
class="math inline">\(\sigma_1^2\)</span>和<span
class="math inline">\(\sigma_2^2\)</span>均已知</h4>
<p><span class="math inline">\(Z\)</span>统计量： <span
class="math display">\[
Z=\frac{\bar X-\bar
Y-(\mu_1-\mu_2)}{\sqrt{\sigma_1^2/n_1+\sigma_2^2/n_2}}\sim N(0,1)
\]</span> 因为<span class="math inline">\(H_0\)</span>假设实际为<span
class="math inline">\(\mu_1=\mu_2\)</span>，所以统计量实际为： <span
class="math display">\[
Z=\frac{\bar X-\bar Y}{\sqrt{\sigma_1^2/n_1+\sigma_2^2/n_2}}\sim N(0,1)
\]</span></p>
<p>此为精确检验。</p>
<h4 id="sigma_12-sigma_22但未知"><span class="math inline">\(\sigma_1^2
= \sigma_2^2\)</span>但未知</h4>
<p>此时称为方差齐。先要进行方差齐性检验，即检验<span
class="math inline">\(\sigma_1^2=\sigma_2^2\)</span>。已知两个独立样本都来自正态总体，因此有：
<span class="math display">\[
\begin{aligned}
\frac{(n_1-1)S_1^2}{\sigma_1^2}\sim \chi^2(n_1-1)\\
\frac{(n_2-1)S_1^2}{\sigma_2^2}\sim \chi^2(n_2-1)
\end{aligned}
\]</span> 可以证明<span class="math inline">\(S_1^2\)</span>和<span
class="math inline">\(S_2^2\)</span>相互独立，则以下统计量： <span
class="math display">\[
\begin{aligned}
F=\frac{(n_1-1)S_1^2}{\sigma_1^2(n_1-1)}/\frac{(n_2-1)S_1^2}{\sigma_2^2(n_1-1)}\\
=\frac{S_1^2}{\sigma_1^2}/\frac{S_1^2}{\sigma_2^2}
\end{aligned}
\]</span> 若方差齐性假设成立，则统计量可简化为： <span
class="math display">\[
F=\frac{S_1^2}{S_2^2}\sim F(n_1-1,n_2-1)
\]</span> 若检验结果认为方差齐，即可以认为<span
class="math inline">\(\sigma_1^2=\sigma_2^2=\sigma^2\)</span>，可以导出均值差的分布为：
<span class="math display">\[
\begin{aligned}
Z=\frac{\bar X_1-\bar
X_2-(\mu_1-\mu_2)}{\sqrt{\frac{\sigma_1^2}{n_1}+\frac{\sigma_2^2}{n_2}}}\\
=\frac{\bar X_1-\bar X_2}{\sigma\sqrt{\frac{1}{n_1}+\frac{1}{n_2}}}\sim
N(0,1)
\end{aligned}
\]</span> 这里的<span
class="math inline">\(\sigma\)</span>仍然未知，仍不能用这个检验。根据卡方分布的可加性，有：
<span class="math display">\[
\begin{aligned}
V=\frac{(n_1-1)S_1^2}{\sigma_1^2}+\frac{(n_2-1)S_1^2}{\sigma_2^2}\\
=\frac{(n_1-1)S_1^2}{\sigma^2}+\frac{(n_2-1)S_1^2}{\sigma^2}\sim
\chi^2(n_1+n_2-2)
\end{aligned}
\]</span> 可以证明<span class="math inline">\(Z\)</span>和<span
class="math inline">\(V\)</span>独立，根据t分布定义，有： <span
class="math display">\[
\begin{aligned}
T=\frac{Z}{\sqrt{V/(n_1+n_2-2)}}\\
=\frac{\bar X_1-\bar
X_2}{\sigma\sqrt{\frac{1}{n_1}+\frac{1}{n_2}}}/\sqrt{\frac{(n_1-1)S_1^2+(n_2-1)S_2^2}{\sigma^2(n_1+n_2-2)}}\\
=\frac{\bar X_1-\bar X_2}{S_p\sqrt{\frac{1}{n_1}+\frac{1}{n_2}}}\sim
t(n_1+n_2-2)
\end{aligned}
\]</span> 其中<span
class="math inline">\(S_p=\frac{(n_1-1)S_1^2+(n_2-1)S_2^2}{(n_1+n_2-2)}\)</span>。此也为精确检验。</p>
<blockquote>
<p>R中用<code>t.test()</code>并指定参数<code>var.equal = TRUE</code>时做方差齐时的两独立样本的t检验。</p>
</blockquote>
<h4 id="sigma_12ne-sigma_22且未知"><span
class="math inline">\(\sigma_1^2\ne \sigma_2^2\)</span>且未知</h4>
<p>当<span class="math inline">\(\sigma_1^2\ne
\sigma_2^2\)</span>时，难以构造一个统计量并找出其精确分布，不过统计量：
<span class="math display">\[
t^*=\frac{\bar X_1-\bar X_2}{\sqrt{\frac{S_1^2}{n_1}+\frac{S_2^2}{n_2}}}
\]</span> 近似服从<span
class="math inline">\(t\)</span>分布，但是自由度需要修正为： <span
class="math display">\[
v=\frac{(\frac{S_1^2}{n_1}+\frac{S_2^2}{n_2})^2}{\frac{(\frac{S_1^2}{n_1})^2}{n_1-1}+\frac{(\frac{S_2^2}{n_2})^2}{n_2-1}}
\]</span> 此方法称为Satterthwaite近似方法。</p>
<blockquote>
<p>R中用<code>t.test()</code>并指定参数<code>var.equal = FALSE</code>时做方差不齐时的两独立样本的t检验。</p>
</blockquote>
<h3 id="非正态分布-1">非正态分布</h3>
<p>若至少有一个连续变量不服从正态分布，此时就不能使用基于正态分布的<span
class="math inline">\(Z\)</span>检验和<span
class="math inline">\(t\)</span>检验了，应该使用非参数检验。</p>
<h4 id="wilcoxon秩和检验">Wilcoxon秩和检验</h4>
<p>两组数据混合之后统一编秩，碰到数据相同时取平均秩次。记两组数据样本量分别为<span
class="math inline">\(n_1,n_2\)</span>，统计两组的秩和。若<span
class="math inline">\(H_0\)</span>成立，在大样本下，第一组秩和<span
class="math inline">\(T_1\)</span>服从： <span class="math display">\[
T_1\mathop{\rightarrow}^d
N(\frac{n_1(n_1+n_2+1)}{2},\frac{n_1n_2(n_1+n_2+1)}{12})
\]</span> 构造统计量： <span class="math display">\[
Z_1=\frac{T_1-\frac{n_1(n_1+n_2+1)}{2}-0.5}{\sqrt{\frac{n_1n_2(n_1+n_2+1)}{12}}}
\]</span> 0.5为连续性矫正，结较多时，公式同样需要进行修正。</p>
<blockquote>
<p>R中用<code>wilcox.test()</code>做两独立样本的wilcoxon检验。</p>
</blockquote>
<h2 id="分类变量-1">分类变量</h2>
<p>无论是两独立样本还是多独立样本之间分类变量的检验，都可用<span
class="math inline">\(Pearson\quad
\chi^2\)</span>检验或似然比检验，通常假定数据服从二项分布或者多项分布，此为近似检验。对于2×2数据而言，还可以用优势比检验和Fisher精确检验（很多统计软件也提供了R×C
Fisher精确检验），Fisher检验实际上假定某个单元格服从超几何分布，在大样本下从Fisher检验可以导出<span
class="math inline">\(Mantel-Haenszel\)</span>
卡方检验，在引入分层变量之后，又可以导出<span
class="math inline">\(Cochran-Mantel-Haenszel\)</span> 卡方检验。</p>
<h3 id="chi2检验似然比检验"><span
class="math inline">\(\chi^2\)</span>检验/似然比检验</h3>
<p>这里仅介绍<span
class="math inline">\(\chi^2\)</span>检验，似然比检验也是类似的。</p>
<p>假设列联表有<span class="math inline">\(R\)</span>行<span
class="math inline">\(C\)</span>列，即有<span
class="math inline">\(R\)</span>个样本（即认为行变量为分组变量），每个样本有<span
class="math inline">\(C\)</span>种类别，令列联表每个单元格取值为<span
class="math inline">\(n_{ij},i=1,2,...,R,j=1,2,...,C\)</span>。检验目的是比较<span
class="math inline">\(R\)</span>个样本的频率或构成比分布是否一致。检验的基本思想是利用卡方拟合优度检验。</p>
<p>零假设为：<span
class="math inline">\(R\)</span>个样本的频率或构成比分布一致。假设总体中各个类别的取值概率均已知，为<span
class="math inline">\(\pi_1,\pi_2,...,\pi_C\)</span>，如果零假设是成立的，对于任意一个样本<span
class="math inline">\(i,i=1,2,...,R\)</span>，我们做一个<span
class="math inline">\(\chi^2\)</span>拟合优度检验： <span
class="math display">\[
\chi^2=\sum_{j=1}^C\frac{(n_{ij}-n_i\pi_j)^2}{n_i\pi_j}\mathop{\rightarrow}^d\chi^2(C-1)
\]</span> <span class="math inline">\(n_i\)</span>为第<span
class="math inline">\(i\)</span>个样本的样本量。由于卡方分布的可加性，统计量：
<span class="math display">\[
\chi^2=\sum_{i=1}^R\sum_{j=1}^C\frac{(n_{ij}-n_i\pi_j)^2}{n_i\pi_j}\mathop{\rightarrow}^d\chi^2(R(C-1))
\]</span> 但是<span
class="math inline">\(\pi_1,\pi_2,...,\pi_C\)</span>通常是未知的，我们需要用<span
class="math inline">\(R\)</span>个样本来估计，最大似然估计将用样本频率<span
class="math inline">\(p_1,p_2,..p_C\)</span>作为总体<span
class="math inline">\(\pi_1,\pi_2,...\pi_C\)</span>的估计值，待估参数有<span
class="math inline">\(C-1\)</span>个。因此统计量： <span
class="math display">\[
\chi^2=\sum_{i=1}^R\sum_{j=1}^C\frac{(n_{ij}-n_ip_j)^2}{n_ip_j}\mathop{\rightarrow}^d\chi^2(R(C-1)-(C-1))
\]</span> 因为<span
class="math inline">\(R(C-1)-(C-1)=(R-1)(C-1)\)</span>，因此该统计量的自由度为<span
class="math inline">\((R-1)(C-1)\)</span>。</p>
<p><strong>注意</strong></p>
<ul>
<li>卡方拟合优度检验实际用到了二项分布的大样本近似（要求<span
class="math inline">\(n\pi \gt5,n(1-\pi)\gt
5\)</span>），因此卡方齐性检验也要求：不宜有20%以上的单元格<span
class="math inline">\(n_i\pi_j\lt 5\)</span>，或者不宜有一个格子的<span
class="math inline">\(n_i\pi_j\)</span>小于1。如果不满足，可以对组进行合并。</li>
<li>对于四格表，还要求样本量不小于40。当样本量大于等于<span
class="math inline">\(40\)</span>，但是某个格子<span
class="math inline">\(n_i\pi_j\in
[1,5)\)</span>，需要进行连续性矫正。不过多独立样本无需矫正。</li>
<li>不服从条件时可以使用Fisher精确检验。</li>
</ul>
<p><strong>多重比较</strong></p>
<p>若检验结果为拒绝零假设，则表明至少有一个样本频率分布与其他样本不同，此时可进行两两比较，共有<span
class="math inline">\(C_R^2\)</span>种组合，需要将单次比较的检验水准进行调整。可使用<span
class="math inline">\(Bonferroni\)</span>法，调整单次检验水准为<span
class="math inline">\(\alpha/C_R^2\)</span>，以保证整个试验犯一类错误的概率不超过<span
class="math inline">\(\alpha\)</span>。</p>
<h3 id="列联表的几种检验方法">2×2列联表的几种检验方法</h3>
<h4 id="分布假定">分布假定</h4>
<p>对于四格表：</p>
<table>
<thead>
<tr class="header">
<th></th>
<th>B1</th>
<th>B2</th>
<th>合计</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>A1</td>
<td><span class="math inline">\(n_{11}\)</span></td>
<td><span class="math inline">\(n_{12}\)</span></td>
<td><span class="math inline">\(n_{1+}\)</span></td>
</tr>
<tr class="even">
<td>A2</td>
<td><span class="math inline">\(n_{21}\)</span></td>
<td><span class="math inline">\(n_{22}\)</span></td>
<td><span class="math inline">\(n_{2+}\)</span></td>
</tr>
<tr class="odd">
<td>合计</td>
<td><span class="math inline">\(n_{+1}\)</span></td>
<td><span class="math inline">\(n_{+2}\)</span></td>
<td><span class="math inline">\(n\)</span></td>
</tr>
</tbody>
</table>
<p>四格表的边际有<span
class="math inline">\(n_{1+},n_{2+},n_{+1},n_{+2},n\)</span>共5个数，它们可以是给定的，也可以是随机的，根据这5个数据是否给定，有以下四种分布：</p>
<ol type="1">
<li>单侧给定，如<span class="math inline">\(n_{1+}\)</span>和<span
class="math inline">\(n_{2+}\)</span>给定，则<span
class="math inline">\(n_{11},n_{21}\)</span>都服从二项分布；</li>
<li>只有总样本量<span class="math inline">\(n\)</span>给定，则<span
class="math inline">\(n_{11},n_{12},n_{21},n_{22}\)</span>都服从多项分布；</li>
<li>两侧都不给定，则<span
class="math inline">\(n_{11},n_{12},n_{21},n_{22}\)</span>都是随机变量，通常假设他们服从Poisson分布；</li>
<li>两侧都给定，则<span
class="math inline">\(n_{11},n_{12},n_{21},n_{22}\)</span>中只有一个是随机变量，它服从超几何分布。</li>
</ol>
<h4 id="fisher精确检验">Fisher精确检验</h4>
<p>卡方检验和似然比检验都是近似检验，对于<span
class="math inline">\(2×2\)</span>四格表而言，我们还可以推出一个精确检验，这就是Fisher精确检验。</p>
<p>对于如下四格表：</p>
<table>
<thead>
<tr class="header">
<th></th>
<th>B1</th>
<th>B2</th>
<th>合计</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>A1</td>
<td><span class="math inline">\(n_{11}\)</span></td>
<td><span class="math inline">\(n_{12}\)</span></td>
<td><span class="math inline">\(n_{1+}\)</span></td>
</tr>
<tr class="even">
<td>A2</td>
<td><span class="math inline">\(n_{21}\)</span></td>
<td><span class="math inline">\(n_{22}\)</span></td>
<td><span class="math inline">\(n_{2+}\)</span></td>
</tr>
<tr class="odd">
<td>合计</td>
<td><span class="math inline">\(n_{+1}\)</span></td>
<td><span class="math inline">\(n_{+2}\)</span></td>
<td><span class="math inline">\(n\)</span></td>
</tr>
</tbody>
</table>
<p>在已知<span
class="math inline">\(n\)</span>后，如果我们假定行边缘频数<span
class="math inline">\(n_{1+},n_{2+}\)</span>和列边缘频数<span
class="math inline">\(n_{+1},n_{+2}\)</span>均已知，易知<span
class="math inline">\(n_{11},n_{12},n_{21},n_{22}\)</span>只要知道一个，另外三个自然也就知道了，因此<span
class="math inline">\(n_{11},n_{12},n_{21},n_{22}\)</span>中实际上只有一个随机变量，假设为<span
class="math inline">\(n_{11}\)</span>，可以证明<span
class="math inline">\(n_{11}\)</span>服从超几何分布。</p>
<p>根据超几何分布，可以计算<span
class="math inline">\(n_{11}\)</span>取不同值的概率。根据零假设，Fisher精确检验将取不利于零假设的累计概率之和作为最终检验的<span
class="math inline">\(p\)</span>值。</p>
<h4 id="mantel-haenszel-卡方检验">Mantel-Haenszel 卡方检验</h4>
<p>Fisher检验在大样本时有一个渐进形式，即MH检验。根据超几何分布的性质，<span
class="math inline">\(n_{11}\)</span>的期望和方差为： <span
class="math display">\[
\begin{aligned}
E(n_{11})=\frac{n_{+1}n_{1+}}{n}\\
D(n_{11})=\frac{n_{1+}}{n}\frac{n_{2+}}{n}\frac{n_{+1}n_{+2}}{n-1}
\end{aligned}
\]</span> 可以证明，在大样本下<span
class="math inline">\(n_{11}\)</span>渐进正态，因此以下<span
class="math inline">\(Z\)</span>统计量： <span class="math display">\[
Z=\frac{n_{11}-\frac{n_{+1}n_{1+}}{n}}{\sqrt{\frac{n_{1+}}{n}\frac{n_{2+}}{n}\frac{n_{+1}n_{+2}}{n-1}}}\mathop{\rightarrow}^d
N(0,1)
\]</span> 或者卡方统计量： <span class="math display">\[
\chi^2=\frac{(n_{11}-\frac{n_{+1}n_{1+}}{n})^2}{\frac{n_{1+}}{n}\frac{n_{2+}}{n}\frac{n_{+1}n_{+2}}{n-1}}\mathop{\rightarrow}^d
\chi^2(1)
\]</span> 这个检验称为Mantel-Haenszel
卡方检验，实际上它最终化简形式与<span
class="math inline">\(2×2\)</span>的pearson卡方检验只差<span
class="math inline">\(\frac{n-1}{n}\)</span>。</p>
<h4 id="优势比检验">优势比检验</h4>
<p>对于2×2四格表，还可进行优势比检验。假设研究对象有<span
class="math inline">\(A\)</span>和<span
class="math inline">\(B\)</span>两个属性，属性<span
class="math inline">\(A\)</span>有<span
class="math inline">\(A1,A2\)</span>两种取值，属性<span
class="math inline">\(B\)</span>有<span
class="math inline">\(B1,B2\)</span>两种取值，各种取值的概率见下面四格表。</p>
<table>
<thead>
<tr class="header">
<th></th>
<th>B1</th>
<th>B2</th>
<th>合计</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>A1</td>
<td><span class="math inline">\(p_{11}\)</span></td>
<td><span class="math inline">\(p_{12}\)</span></td>
<td><span class="math inline">\(p_{1+}\)</span></td>
</tr>
<tr class="even">
<td>A2</td>
<td><span class="math inline">\(p_{21}\)</span></td>
<td><span class="math inline">\(p_{22}\)</span></td>
<td><span class="math inline">\(p_{2+}\)</span></td>
</tr>
<tr class="odd">
<td>合计</td>
<td><span class="math inline">\(p_{+1}\)</span></td>
<td><span class="math inline">\(p_{+2}\)</span></td>
<td><span class="math inline">\(1\)</span></td>
</tr>
</tbody>
</table>
<p>对象有<span class="math inline">\(A1\)</span>属性的条件下，还有<span
class="math inline">\(B1\)</span>属性的条件概率为<span
class="math inline">\(P(B1|A1)=p_{11}/p_{1+}\)</span>，还有<span
class="math inline">\(B2\)</span>属性的条件概率为<span
class="math inline">\(P(B2|A1)=p_{12}/p_{1+}\)</span>，此时可定义对象有属性<span
class="math inline">\(A1\)</span>时，有属性<span
class="math inline">\(B1\)</span>比有属性<span
class="math inline">\(B2\)</span>的优势为： <span
class="math display">\[
\frac{p_{11}/p_{1+}}{p_{12}/p_{1+}}=p_{11}/p_{12}
\]</span> 同理，对象有属性<span
class="math inline">\(A2\)</span>时，有属性<span
class="math inline">\(B1\)</span>比有属性<span
class="math inline">\(B2\)</span>的优势为： <span
class="math display">\[
\frac{p_{21}/p_{2+}}{p_{22}/p_{2+}}=p_{21}/p_{22}
\]</span> 优势比<span
class="math inline">\(\theta\)</span>为两个优势之比： <span
class="math display">\[
\theta=\frac{p_{11}}{p_{12}}/\frac{p_{21}}{p_{22}}=\frac{p_{11}p_{22}}{p_{12}p_{21}}
\]</span> 当属性<span class="math inline">\(A\)</span>和<span
class="math inline">\(B\)</span>相互独立时，优势比<span
class="math inline">\(\theta=1\)</span>，因此可以检验优势比是否为1来判断两个属性的独立性。</p>
<p>对于样本而言，对于如下四格表：</p>
<table>
<thead>
<tr class="header">
<th></th>
<th>B1</th>
<th>B2</th>
<th>合计</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>A1</td>
<td><span class="math inline">\(n_{11}\)</span></td>
<td><span class="math inline">\(n_{12}\)</span></td>
<td><span class="math inline">\(n_{1+}\)</span></td>
</tr>
<tr class="even">
<td>A2</td>
<td><span class="math inline">\(n_{21}\)</span></td>
<td><span class="math inline">\(n_{22}\)</span></td>
<td><span class="math inline">\(n_{2+}\)</span></td>
</tr>
<tr class="odd">
<td>合计</td>
<td><span class="math inline">\(n_{+1}\)</span></td>
<td><span class="math inline">\(n_{+2}\)</span></td>
<td><span class="math inline">\(n\)</span></td>
</tr>
</tbody>
</table>
<p>优势比的估计值<span class="math inline">\(\hat \theta\)</span>为：
<span class="math display">\[
\hat \theta=\frac{n_{11}n_{22}}{n_{12}n_{21}}
\]</span> 优势比的对数<span class="math inline">\(\sqrt{n}(ln \hat
\theta-ln \theta)\)</span>渐进正态，即： <span class="math display">\[
\sqrt{n}(ln \hat \theta-ln \theta)\mathop{\rightarrow}^d
N(0,\frac{1}{p_{11}}+\frac{1}{p_{12}}+\frac{1}{p_{21}}+\frac{1}{p_{22}})
\]</span> 用样本估计<span
class="math inline">\(p_{11},p_{22},p_{12},p_{21}\)</span>为： <span
class="math display">\[
\sqrt{n}(ln \hat \theta-ln \theta)\mathop{\rightarrow}^d
N(0,\frac{n}{n_{11}}+\frac{n}{n_{12}}+\frac{n}{n_{21}}+\frac{n}{n_{22}})
\]</span> 因此： <span class="math display">\[
ln \hat \theta\mathop{\rightarrow}^d
N(ln\theta,\frac{1}{n_{11}}+\frac{1}{n_{12}}+\frac{1}{n_{21}}+\frac{1}{n_{22}})
\]</span> 当属性<span class="math inline">\(A\)</span>和属性<span
class="math inline">\(B\)</span>独立时，<span
class="math inline">\(\theta=1,ln\theta =0\)</span>，因此： <span
class="math display">\[
ln \hat \theta\mathop{\rightarrow}^d
N(0,\frac{1}{n_{11}}+\frac{1}{n_{12}}+\frac{1}{n_{21}}+\frac{1}{n_{22}})
\]</span> 可构造<span class="math inline">\(Z\)</span>或者卡方统计量：
<span class="math display">\[
\begin{aligned}
Z=\frac{ln \hat
\theta}{\sqrt{\frac{1}{n_{11}}+\frac{1}{n_{12}}+\frac{1}{n_{21}}+\frac{1}{n_{22}}}}\mathop{\rightarrow}^d
N(0,1)\\
\chi^2=Z^2=\frac{[ln \hat
\theta]^2}{\frac{1}{n_{11}}+\frac{1}{n_{12}}+\frac{1}{n_{21}}+\frac{1}{n_{22}}}\mathop{\rightarrow}^d
\chi^2(1)
\end{aligned}
\]</span></p>
<h4
id="cochran-mantel-haenszel-卡方检验分层分析">Cochran-Mantel-Haenszel
卡方检验（分层分析）</h4>
<p>该检验其实是对Mantel-Haenszel 卡方检验的扩展，增加了一个层变量<span
class="math inline">\(C\)</span>。假定层变量<span
class="math inline">\(C\)</span>有<span
class="math inline">\(k\)</span>个取值，这样就有<span
class="math inline">\(k\)</span>个<span
class="math inline">\(2×2\)</span>列联表。检验的目的是给定<span
class="math inline">\(C\)</span>后，属性<span
class="math inline">\(A\)</span>和<span
class="math inline">\(B\)</span>相互独立（条件独立）。</p>
<p>对于<span class="math inline">\(k\)</span>个<span
class="math inline">\(2×2\)</span>列联表，根据Mantel-Haenszel
卡方检验，每个<span
class="math inline">\(2×2\)</span>列联表都可以构造一个<span
class="math inline">\(Z\)</span>检验或者卡方检验，这样就能构造<span
class="math inline">\(k\)</span>个<span
class="math inline">\(Z\)</span>检验或者卡方检验，我们将这<span
class="math inline">\(k\)</span>个检验合并到一块，可以证明： <span
class="math display">\[
Z=\frac{\sum_{i=1}^kn_{i11}-\sum_{i=1}^k\frac{n_{i1+}n_{i+1}}{n_i}}{\sqrt{\sum_{i=1}^k\frac{n_{i1+}}{n_i}\frac{n_{i2+}}{n_i}\frac{n_{i+1}n_{i+2}}{n_i-1}}}\mathop{\rightarrow}^dN(0,1)
\]</span> 这里的<span
class="math inline">\(n_{i11},i=1,2,...,k\)</span>表示第<span
class="math inline">\(i\)</span>层那个随机变量的实际取值，<span
class="math inline">\(n_{i1+}\)</span>表示第<span
class="math inline">\(i\)</span>层第1行合计，<span
class="math inline">\(n_{i2+}\)</span>表示第<span
class="math inline">\(i\)</span>层第2行合计，<span
class="math inline">\(n_{i+1}\)</span>表示第<span
class="math inline">\(i\)</span>层第1列合计，<span
class="math inline">\(n_{i+2}\)</span>表示第<span
class="math inline">\(i\)</span>层第2列合计，<span
class="math inline">\(n_i\)</span>表示第<span
class="math inline">\(i\)</span>层总样本量。</p>
<p>或者可以使用卡方统计量： <span class="math display">\[
\chi^2=Z^2\mathop{\rightarrow}^d \chi^2(1)
\]</span> Cochran-Mantel-Haenszel
卡方检验的前提是各层是同质的，也就是说，无论那一层，属性A和属性B都是条件独立的，只有同质性满足，才能直接合并。这个需要先进行同质性检验，可以用<span
class="math inline">\(Breslow-Day\)</span>方法来进行同质性检验。</p>
<p><span
class="math inline">\(Breslow-Day\)</span>检验其实是对各层的优势比<span
class="math inline">\(\theta_i,i=1,2,...,k\)</span>是否相等进行的检验，零假设为<span
class="math inline">\(H_0:\theta_1=\theta_2=...=\theta_k\)</span>，检验统计量为：
<span class="math display">\[
\chi^2=\sum_{i=1}^k\frac{(\eta_i-\bar
\eta)^2}{Var(\eta_i)}\mathop{\rightarrow}^d\chi^2(k-1)
\]</span> 其中<span class="math inline">\(\eta_i=ln\hat
\theta_i\)</span>，<span
class="math inline">\(Var(\eta_i)=\frac{1}{n_{i11}}+\frac{1}{n_{i12}}+\frac{1}{n_{i21}}+\frac{1}{n_{i22}}\)</span>，<span
class="math inline">\(\bar \eta\)</span>是以方差的导数为权重的<span
class="math inline">\(\eta_i\)</span>的加权平均： <span
class="math display">\[
\bar
\eta=\frac{\sum_{i=1}^k\frac{\eta_i}{Var(\eta_i)}}{\sum_{i=1}^k\frac{1}{Var(\eta_i)}}
\]</span> ##### 混杂因素</p>
<p>在分层分析中，如果层变量与行变量或列变量相关，列变量又与行变量相关。这时候如果不分层，层变量可能会混淆行变量与列变量之间的真实关系，层变量将成为混杂因素。</p>
<h2 id="有序变量-1">有序变量</h2>
<p>两独立样本用Wilcoxon秩和检验，和上面非正态连续变量检验方法相同。多独立样本用Kruskal-Wallis
H检验，见方差分析一文介绍。</p>
<h1 id="配对样本的假设检验">配对样本的假设检验</h1>
<pre class="mermaid">graph LR;
配对样本-->连续变量--计算差值-->正态分布
正态分布--是-->t检验
正态分布--否-->d[1.符号检验</br>2.Wilcoxon符号秩和检验]
配对样本-->分类变量
分类变量--边缘齐性检验-->McNemar检验
分类变量--一致性检验-->Kappa检验
配对样本-->有序变量--一致性检验-->加权Kappa检验</pre>
<h2 id="连续变量-2">连续变量</h2>
<p>主要计算配对之间的差值<span
class="math inline">\(d\)</span>，根据差值的分布决定使用那种检验方法。</p>
<h3 id="差值为正态分布">差值为正态分布</h3>
<p>若差值服从正态分布，则差值的均数应该服从： <span
class="math display">\[
\bar d\sim N(0,\sigma^2/n)
\]</span> 因为<span
class="math inline">\(\sigma^2\)</span>未知，因此可以用t检验： <span
class="math display">\[
T=\frac{\bar d}{S_{d}/\sqrt n}
\]</span></p>
<blockquote>
<p>R中用<code>t.test()</code>并指定参数<code>paired = TRUE</code>时做配对t检验。</p>
</blockquote>
<h3 id="差值不服从正态分布">差值不服从正态分布</h3>
<p>若差值不服从正态分布，可以使用符号检验或者Wilcoxon符号秩检验。</p>
<h4 id="符号检验-1">符号检验</h4>
<p>若<span
class="math inline">\(H_0\)</span>成立，出现差值中出现正号和负号的概率同样各为0.5，然后用伯努利分布的精确检验或者大样本下的<span
class="math inline">\(Z\)</span>检验。</p>
<blockquote>
<p>R中使用<code>binom.test()</code>函数做配对样本的符号检验。</p>
</blockquote>
<h4 id="wilcoxon符号秩和检验">Wilcoxon符号秩和检验</h4>
<p>记录差值的符号，若差值为0则舍去。对差值取绝对值，然后排序记录每个差值的秩次（若差值的绝对值相等取平均秩次）。记删除差值为0之后的样本数目为n，显然总的秩次和为<span
class="math inline">\(\frac{n(n+1)}{2}\)</span>，然后统计符号为正的秩次之和。若<span
class="math inline">\(H_0\)</span>成立，大样本下（<span
class="math inline">\(n\gt 50\)</span>）符号为正的秩次之和<span
class="math inline">\(T_+\)</span>近似服从： <span
class="math display">\[
T_+\sim N(\frac{n(n+1)}{4},\frac{n(n+1)(2n+1)}{4})
\]</span> 因此，使用<span class="math inline">\(Z\)</span>检验为： <span
class="math display">\[
Z=\frac{T_+-\frac{n(n+1)}{4}-0.5}{\frac{n(n+1)(2n+1)}{4}}
\]</span>
0.5为连续性矫正。取差值之后，可能有多个差值绝对值相等，称为“结”，若“结”较多时，上述公式需要修正。</p>
<blockquote>
<p>R中用<code>wilcox.test()</code>指定参数<code>paired=T</code>做配对样本的wilcoxon检验。</p>
</blockquote>
<h2 id="分类变量-2">分类变量</h2>
<h3 id="边缘齐性检验">边缘齐性检验</h3>
<p>McNeMar检验主要用于<span
class="math inline">\(2×2\)</span>四格表。对于下面的配对四格表：</p>
<table>
<thead>
<tr class="header">
<th></th>
<th>B1</th>
<th>B2</th>
<th>合计</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>A1</td>
<td><span class="math inline">\(p_{11}\)</span></td>
<td><span class="math inline">\(p_{12}\)</span></td>
<td><span class="math inline">\(p_{1+}\)</span></td>
</tr>
<tr class="even">
<td>A2</td>
<td><span class="math inline">\(p_{21}\)</span></td>
<td><span class="math inline">\(p_{22}\)</span></td>
<td><span class="math inline">\(p_{2+}\)</span></td>
</tr>
<tr class="odd">
<td>合计</td>
<td><span class="math inline">\(p_{+1}\)</span></td>
<td><span class="math inline">\(p_{+2}\)</span></td>
<td><span class="math inline">\(1\)</span></td>
</tr>
</tbody>
</table>
<p>McNeMar检验主要检验边缘齐性，即： <span class="math display">\[
p_{1+}=p_{+1}\rightarrow p_{11}+p_{12}=p_{11}+p_{21}\\
p_{2+}=p_{+2}\rightarrow p_{12}+p_{22}=p_{21}+p_{22}
\]</span> 对于样本数据：</p>
<table>
<thead>
<tr class="header">
<th></th>
<th>B1</th>
<th>B2</th>
<th>合计</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>A1</td>
<td><span class="math inline">\(n_{11}\)</span></td>
<td><span class="math inline">\(n_{12}\)</span></td>
<td><span class="math inline">\(n_{1+}\)</span></td>
</tr>
<tr class="even">
<td>A2</td>
<td><span class="math inline">\(n_{21}\)</span></td>
<td><span class="math inline">\(n_{22}\)</span></td>
<td><span class="math inline">\(n_{2+}\)</span></td>
</tr>
<tr class="odd">
<td>合计</td>
<td><span class="math inline">\(n_{+1}\)</span></td>
<td><span class="math inline">\(n_{+2}\)</span></td>
<td><span class="math inline">\(n\)</span></td>
</tr>
</tbody>
</table>
<p>统计量为： <span class="math display">\[
\chi^2=\frac{(n_{12}-n_{21})^2}{n_{12}+n_{21}}\mathop{\rightarrow}^d\chi^2(1)
\]</span> 当$n_{12}+n_{21}$40时，需要使用矫正公式： <span
class="math display">\[
\chi_C^2=\frac{(|n_{12}-n_{21}|-1)^2}{n_{12}+n_{21}}\mathop{\rightarrow}^d\chi^2(1)
\]</span></p>
<h3 id="一致性检验">一致性检验</h3>
<p>主要用于方表的一致性比较，典型例子就是两位检验员<span
class="math inline">\(A\)</span>和<span
class="math inline">\(B\)</span>对同一份样本进行检验，对两位检验员检验结果的一致性进行评价。最终得到如下的方表：</p>
<table>
<thead>
<tr class="header">
<th></th>
<th>B1</th>
<th>B2</th>
<th>...</th>
<th>Bk</th>
<th>合计</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>A1</td>
<td><span class="math inline">\(n_{11}\)</span></td>
<td><span class="math inline">\(n_{12}\)</span></td>
<td>...</td>
<td><span class="math inline">\(n_{1k}\)</span></td>
<td><span class="math inline">\(n_{1+}\)</span></td>
</tr>
<tr class="even">
<td>A2</td>
<td><span class="math inline">\(n_{21}\)</span></td>
<td><span class="math inline">\(n_{22}\)</span></td>
<td>...</td>
<td><span class="math inline">\(n_{2k}\)</span></td>
<td><span class="math inline">\(n_{2+}\)</span></td>
</tr>
<tr class="odd">
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
</tr>
<tr class="even">
<td>Ak</td>
<td><span class="math inline">\(n_{k1}\)</span></td>
<td>...</td>
<td>...</td>
<td><span class="math inline">\(n_{kk}\)</span></td>
<td><span class="math inline">\(n_{k+}\)</span></td>
</tr>
<tr class="odd">
<td>合计</td>
<td><span class="math inline">\(n_{+1}\)</span></td>
<td><span class="math inline">\(n_{+2}\)</span></td>
<td>...</td>
<td><span class="math inline">\(n_{+k}\)</span></td>
<td><span class="math inline">\(n\)</span></td>
</tr>
</tbody>
</table>
<p>行表示<span
class="math inline">\(A\)</span>检验员的检验结果，列表示<span
class="math inline">\(B\)</span>检验员的检验结果。很显然二位检验员之间的一致性主要体现在方表的对角线上，可用二者一致的数目占总样本的比例来评价一致性：
<span class="math display">\[
q1=\frac{n_{11}+n_{22}+...+n_{kk}}{n}
\]</span> <span
class="math inline">\(q_{11}\)</span>显然是有问题的，因为如果两位检验员都是随机检验，仍然可能有部分样本被两位检验员检出相同的结果，即方表的主对角线上不可能为0。话句话说，我们需要从<span
class="math inline">\(q1\)</span>中扣除随机导致的一致性。可以证明对角线上的元素<span
class="math inline">\(n_{ii},i=1,2,...,k\)</span>的期望为： <span
class="math display">\[
E(n_{ii})=\frac{n_{+i}n_{i+}}{n}
\]</span> 那么随机导致的一致性为： <span class="math display">\[
q2=\frac{E(n_{11})+...+E(n_{kk})}{n}=\frac{\sum_{i=1}^kn_{+i}n_{i+}}{n^2}
\]</span> 实际的一致性为<span class="math inline">\(q1-q2\)</span>：
<span class="math display">\[
q1-q2=\frac{\sum_{i=1}^kn_{ii}}{n}-\frac{\sum_{i=1}^kn_{+i}n_{i+}}{n^2}
\]</span> <span class="math inline">\(q1-q2\)</span>的最大值为<span
class="math inline">\(1-q2\)</span>，我们将<span
class="math inline">\(q1-q2\)</span>除以最大值，可以将统计量归一化到<span
class="math inline">\([0,1]\)</span>，这就是<span
class="math inline">\(cohen\)</span>等人提出的<span
class="math inline">\(kappa\)</span>统计量： <span
class="math display">\[
kappa=\frac{\frac{\sum_{i=1}^kn_{ii}}{n}-\frac{\sum_{i=1}^kn_{+i}n_{i+}}{n^2}}{1-\frac{\sum_{i=1}^kn_{+i}n_{i+}}{n^2}}
\]</span>
很显然，当两个检验员都是随机检验时，kappa值为0，当非主对角线元素都为0时，kappa值为1。</p>
<p><span
class="math inline">\(kappa\)</span>检验的零假设为：两位检验员均为随机检验，此时kappa统计量分子为0，kappa的期望为0。可以证明kappa值的方差为：
<span class="math display">\[
\hat
Var(kappa)=\frac{1}{n-1}\frac{q_2+q_2^2-\sum_{i=1}^kp_{i+}p_{+i}(p_{i+}+p_{+i})}{(1-q_2)^2}
\]</span> 同样可以证明，<span
class="math inline">\(kappa\)</span>统计量的渐进分布同样为正态分布，因此构造<span
class="math inline">\(Z\)</span>统计量进行检验： <span
class="math display">\[
Z=\frac{kappa}{\sqrt{\hat {Var}(kappa)}}\mathop{\rightarrow}^d N(0,1)
\]</span> 也可以将<span
class="math inline">\(Z\)</span>平方得到卡方检验。</p>
<h2 id="有序变量-2">有序变量</h2>
<h3 id="一致性检验-1">一致性检验</h3>
<p>使用加权kappa检验。</p>
<h1 id="双变量的相关性相合性检验">双变量的相关性/相合性检验</h1>
<pre class="mermaid">graph LR;
双变量的相关性检验-->连续变量
连续变量--正态分布-->Pearson矩相关系数
连续变量--非正态分布-->Spearman秩相关系数
双变量的相关性检验-->分类变量
分类变量-->Φ系数
分类变量-->CramerV系数
分类变量-->Pearson列联系数
双变量的相关性检验-->有序变量
有序变量-->Spearman秩相关系数
有序变量-->Gamma系数
有序变量-->Somers'D系数
有序变量-->τa系数
有序变量-->τb系数
有序变量-->τc系数</pre>
<h2 id="连续变量-3">连续变量</h2>
<p>若数据服从双变量正态分布，则可以使用<span
class="math inline">\(Pearson\)</span>矩相关系数，否则使用<span
class="math inline">\(Spearman\)</span>秩相关系数。</p>
<h3 id="正态分布-2">正态分布</h3>
<p>公式为： <span class="math display">\[
r=\frac{\sum_{i=1}^n(x_i-\bar x)(y_i-\bar
y)}{\sqrt{\sum_{i=1}^n(x_i-\bar x)^2}\sqrt{\sum_{i=1}^n(y_i-\bar y)^2}}
\]</span> 得到样本相关系数的<span
class="math inline">\(r\)</span>后，还需要对总体相关系数<span
class="math inline">\(\rho=0\)</span>进行假设检验，样本量较大时<span
class="math inline">\(n\gt 50\)</span>，一般用的是<span
class="math inline">\(t\)</span>检验： <span class="math display">\[
t=\frac{r-0}{S_r}\\
S_r=\sqrt{\frac{1-r^2}{n-2}}
\]</span></p>
<h3 id="非正态分布-2">非正态分布</h3>
<p>基本思路是对两个变量<span class="math inline">\(x\)</span>和<span
class="math inline">\(y\)</span>编秩，然后用秩代替原始值带入<span
class="math inline">\(Pearson\)</span>矩相关系数公式中，由此得到的是秩相关系数，它的含义与<span
class="math inline">\(Pearson\)</span>矩相关系数类似。秩相关系数也可以做假设检验，样本量较大时也是使用<span
class="math inline">\(t\)</span>检验。</p>
<h2 id="分类变量-3">分类变量</h2>
<p>对于两个分类变量之间的关联性分析，主要是做两种属性的<span
class="math inline">\(\chi^2\)</span>检验（假定各单元格服从多项分布），然后计算关联系数，常用的关联系数有：</p>
<h3 id="phi系数"><span class="math inline">\(\phi\)</span>系数</h3>
<p>对于四格表，可以用<span
class="math inline">\(\phi\)</span>系数，公式为： <span
class="math display">\[
\phi=\sqrt{\frac{\chi^2}{n}}
\]</span> <span class="math inline">\(\chi^2\)</span>为<span
class="math inline">\(Pearson\quad\chi^2\)</span>值，<span
class="math inline">\(n\)</span>为样本总列数。</p>
<h3 id="cramerquad-v系数"><span class="math inline">\(Cramer\quad
V\)</span>系数</h3>
<p>对于多行多列的列联表，可以用<span class="math inline">\(Cramer\quad
V\)</span>系数，公式为： <span class="math display">\[
V=\sqrt{\frac{\chi^2}{n(k-1)}},\quad k=min(R,C)
\]</span> <span
class="math inline">\(k\)</span>为行数或列数的最小值。</p>
<h3 id="pearson列联系数"><span
class="math inline">\(Pearson\)</span>列联系数</h3>
<p><span class="math inline">\(Pearson\)</span>列联系数
一般无法取到1，因此用的不多，公式为： <span class="math display">\[
r=\sqrt{\frac{\chi^2}{\chi^2+n}}
\]</span> 这些系数理论上也应该做假设检验，实际上已经与<span
class="math inline">\(\chi^2\)</span>检验等价。</p>
<h2 id="有序变量-3">有序变量</h2>
<p>对于有序变量之间的相关性，有时候也称为相合性。相合性高意味着行变量等级高列变量等级也会高。行变量等级高列变量等级也高称为正相合，行变量等级高列变量等级低称为负相合。</p>
<p>测量有序变量之间相合性的指标有很多，但他们都是以<span
class="math inline">\(kendall&#39;s\quad
\tau_a\)</span>系数衍生出来的，为了推导这些指标，需要先介绍一些概念。</p>
<h3 id="一些概念">一些概念</h3>
<p>如果一个点，由<span class="math inline">\(x\)</span>和<span
class="math inline">\(y\)</span>组成的坐标<span
class="math inline">\((x,y)\)</span>代表，一对点就是<span
class="math inline">\((x_1,y_1)\)</span>和<span
class="math inline">\((x_2,y_2)\)</span>这样的点对。假设有<span
class="math inline">\(n\)</span>个样本点，这样的点对将有<span
class="math inline">\(C_n^2=\frac{n(n-1)}{2}\)</span>个，我们令其为<span
class="math inline">\(T=\frac{n(n-1)}{2}\)</span>。我们考察每一对中<span
class="math inline">\((x,y)\)</span>是否同时增加或减小。例如有点对<span
class="math inline">\((x_1,y_1)\)</span>和<span
class="math inline">\((x_2,y_2)\)</span>，可以计算乘积<span
class="math inline">\((x_1-x_2)(y_1-y_2)\)</span>，判断乘积是否大于0，对于这<span
class="math inline">\(T\)</span>对，有下面这几种情况：</p>
<ul>
<li>乘积大于0，我们称这一点对为同序对；</li>
<li>乘积小于0，我们称这一点对为异序对；</li>
<li>乘积为0，此时意味这这一对点要么<span
class="math inline">\(x\)</span>坐标相同，要么<span
class="math inline">\(y\)</span>坐标相同，我们称这一点对为同分对；</li>
</ul>
<p>我们记同序对个数为<span
class="math inline">\(P\)</span>，异序对个数为<span
class="math inline">\(Q\)</span>，<span
class="math inline">\(x\)</span>坐标相同的同分对个数为<span
class="math inline">\(P_x\)</span>，<span
class="math inline">\(y\)</span>坐标相同的同分对个数为<span
class="math inline">\(P_y\)</span>，<span
class="math inline">\(x,y\)</span>坐标都相同的同分对个数为<span
class="math inline">\(P_{xy}\)</span>，则有： <span
class="math display">\[
P+Q+P_{x}+P_{y}-P_{xy}=T
\]</span> 因为<span
class="math inline">\(x,y\)</span>坐标都相同的点对会同时计入<span
class="math inline">\(P_x\)</span>和<span
class="math inline">\(P_y\)</span>，相当于算了两次，所以最后减掉一个<span
class="math inline">\(P_{xy}\)</span>。</p>
<h3 id="kendallsquad-tau_a"><span
class="math inline">\(kendall&#39;s\quad \tau_a\)</span></h3>
<p>计算公式为： <span class="math display">\[
\tau_a=\frac{P-Q}{T}
\]</span> 当所有点的<span class="math inline">\(x\)</span>和<span
class="math inline">\(y\)</span>坐标都不同，此时<span
class="math inline">\(P_x=P_y=P_{xy}=0\)</span>，当<span
class="math inline">\(Q=0\)</span>时，<span
class="math inline">\(P=n(n-1)2\)</span>，<span
class="math inline">\(\tau_a\)</span>取得最大值<span
class="math inline">\(1\)</span>，当<span
class="math inline">\(P=0\)</span>时，<span
class="math inline">\(\tau_a\)</span>取得最小值<span
class="math inline">\(-1\)</span>。然而数据中常常有一些点的<span
class="math inline">\(x\)</span>坐标或<span
class="math inline">\(y\)</span>坐标是相同的，这将导致<span
class="math inline">\(\tau_a\)</span>取不到最大值<span
class="math inline">\(1\)</span>或最小值<span
class="math inline">\(-1\)</span>，这是<span
class="math inline">\(\tau_a\)</span>系数的缺点。</p>
<h3 id="kendallsquad-tau_b"><span
class="math inline">\(kendall&#39;s\quad \tau_b\)</span></h3>
<p>针对<span
class="math inline">\(\tau_a\)</span>往往取不到1或-1的问题，<span
class="math inline">\(\tau_b\)</span>对<span
class="math inline">\(\tau_a\)</span>的分母进行了修正，实际上就是从总对数中减去<span
class="math inline">\(x\)</span>坐标或<span
class="math inline">\(y\)</span>坐标相同的点对，计算公式为： <span
class="math display">\[
\tau_b=\frac{P-Q}{\sqrt{T-P_x}\sqrt{T-P_y}}
\]</span> 可以发现<span class="math inline">\(P_x\)</span>和<span
class="math inline">\(P_y\)</span>为0时，<span
class="math inline">\(\tau_b\)</span>退化为<span
class="math inline">\(\tau_a\)</span>。观察这个公式容易发现，只有当<span
class="math inline">\(P_x=P_y=P_{xy}\)</span>且<span
class="math inline">\(Q=0\)</span>时，<span
class="math inline">\(P=T-P_x\)</span>，此时<span
class="math inline">\(\tau_b=1\)</span>，当<span
class="math inline">\(P_x=P_y=P_{xy}\)</span>且<span
class="math inline">\(P=0\)</span>时，<span
class="math inline">\(Q=T-P_x\)</span>，此时<span
class="math inline">\(\tau_b=-1\)</span>。而<span
class="math inline">\(P_x=P_y=P_{xy}\)</span>时，意味着列联表必须为方表才行，对于长方形表，<span
class="math inline">\(\tau_b\)</span>无法取到±1，这是<span
class="math inline">\(\tau_b\)</span>系数的缺点。</p>
<h3 id="kendallsquad-tau_c"><span
class="math inline">\(kendall&#39;s\quad \tau_c\)</span></h3>
<p>公式为： <span class="math display">\[
\tau_c=\frac{P-Q}{\frac{n^2}{2}\frac{m-1}{m}}
\]</span> <span
class="math inline">\(m\)</span>表示列联表中较小的行数（或列数）。<span
class="math inline">\(\tau_c\)</span>一般用于长方形表的相合性评价。</p>
<h3 id="gamma系数"><span class="math inline">\(gamma\)</span>系数</h3>
<p>计算公式为： <span class="math display">\[
gamma=\frac{P-Q}{P+Q}
\]</span></p>
<h3 id="somers-d系数">Somers' D系数</h3>
<p>Somers'
D系数也主要用于长方形表的相合性评价，它是一种不对称相合关系的评价指标。可以计算两个系数<span
class="math inline">\(d_{xy},d_{yx}\)</span>： <span
class="math display">\[
\begin{aligned}
d_{xy}=\frac{P-Q}{T-P_y}\\
d_{yx}=\frac{P-Q}{T-P_x}
\end{aligned}
\]</span> <span class="math inline">\(d_{xy}\)</span>表示<span
class="math inline">\(y\)</span>为自变量<span
class="math inline">\(x\)</span>为因变量时的情况，<span
class="math inline">\(d_{yx}\)</span>为自变量为<span
class="math inline">\(x\)</span>因变量为<span
class="math inline">\(y\)</span>时的情况。</p>
<h3 id="相合性检验">相合性检验</h3>
<p>上面介绍的5种相合系数都需要做假设检验，由于公式十分复杂，这里就不过多介绍了，感兴趣的可以参考其他文档。</p>
]]></content>
      <categories>
        <category>数理统计</category>
      </categories>
      <tags>
        <tag>数理统计</tag>
      </tags>
  </entry>
  <entry>
    <title>Hello World</title>
    <url>/2022/12/23/hello-world/</url>
    <content><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very
first post. Check <a href="https://hexo.io/docs/">documentation</a> for
more info. If you get any problems when using Hexo, you can find the
answer in <a
href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or
you can ask me on <a
href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p>
<span id="more"></span>
<h2 id="quick-start">Quick Start</h2>
<h3 id="create-a-new-post">Create a new post</h3>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure>
<p>More info: <a
href="https://hexo.io/docs/writing.html">Writing</a></p>
<h3 id="run-server">Run server</h3>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p>
<h3 id="generate-static-files">Generate static files</h3>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>
<p>More info: <a
href="https://hexo.io/docs/generating.html">Generating</a></p>
<h3 id="deploy-to-remote-sites">Deploy to remote sites</h3>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>
<p>More info: <a
href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>
]]></content>
  </entry>
  <entry>
    <title>硬间隔支持向量机</title>
    <url>/2023/01/12/%E7%A1%AC%E9%97%B4%E9%9A%94SVM/</url>
    <content><![CDATA[<p>​</p>
<span id="more"></span>
<h1 id="模型">模型</h1>
<p>SVM模型其实是寻找一个超平面： <span class="math display">\[
\vec x^T\vec w+b=0
\]</span> 然后用<span
class="math inline">\(sign\)</span>函数来帮助决策： <span
class="math display">\[
sign(\vec x^T\vec w+b)
\]</span> 当<span class="math inline">\(z\gt0\)</span>时，就判为为<span
class="math inline">\(+1\)</span>，当<span class="math inline">\(z\lt
0\)</span>时判为<span class="math inline">\(-1\)</span>。</p>
<h1 id="损失">损失</h1>
<p>在SVM模型中，如果将所有的正样本都分配到超平面<span
class="math inline">\(\vec x^T\vec
w+b=0\)</span>的一侧，所有的负样本都分配到超平面<span
class="math inline">\(\vec x^T\vec
w+b=0\)</span>的另一侧，就表示所有样本都分类正确了。容易发现，这时候的超平面满足：
<span class="math display">\[
y_i(\vec x_i^T\vec w+b)\gt0 \quad i=1,2,...,n
\]</span>
很显然这样的超平面有很多个，我们希望从中找到一个最优的超平面。怎么定义这个最优的超平面呢？一个自然的想法就是要求每一个数据点距离超平面都应该尽可能的远，只有这样才能保证所有数据都能分类正确，特别是对于那些本身距离超平面很接近的“危险点”。如果再深入思考一下，容易发现，其实我们只要能够保证距离超平面最近的数据点距离超平面尽可能的远，就能够实现这个目的了。由于超平面位于正负样本之间，因此我们需要既保证超平面距离最近负样本尽可能的远，也需要保证超平面距最近正样本尽可能的远，因此这个超平面只能过最近负样本和最近正样本连线的中点处，且与最近正样本和最近负样本距离相等才行，记这个距离为“间隔”，用字母<span
class="math inline">\(d\)</span>表示，它表示了最近正样本或最近负样本距离超平面的距离。已知点到超平面的距离公式为：
<span class="math display">\[
d=\frac{|\vec x^T\vec w+b|}{||\vec w||_2}
\]</span> 我们需要做的就是最大化这个间隔<span
class="math inline">\(d\)</span>。为了方便运算，我们可以令公式的分子部分等于1，即：
<span class="math display">\[
d=\frac{1}{||\vec w||_2}
\]</span>
这是可行的，因为分母部分会自动进行缩放，并不影响最终的超平面。进一步，最大化<span
class="math inline">\(d\)</span>，等价于： <span class="math display">\[
\mathop{min}_{\vec w,b}\frac{1}{2}\vec w^T\vec w
\]</span> 别忘了还有约束条件呢。由于最近正样本/负样本<span
class="math inline">\(x_j\)</span>满足<span class="math inline">\(|\vec
x_j^T\vec w+b|=1\)</span>，因此对于所有样本点都会有<span
class="math inline">\(|\vec x_i^T\vec w+b|\ge
1\)</span>，最终SVM实际上要解决的是： <span class="math display">\[
\mathop{min}_{\vec w,b}\frac{1}{2}\vec w^T\vec w\\
s.t. \quad y_i(\vec x_i^T\vec w+b)\ge 1\quad i=1,2,...,n
\]</span> 我们定义这个约束优化问题为SVM的原问题。</p>
<h1 id="优化">优化</h1>
<p>容易发现SVM最终是一个约束优化问题，我们通常使用拉格朗日对偶来解决。首先构造拉格朗日函数：
<span class="math display">\[
L=\frac{1}{2}\vec w^T\vec w+\sum_{i=1}^n\alpha_i(1-y_i(\vec x_i^T\vec
w+b))
\]</span> 上述约束优化问题实际上等价于： <span class="math display">\[
\mathop{min}_{\vec w,b}\mathop{max}_{\alpha_i}L\\
s.t.\quad \alpha_i\ge 0\quad i,=1,2,...,n
\]</span> 这是显然易见的。因为如果<span class="math inline">\(\vec
x_i\)</span>满足约束条件<span class="math inline">\(1-y_i(\vec x_i^T\vec
w+b)\le0\)</span>，则<span class="math inline">\(\alpha_i(1-y_i(\vec
x_i^T\vec w+b))\le 0\)</span>，这时候<span
class="math inline">\(\mathop{max}_{\alpha}L\)</span>就要要求<span
class="math inline">\(\sum_{i=1}^n\alpha_i(1-y_i(\vec x_i^T\vec
w+b))=0\)</span>，这时候<span
class="math inline">\(\mathop{max}_{\alpha_i}L=\frac{1}{2}\vec w^T\vec
w\)</span>。</p>
<p>相反，如果<span class="math inline">\(\vec
x_i\)</span>不满足约束条件，即<span class="math inline">\(1-y_i(\vec
x_i^T\vec w+b)\gt0\)</span>，则<span
class="math inline">\(\alpha_i(1-y_i(\vec x_i^T\vec w+b))\gt
0\)</span>，则<span class="math inline">\(max_{\alpha_i}L\rightarrow
+\infty\)</span>。</p>
<p>进而再取最小值<span class="math inline">\(\mathop{min}_{\vec
w,b}\)</span>时，其实就是<span class="math inline">\(min_{\vec
w,b}(\frac{1}{2}\vec w^T\vec w,+\infty)=min_{\vec w,b}\frac{1}{2}\vec
w^T\vec w\)</span>，因此与上面推导的约束问题等价。</p>
<p>拉格朗日对偶问题为： <span class="math display">\[
\mathop{max}_{\alpha_i}\mathop{min}_{\vec w,b}L\\
s.t.\quad \alpha_i\ge 0\quad i,=1,2,...,n
\]</span>
可以证明，对于SVM，原问题和拉格朗日对偶问题的最优解和最优值都是相同的（参见凸优化教程）。因此对原问题的求解
可以转化为对拉格朗日对偶问题的求解。</p>
<p>先看内层<span class="math inline">\(min_{\vec
w,b}L\)</span>，容易发现它是关于<span class="math inline">\(\vec
w,b\)</span>的凸函数，因此最优解在梯度为0处取得： <span
class="math display">\[
\frac{\partial L}{\partial \vec w}=\vec w-\sum_{i=1}^n\alpha_iy_i\vec
x_i\\
\frac{\partial L}{\partial b}=\sum_{i=1}^n\alpha_iy_i
\]</span> 可得： <span class="math display">\[
\vec w=\sum_{i=1}^n\alpha_iy_i\vec x_i\\
\sum_{i=1}^n\alpha_iy_i=0
\]</span> 将得到的解带回到拉格朗日函数中，可以得到： <span
class="math display">\[
L=\frac{1}{2}\sum_{i=1}^n\alpha_iy_i\vec
x_i^T\sum_{j=1}^n\alpha_jy_j\vec
x_j^T+\sum_{i=1}^n\alpha_i-\sum_{i=1}^n\alpha_iy_i\vec
x_i^T\sum_{j=1}^n\alpha_jy_j\vec x_j-b\sum_{i=1}^n\alpha_iy_i
\]</span> 整理得： <span class="math display">\[
L=\frac{1}{2}\sum_{i=1}^n\sum_{j=1}^n\alpha_i\alpha_jy_iy_j\vec
x_i^T\vec
x_j+\sum_{i=1}^n\alpha_i-\sum_{i=1}^n\sum_{j=1}^n\alpha_i\alpha_jy_iy_j\vec
x_i^T\vec x_j\\
=\sum_{i=1}^n\alpha_i-\frac{1}{2}\sum_{i=1}^n\sum_{j=1}^n\alpha_i\alpha_jy_iy_j\vec
x_i^T\vec x_j
\]</span> 再看外层是针对<span
class="math inline">\(max_{\alpha_i}\)</span>的最大化问题，因此问题转换为：
<span class="math display">\[
\mathop{max}_{\alpha_i}[\sum_{i=1}^n\alpha_i-\frac{1}{2}\sum_{i=1}^n\sum_{j=1}^n\alpha_i\alpha_jy_iy_j\vec
x_i^T\vec x_j]
\]</span> 别忘了还有两个约束条件：<span
class="math inline">\(\alpha_i\ge
0,\sum_{i=1}^n\alpha_iy_i=0\)</span>。由于我们习惯求最小化，因此问题还等价于：
<span class="math display">\[
\mathop{min}_{\alpha_i}[\frac{1}{2}\sum_{i=1}^n\sum_{j=1}^n\alpha_i\alpha_jy_iy_j\vec
x_i^T\vec x_j-\sum_{i=1}^n\alpha_i]\\
s.t.\quad \alpha_i\ge0\\
\sum_{i=1}^n\alpha_iy_i=0,\quad i=1,2,...,n
\]</span></p>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>分类</tag>
      </tags>
  </entry>
  <entry>
    <title></title>
    <url>/2023/03/09/ggplot2/</url>
    <content><![CDATA[
]]></content>
  </entry>
</search>
