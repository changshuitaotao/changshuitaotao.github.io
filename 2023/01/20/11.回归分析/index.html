<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 6.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">

<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.2.1/css/all.min.css" integrity="sha256-Z1K5uhUaJXA7Ll0XrZ/0JhX4lAtZFpT6jkKrEDT0drU=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"example.com","root":"/","images":"/images","scheme":"Pisces","darkmode":false,"version":"8.14.2","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":{"enable":true,"style":null},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"/search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script>

    <meta name="description" content="​">
<meta property="og:type" content="article">
<meta property="og:title" content="回归分析">
<meta property="og:url" content="http://example.com/2023/01/20/11.%E5%9B%9E%E5%BD%92%E5%88%86%E6%9E%90/index.html">
<meta property="og:site_name" content="长水滔滔的博客">
<meta property="og:description" content="​">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2023-01-19T16:00:00.000Z">
<meta property="article:modified_time" content="2023-01-22T15:36:29.803Z">
<meta property="article:author" content="长水滔滔">
<meta property="article:tag" content="回归">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="http://example.com/2023/01/20/11.%E5%9B%9E%E5%BD%92%E5%88%86%E6%9E%90/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"http://example.com/2023/01/20/11.%E5%9B%9E%E5%BD%92%E5%88%86%E6%9E%90/","path":"2023/01/20/11.回归分析/","title":"回归分析"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>回归分析 | 长水滔滔的博客</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">长水滔滔的博客</p>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%A8%A1%E5%9E%8B"><span class="nav-number">1.</span> <span class="nav-text">模型</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%81%87%E5%AE%9A"><span class="nav-number">2.</span> <span class="nav-text">假定</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1"><span class="nav-number">3.</span> <span class="nav-text">参数估计</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BC%B0%E8%AE%A1%E9%87%8F%E7%9A%84%E6%80%A7%E8%B4%A8"><span class="nav-number">3.1.</span> <span class="nav-text">估计量的性质</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#ols%E7%9A%84%E6%AD%A3%E4%BA%A4%E6%80%A7"><span class="nav-number">3.2.</span> <span class="nav-text">OLS的正交性</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%B9%B3%E6%96%B9%E5%92%8C%E5%88%86%E8%A7%A3"><span class="nav-number">3.3.</span> <span class="nav-text">平方和分解</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%81%87%E8%AE%BE%E6%A3%80%E9%AA%8C"><span class="nav-number">4.</span> <span class="nav-text">假设检验</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#f%E6%A3%80%E9%AA%8C"><span class="nav-number">4.1.</span> <span class="nav-text">F检验</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#t%E6%A3%80%E9%AA%8C"><span class="nav-number">4.2.</span> <span class="nav-text">t检验</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%81%8F%E5%86%B3%E5%AE%9A%E7%B3%BB%E6%95%B0%E4%B8%8E%E5%81%8Ff%E6%A3%80%E9%AA%8C"><span class="nav-number">4.3.</span> <span class="nav-text">偏决定系数与偏F检验</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%81%8F%E7%9B%B8%E5%85%B3%E7%B3%BB%E6%95%B0"><span class="nav-number">4.4.</span> <span class="nav-text">偏相关系数</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%8B%9F%E5%90%88%E4%BC%98%E5%BA%A6"><span class="nav-number">5.</span> <span class="nav-text">拟合优度</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%86%B3%E5%AE%9A%E7%B3%BB%E6%95%B0r2"><span class="nav-number">5.1.</span> <span class="nav-text">决定系数\(R^2\)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%B0%83%E6%95%B4%E5%86%B3%E5%AE%9A%E7%B3%BB%E6%95%B0r_a2"><span class="nav-number">5.2.</span> <span class="nav-text">调整决定系数\(R_a^2\)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%A4%8D%E7%9B%B8%E5%85%B3%E7%B3%BB%E6%95%B0"><span class="nav-number">5.3.</span> <span class="nav-text">复相关系数</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%97%A0%E5%B8%B8%E6%95%B0%E9%A1%B9%E7%9A%84%E5%9B%9E%E5%BD%92"><span class="nav-number">5.4.</span> <span class="nav-text">无常数项的回归</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%A8%A1%E5%9E%8B%E9%80%89%E6%8B%A9"><span class="nav-number">6.</span> <span class="nav-text">模型选择</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%85%A8%E5%AD%90%E9%9B%86%E5%9B%9E%E5%BD%92"><span class="nav-number">6.1.</span> <span class="nav-text">全子集回归</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%B0%83%E6%95%B4r2_a"><span class="nav-number">6.1.1.</span> <span class="nav-text">调整\(R^2_a\)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#hat-sigma2"><span class="nav-number">6.1.2.</span> <span class="nav-text">\(\hat
\sigma^2\)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%B5%A4%E6%B1%A0%E4%BF%A1%E6%81%AF%E5%87%86%E5%88%99aic"><span class="nav-number">6.1.3.</span> <span class="nav-text">赤池信息准则AIC</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#c_p%E7%BB%9F%E8%AE%A1%E9%87%8F"><span class="nav-number">6.1.4.</span> <span class="nav-text">\(C_p\)统计量</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%80%90%E6%AD%A5%E5%9B%9E%E5%BD%92"><span class="nav-number">6.2.</span> <span class="nav-text">逐步回归</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%89%8D%E8%BF%9B%E6%B3%95"><span class="nav-number">6.2.1.</span> <span class="nav-text">前进法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%90%8E%E9%80%80%E6%B3%95"><span class="nav-number">6.2.2.</span> <span class="nav-text">后退法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%80%90%E6%AD%A5%E6%B3%95"><span class="nav-number">6.2.3.</span> <span class="nav-text">逐步法</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%9B%9E%E5%BD%92%E8%AF%8A%E6%96%AD"><span class="nav-number">7.</span> <span class="nav-text">回归诊断</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%A8%A1%E5%9E%8B%E9%80%82%E7%94%A8%E6%9D%A1%E4%BB%B6%E5%88%A4%E6%96%AD"><span class="nav-number">7.1.</span> <span class="nav-text">模型适用条件判断</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%BC%82%E6%96%B9%E5%B7%AE%E6%A3%80%E9%AA%8C%E6%96%B9%E5%B7%AE%E9%BD%90%E6%80%A7"><span class="nav-number">7.1.1.</span> <span class="nav-text">异方差（检验方差齐性）</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%9B%BE%E7%A4%BA%E6%B3%95"><span class="nav-number">7.1.1.1.</span> <span class="nav-text">图示法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%81%87%E8%AE%BE%E6%A3%80%E9%AA%8C%E6%B3%95"><span class="nav-number">7.1.1.2.</span> <span class="nav-text">假设检验法</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%87%AA%E7%9B%B8%E5%85%B3%E6%A3%80%E9%AA%8C%E7%8B%AC%E7%AB%8B%E6%80%A7"><span class="nav-number">7.1.2.</span> <span class="nav-text">自相关（检验独立性）</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%9B%BE%E7%A4%BA%E6%B3%95-1"><span class="nav-number">7.1.2.1.</span> <span class="nav-text">图示法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%81%87%E8%AE%BE%E6%A3%80%E9%AA%8C%E6%B3%95-1"><span class="nav-number">7.1.2.2.</span> <span class="nav-text">假设检验法</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%AD%A3%E6%80%81%E6%80%A7"><span class="nav-number">7.1.3.</span> <span class="nav-text">正态性</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%9B%BE%E7%A4%BA%E6%B3%95-2"><span class="nav-number">7.1.3.1.</span> <span class="nav-text">图示法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%81%87%E8%AE%BE%E6%A3%80%E9%AA%8C%E6%B3%95-2"><span class="nav-number">7.1.3.2.</span> <span class="nav-text">假设检验法</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%BC%82%E5%B8%B8%E7%82%B9%E4%B8%8E%E5%BC%BA%E5%BD%B1%E5%93%8D%E7%82%B9%E5%88%A4%E6%96%AD"><span class="nav-number">7.2.</span> <span class="nav-text">异常点与强影响点判断</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%9D%A0%E6%9D%86%E7%82%B9"><span class="nav-number">7.2.1.</span> <span class="nav-text">杠杆点</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%A6%BB%E7%BE%A4%E7%82%B9"><span class="nav-number">7.2.2.</span> <span class="nav-text">离群点</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%9C%AA%E6%A0%87%E5%87%86%E5%8C%96%E6%AE%8B%E5%B7%AE"><span class="nav-number">7.2.2.1.</span> <span class="nav-text">未标准化残差</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%A0%87%E5%87%86%E5%8C%96%E6%AE%8B%E5%B7%AE"><span class="nav-number">7.2.2.2.</span> <span class="nav-text">标准化残差</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%AD%A6%E7%94%9F%E5%8C%96%E6%AE%8B%E5%B7%AE"><span class="nav-number">7.2.2.3.</span> <span class="nav-text">学生化残差</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%89%94%E9%99%A4%E6%AE%8B%E5%B7%AE"><span class="nav-number">7.2.2.4.</span> <span class="nav-text">剔除残差</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%AD%A6%E7%94%9F%E5%8C%96%E5%89%94%E9%99%A4%E6%AE%8B%E5%B7%AE"><span class="nav-number">7.2.2.5.</span> <span class="nav-text">学生化剔除残差</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%BC%BA%E5%BD%B1%E5%93%8D%E7%82%B9"><span class="nav-number">7.2.3.</span> <span class="nav-text">强影响点</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#cook%E8%B7%9D%E7%A6%BB"><span class="nav-number">7.2.3.1.</span> <span class="nav-text">Cook距离</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#dfbeta"><span class="nav-number">7.2.3.2.</span> <span class="nav-text">DfBeta</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%A0%87%E5%87%86%E5%8C%96dfbeta"><span class="nav-number">7.2.3.3.</span> <span class="nav-text">标准化DfBeta</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#dffit"><span class="nav-number">7.2.3.4.</span> <span class="nav-text">DfFit</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%A0%87%E5%87%86%E5%8C%96dffit"><span class="nav-number">7.2.3.5.</span> <span class="nav-text">标准化DfFit</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%85%B1%E7%BA%BF%E6%80%A7%E5%88%A4%E6%96%AD"><span class="nav-number">7.3.</span> <span class="nav-text">共线性判断</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BD%95%E6%97%B6%E9%9C%80%E8%A6%81%E5%A4%84%E7%90%86%E5%85%B1%E7%BA%BF%E6%80%A7"><span class="nav-number">7.3.1.</span> <span class="nav-text">何时需要处理共线性</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%A6%82%E4%BD%95%E5%88%A4%E6%96%AD%E5%85%B1%E7%BA%BF%E6%80%A7"><span class="nav-number">7.3.2.</span> <span class="nav-text">如何判断共线性</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%96%B9%E5%B7%AE%E8%86%A8%E8%83%80%E5%9B%A0%E5%AD%90"><span class="nav-number">7.3.2.1.</span> <span class="nav-text">方差膨胀因子</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%89%B9%E5%BE%81%E5%80%BC"><span class="nav-number">7.3.2.2.</span> <span class="nav-text">特征值</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%9D%A1%E4%BB%B6%E6%95%B0"><span class="nav-number">7.3.2.3.</span> <span class="nav-text">条件数</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%A6%82%E4%BD%95%E5%A4%84%E7%90%86%E5%A4%9A%E9%87%8D%E5%85%B1%E7%BA%BF%E6%80%A7"><span class="nav-number">7.3.3.</span> <span class="nav-text">如何处理多重共线性</span></a></li></ol></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="长水滔滔"
      src="/images/avatar.png">
  <p class="site-author-name" itemprop="name">长水滔滔</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">14</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">3</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">6</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>

        </div>
      </div>
    </div>

    
        <div class="sidebar-inner sidebar-post-related">
          <div class="animated">
              <div class="links-of-blogroll-title"><i class="fa fa-signs-post fa-fw"></i>
    相关文章
  </div>
  <ul class="popular-posts">
    <li class="popular-posts-item">
      <a class="popular-posts-link" href="/2023/01/07/8.%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1%E4%B8%AD%E7%9A%84%E4%B8%80%E4%BA%9B%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5/" rel="bookmark">
        <time class="popular-posts-time">2023-01-07</time>
        <br>
      数理统计中的一些基本概念
      </a>
    </li>
    <li class="popular-posts-item">
      <a class="popular-posts-link" href="/2022/12/24/1.%E5%9B%9E%E5%BD%92%E7%AE%97%E6%B3%95/" rel="bookmark">
        <time class="popular-posts-time">2022-12-24</time>
        <br>
      回归算法
      </a>
    </li>
    <li class="popular-posts-item">
      <a class="popular-posts-link" href="/2023/01/08/9.%E5%B8%B8%E7%94%A8%E5%81%87%E8%AE%BE%E6%A3%80%E9%AA%8C%E6%96%B9%E6%B3%95/" rel="bookmark">
        <time class="popular-posts-time">2023-01-08</time>
        <br>
      常用假设检验方法
      </a>
    </li>
  </ul>

          </div>
        </div>
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/01/20/11.%E5%9B%9E%E5%BD%92%E5%88%86%E6%9E%90/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.png">
      <meta itemprop="name" content="长水滔滔">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="长水滔滔的博客">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="回归分析 | 长水滔滔的博客">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          回归分析
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2023-01-20 00:00:00" itemprop="dateCreated datePublished" datetime="2023-01-20T00:00:00+08:00">2023-01-20</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2023-01-22 23:36:29" itemprop="dateModified" datetime="2023-01-22T23:36:29+08:00">2023-01-22</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/" itemprop="url" rel="index"><span itemprop="name">数理统计</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>4.6k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>17 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <p>​</p>
<span id="more"></span>
<h1 id="模型">模型</h1>
<p><span class="math display">\[
\hat y=\beta_0+\beta_1x_1+\beta_2x_2+...+\beta_px_p+\epsilon\\
=\vec x^T\vec \beta+\epsilon\\
\vec \beta \in R^{p+1},\vec x\in R^{p+1}
\]</span></p>
<h1 id="假定">假定</h1>
<ol type="1">
<li>线性假定：每个解释变量对<span
class="math inline">\(y\)</span>的边际效应为常数<span
class="math inline">\(\beta\)</span>；</li>
<li>严格外生性：<span
class="math inline">\(E(\epsilon_i|X)=0\)</span>，就是说给定数据矩阵<span
class="math inline">\(X\)</span>之后，扰动项<span
class="math inline">\(\epsilon_i\)</span>的条件期望为0，也即<span
class="math inline">\(\epsilon_i\)</span>均值独立于所有解释变量的观测数据，而不仅仅是同一观测数据<span
class="math inline">\(\vec x_i\)</span>的解释变量。这意味着<span
class="math inline">\(\epsilon\)</span>与所有个体的解释变量都不相关。此假定能保证OLS是一个无偏估计。</li>
<li>不存在严格多重共线性；</li>
<li>球形扰动项：即扰动项具有零均值、等方差、不相关。此假定能保证OLS是方差最小的无偏估计。</li>
</ol>
<h1 id="参数估计">参数估计</h1>
<p><span class="math display">\[
\hat {\vec\beta}=(X^TX)^{-1}X^T\vec y
\]</span> # 性质</p>
<h2 id="估计量的性质">估计量的性质</h2>
<ol type="1">
<li>线性性：估计量是<span
class="math inline">\(y\)</span>的线性函数；</li>
<li>无偏性：估计量是真实值的无偏估计；</li>
<li>有效性：方差最小，即</li>
</ol>
<p><span class="math display">\[
{Var(\hat{\vec \beta})}={Var((X^TX)^{-1}X^T\vec y)}\\
=Var((X^TX)^{-1}X^T(X\vec \beta+\epsilon))
\]</span></p>
<p>因<span class="math inline">\(\vec\beta\)</span>为常数，因此 <span
class="math display">\[
=Var((X^TX)^{-1}X^T\epsilon)\\
=(X^TX)^{-1}X^TVar(\epsilon)X(X^TX)^{-1}
\]</span> 当假定球星扰动项时，即<span
class="math inline">\(Var(\epsilon)=\sigma^2I\)</span>： <span
class="math display">\[
=(X^TX)^{-1}X^T\sigma^2 IX(X^TX)^{-1}\\
=(X^TX)^{-1}\sigma^2
\]</span> <span
class="math inline">\(\sigma^2\)</span>常用扰动项方差，常用误差均方<span
class="math inline">\(s^2\)</span>来估计。因为<span
class="math inline">\(e_i\)</span>的均值为0，因此<span
class="math inline">\(\sigma^2\)</span>的无偏估计为： <span
class="math display">\[
\hat \sigma^2=\vec e^T\vec e/(n-p-1)
\]</span></p>
<ol start="4" type="1">
<li>当残差服从正态分布时，估计量也将服从正态分布。</li>
</ol>
<h2 id="ols的正交性">OLS的正交性</h2>
<p>残差向量<span class="math inline">\(\vec e\)</span>与拟合值向量<span
class="math inline">\(\hat {\vec y}\)</span>，常数1向量<span
class="math inline">\(\vec 1\)</span>以及自变量向量<span
class="math inline">\(\vec x\)</span>正交，即内积为0。</p>
<h2 id="平方和分解">平方和分解</h2>
<p>有常数项时，总离差平方和可以分解为模型的回归平方和和残差平方和：
<span class="math display">\[
\sum_{i=1}^n(y_i-\bar y)^2\\
=\sum_{i=1}^n(y_i-\hat y_i+\hat y_i-\bar y)^2\\
=\sum_{i=1}^n[(y_i-\hat y_i)^2+2(y_i-\hat y_i)(\hat y_i-\bar y)+(\hat
y_i-\bar y)^2]\\
=\sum_{i=1}^n[e_i^2+2e_i(\hat y_i-\bar y)+(\hat y_i-\bar y)^2]\\
=\sum_{i=1}^ne_i^2+\sum_{i=1}^n(\hat y_i-\bar y)^2+2\sum_{i=1}^ne_i\hat
y_i-2\bar y\sum_{i=1}^ne_i
\]</span> 由于<span class="math inline">\(\vec e\)</span>与<span
class="math inline">\(\vec x,\vec y,\vec 1\)</span>均正交，因此<span
class="math inline">\(\sum_{i=1}^ne_i\hat
y_i=0,\sum_{i=1}^ne_i=0\)</span>，因此有： <span class="math display">\[
\sum_{i=1}^n(y_i-\bar y)^2=\sum_{i=1}^ne_i^2+\sum_{i=1}^n(\hat y_i-\bar
y)^2
\]</span>
<strong>这个分解公式只有回归方程中有常数项才成立，</strong>因为此时才能保证<span
class="math inline">\(\sum_{i=1}^ne_i=0\)</span>成立。</p>
<h1 id="假设检验">假设检验</h1>
<h2 id="f检验">F检验</h2>
<p>常见的<span class="math inline">\(H_0\)</span>假设为： <span
class="math display">\[
H_0:\beta_1=\beta_2=...=\beta_p=0
\]</span> 利用总离差平方和分解公式： <span class="math display">\[
\sum_{i=1}^n(y_i-\bar y)^2=\sum_{i=1}^n(y_i-\hat
y_i)^2+\sum_{i=1}^n(\hat y_i-\bar y)^2\\
=SSE+SSR
\]</span> 可以证明，下面的统计量服从<span
class="math inline">\(F\)</span>分布： <span class="math display">\[
F=\frac{SSR/p}{SSE/(n-p-1)}
\]</span></p>
<h2 id="t检验">t检验</h2>
<p>主要检验单个系数，例如<span class="math inline">\(\beta_j,\quad
j=1,2,...p\)</span>，如： <span class="math display">\[
H_0:\beta_j=0
\]</span> 从<span class="math inline">\(\hat{\vec
\beta}\)</span>的协方差<span
class="math inline">\(\sigma^2(X^TX)^{-1}\)</span>中提取<span
class="math inline">\(\hat\beta_{j}\)</span>的方差，利用统计量： <span
class="math display">\[
t_j=\frac{\hat\beta_j}{\hat\sigma\sqrt{c_{jj}}}
\]</span> <span class="math inline">\(c_{jj}\)</span>为<span
class="math inline">\((X^TX)^{-1}\)</span>中第<span
class="math inline">\(j\)</span>行<span
class="math inline">\(j\)</span>列元素。</p>
<h2 id="偏决定系数与偏f检验">偏决定系数与偏F检验</h2>
<p>偏决定系数测量的是在回归方程中包含若干个自变量时，再引入某一个新的自变量时残差平方和的减少量。例如，记<span
class="math inline">\(SSE(x_2)\)</span>是模型中只有自变量<span
class="math inline">\(x_2\)</span>时的残差平方和，<span
class="math inline">\(SSE(x_1,x_2)\)</span>是模型中有自变量<span
class="math inline">\(x_1,x_2\)</span>时的残差平方和，因此在模型中已有<span
class="math inline">\(x_2\)</span>时，<span
class="math inline">\(x_1\)</span>和<span
class="math inline">\(y\)</span>的偏决定系数为： <span
class="math display">\[
\frac{SSE(x_2)-SSE(x_1,x_2)}{SSE(x_2)}
\]</span> 反之亦然。</p>
<p>不失一般性，令<span class="math inline">\(y\)</span>对自变量<span
class="math inline">\(x_1,x_2,...,x_p\)</span>的残差平方和为<span
class="math inline">\(SSE\)</span>，回归平方和为<span
class="math inline">\(SSR\)</span>，在模型中剔除自变量<span
class="math inline">\(x_j\)</span>时候，用<span
class="math inline">\(y\)</span>对剩下的<span
class="math inline">\(p-1\)</span>个自变量做回归，记所得模型的残差平方和为<span
class="math inline">\(SSE(j)\)</span>，回归平方和为<span
class="math inline">\(SSR(j)\)</span>，则自变量<span
class="math inline">\(x_j\)</span>对回归的贡献为<span
class="math inline">\(\Delta=SSR-SSR(j)\)</span>，称为<span
class="math inline">\(x_j\)</span>的偏回归平方和。可构造如下偏F统计量：
<span class="math display">\[
F_j=\frac{\Delta/1}{SSE/(n-p-1)}
\]</span> 此统计量实际上与<span
class="math inline">\(t\)</span>检验是等价的。</p>
<h2 id="偏相关系数">偏相关系数</h2>
<p>当其他变量固定之后，给定的任意两个变量之间的相关系数，叫偏相关系数。偏相关系数等于偏决定系数的平方根，也等于其他变量对两个变量做回归，得到两组残差值，计算两组残差之间的相关系数。</p>
<h1 id="拟合优度">拟合优度</h1>
<h2 id="决定系数r2">决定系数<span
class="math inline">\(R^2\)</span></h2>
<p>用决定系数<span class="math inline">\(R^2\)</span>来度量拟合优度，即
<span class="math display">\[
R^2=\frac{\sum_{i=1}^n(\hat y_i-\bar y)^2}{\sum_{i=1}^n(y_i-\bar
y)^2}=1-\frac{\sum_{i=1}^ne_i^2}{\sum_{i=1}^n(y_i-\bar y)^2}
\]</span> 在有常数项时，<span
class="math inline">\(R^2\)</span>等于拟合值<span
class="math inline">\(\hat y_i\)</span>和<span
class="math inline">\(y_i\)</span>之间相关系数的平方。决定系数的缺点在于：即便增加无效的预测变量也能使得决定系数增加，因此又有了调整决定系数。</p>
<h2 id="调整决定系数r_a2">调整决定系数<span
class="math inline">\(R_a^2\)</span></h2>
<p>调整<span
class="math inline">\(R^2_a\)</span>，主要考虑了解释变量个数： <span
class="math display">\[
R^2_a=1-\frac{\sum_{i=1}^ne_i^2/(n-p-1)}{\sum_{i=1}^n(y_i-\bar
y)^2/(n-1)}
\]</span> 分子自由度为<span
class="math inline">\(n-p\)</span>是因为有<span
class="math inline">\(p+1\)</span>个正规方程。</p>
<h2 id="复相关系数">复相关系数</h2>
<p>为决定系数<span
class="math inline">\(R^2\)</span>的开方，实际上等于拟合值<span
class="math inline">\(\hat y_i\)</span>和<span
class="math inline">\(y_i\)</span>之间相关系数。</p>
<h2 id="无常数项的回归">无常数项的回归</h2>
<p>无常数项回归即消去<span class="math inline">\(\beta_0\)</span>：
<span class="math display">\[
\hat y=\beta_1x_1+\beta_2x_2+...+\beta_px_p+\epsilon
\]</span> 无常数项回归必然经过原点。此时残差向量<span
class="math inline">\(\vec e\)</span>与拟合值向量<span
class="math inline">\(\hat {\vec y}\)</span>，自变量向量<span
class="math inline">\(\vec
x\)</span>正交，即内积为0，<strong>但是不一定与常数1向量<span
class="math inline">\(\vec 1\)</span>正交。</strong></p>
<h1 id="模型选择">模型选择</h1>
<h2 id="全子集回归">全子集回归</h2>
<p>设共有<span
class="math inline">\(p\)</span>个自变量（这里的自变量不包括常数项），每个自变量都有进入或者不进入模型的可能，那么<span
class="math inline">\(y\)</span>关于这些自变量的所有可能的回归模型有<span
class="math inline">\(2^p\)</span>个。当所有自变量都不进入模型，此时模型中只有一个常数项，如果不考虑这个模型，则可能的回归模型有<span
class="math inline">\(2^p-1\)</span>个。</p>
<p>从这<span
class="math inline">\(2^p\)</span>个模型中选择一个最优的模型，就称为全子集回归。如何选择呢，这就需要一个标准，常见的标准有：</p>
<h3 id="调整r2_a">调整<span class="math inline">\(R^2_a\)</span></h3>
<p><span class="math display">\[
R^2_a=1-\frac{\sum_{i=1}^ne_i^2/(n-p-1)}{\sum_{i=1}^n(y_i-\bar
y)^2/(n-1)}
\]</span></p>
<p><span class="math inline">\(R^2_a\)</span>越大模型越好。</p>
<h3 id="hat-sigma2"><span class="math inline">\(\hat
\sigma^2\)</span></h3>
<p><span class="math display">\[
\hat \sigma^2=\frac{\vec e^T\vec e}{n-p-1}
\]</span></p>
<p><span class="math inline">\(\hat \sigma^2\)</span>是<span
class="math inline">\(\sigma^2\)</span>的无偏估计，实际上<span
class="math inline">\(\hat \sigma^2\)</span>和调整<span
class="math inline">\(R_a^2\)</span>是等价的。<span
class="math inline">\(\hat \sigma^2\)</span>越小，模型越好。</p>
<h3 id="赤池信息准则aic">赤池信息准则AIC</h3>
<p>AIC等于-2倍对数似然函数值加上参数个数的2倍： <span
class="math display">\[
AIC=-2lnL+2p
\]</span> AIC越小，模型越好。</p>
<h3 id="c_p统计量"><span class="math inline">\(C_p\)</span>统计量</h3>
<p><span class="math display">\[
C_p=\frac{SSE}{\hat \sigma^2}+2p-n
\]</span></p>
<p><span class="math inline">\(SSE\)</span>是当前模型的误差平方和，<span
class="math inline">\(\hat\sigma^2\)</span>是所有自变量均纳入时模型的均方误差。<span
class="math inline">\(C_p\)</span>统计量越小模型越好。</p>
<h2 id="逐步回归">逐步回归</h2>
<h3 id="前进法">前进法</h3>
<p>前进法的思想是变量由少到多，每次增加一个，直到没有可引入的变量为止。具体做法是先将<span
class="math inline">\(p\)</span>个自变量，分别对因变量<span
class="math inline">\(y\)</span>建立<span
class="math inline">\(p\)</span>个一元线性回归方程，选择<span
class="math inline">\(p\)</span>值显著且最小（也即偏<span
class="math inline">\(F\)</span>值最大）的那个自变量先进入模型，设为<span
class="math inline">\(x_i\)</span>。然后在已经引入<span
class="math inline">\(x_i\)</span>的基础上，依次引入其他<span
class="math inline">\(p-1\)</span>个自变量，建立<span
class="math inline">\(p-1\)</span>个包含两个自变量的回归模型，从中选择<span
class="math inline">\(p\)</span>值最小且显著（偏<span
class="math inline">\(F\)</span>值最大）的那个变量进入方程，依次类推，直到模型外变量均不显著为止。</p>
<p>前进法有明显的不足：某个自变量刚引入时可能是显著的，但是当引入其他自变量后，它变得不显著了，但是前进法不会将其进行剔除。即一旦引入方程的自变量，将是“终身制“。即前进法只能”进“。</p>
<h3 id="后退法">后退法</h3>
<p>后退法与前进法相反，首先用全部<span
class="math inline">\(p\)</span>个自变量建立一个回归方程，然后在这<span
class="math inline">\(p\)</span>个变量中选择一个最不重要的变量，将它从方程中剔除。选择的标准是<span
class="math inline">\(p\)</span>值最大（对应的偏<span
class="math inline">\(F\)</span>值最小）的自变量。借着从剩下的<span
class="math inline">\(p-1\)</span>个自变量重新建立回归方程，重复这一过程，直到没有可剔除的自变量为止。</p>
<p>后退法也有不足：一是一开始把所有自变量引入方程，导致计算量很大。再就是一旦某个自变量被剔除，就再也没有机会进入回归方程。即后退法只能”出“。</p>
<h3 id="逐步法">逐步法</h3>
<p>逐步法的基本思想是有进有出。具体做法是将变量一个一个引入，当每引入一个自变量后，会对已入选的变量逐个检验，如果不再显著，会将其剔除。同样，剔除一个变量之后，会再次对已入选的变量逐个检验，如有有变量不再显著，会将其剔除。检验显著性仍然使用偏F检验或对应的<span
class="math inline">\(p\)</span>值。这个过程反复进行，直到既无显著自变量进入方程，也无不显著自变量从回归方程中剔除。</p>
<p>逐步法需要注意的是：引入自变量的显著性水平<span
class="math inline">\(\alpha_{in}\)</span>需要小于剔除自变量的显著性水平<span
class="math inline">\(\alpha_{out}\)</span>，否则可能出现”死循环“。</p>
<h1 id="回归诊断">回归诊断</h1>
<h2 id="模型适用条件判断">模型适用条件判断</h2>
<h3 id="异方差检验方差齐性">异方差（检验方差齐性）</h3>
<h4 id="图示法">图示法</h4>
<p>画标准化残差或学生化残差与拟合值<span class="math inline">\(\hat
y_i\)</span>的散点图，或者与某一个解释变量<span
class="math inline">\(x_{ik}\)</span>的散点图。如果等方差，残差应该不会随<span
class="math inline">\(\hat y_i\)</span>或<span
class="math inline">\(x_{ik}\)</span>发生变化。</p>
<h4 id="假设检验法">假设检验法</h4>
<h3 id="自相关检验独立性">自相关（检验独立性）</h3>
<h4 id="图示法-1">图示法</h4>
<p>画残差与其滞后之间的散点图，或者对残差绘制自相关图或计算偏自相关系数。</p>
<h4 id="假设检验法-1">假设检验法</h4>
<h3 id="正态性">正态性</h3>
<h4 id="图示法-2">图示法</h4>
<p>考察普通残差<span
class="math inline">\(e_i\)</span>是否服从正态分布，可以画直方图、qq图等。</p>
<h4 id="假设检验法-2">假设检验法</h4>
<p>使用W检验等。</p>
<h2 id="异常点与强影响点判断">异常点与强影响点判断</h2>
<p>异常点主要包括杠杆点和离群点。X空间异常的点称为杠杆点，Y空间异常的点称为离群点。<strong>强影响点</strong>(influential
points)
<strong>是指对模型有较大影响的点，模型中包含该点与不包含该点会使求得的回归系数相差很大</strong>。如果某点既是离群点又是高杠杆点，则该点很有可能是强影响点。</p>
<h3 id="杠杆点">杠杆点</h3>
<p>线性回归的预测值为： <span class="math display">\[
\hat {\vec y}=X\hat{\vec \beta}\\
=X(X^TX)^{-1}X^T\vec y\\
=H\vec y
\]</span> 矩阵<span
class="math inline">\(H=X(X^TX)^{-1}X^T\)</span>是一个投影矩阵，也称为帽子矩阵（给<span
class="math inline">\(\vec y\)</span>戴了一顶帽子），作用是将<span
class="math inline">\(\vec y\)</span>投影到<span
class="math inline">\(X\)</span>的列空间。帽子矩阵是一个<span
class="math inline">\(n*n\)</span>的对称矩阵，矩阵的主对角线上的元素（共有n个）称为帽子统计量。帽子矩阵的迹，也即帽子统计量的和为<span
class="math inline">\(p+1\)</span>，证明其实很简单，因为<span
class="math inline">\(tr(AB)=tr(BA)\)</span>，因此： <span
class="math display">\[
tr(X(X^TX)^{-1}X^T)=tr(X^TX(X^TX)^{-1})\\
=tr(I_{p+1})\\
=p+1
\]</span> 因此帽子统计量的平均值等于 <span class="math display">\[
\frac{p+1}{n}
\]</span>
如果某条观测的帽子统计量是帽子均值的2或3倍，即可视为高杠杆点。</p>
<h3 id="离群点">离群点</h3>
<p>离群点主要通过残差<span
class="math inline">\(e_i,i=1,2,...,n\)</span>来判断。定义残差向量<span
class="math inline">\(\vec e=[e_1,e_2,...,e_i,...,e_n]^T\)</span>，则：
<span class="math display">\[
\vec e=\vec y-\hat{\vec y}\\
=\vec y-H\vec y\\
=(I-H)\vec y
\]</span> 残差向量的协方差矩阵为： <span class="math display">\[
Cov(\vec e,\vec e)=Cov((I-H)\vec y,(I-H)\vec y)\\
=(I-H)Cov(\vec y,\vec y)(I-H)^{T}\\
=(I-H)\sigma^2I(I-H)^T\\
=\sigma^2[II^T-IH^T-HI+HH^T]
\]</span> 因为<span
class="math inline">\(H\)</span>是对称矩阵，也是幂等矩阵，因此有： <span
class="math display">\[
Cov(\vec e,\vec e)=\sigma^2(I-H)
\]</span> 因此可知每个残差<span
class="math inline">\(e_i\)</span>的方差为<span
class="math inline">\((1-h_{ii})\sigma^2\)</span>，<span
class="math inline">\(h_{ii}\)</span>为第<span
class="math inline">\(i\)</span>条观测数据的帽子统计量。</p>
<p>根据上述结论，可以定义下面5类残差：</p>
<h4 id="未标准化残差">未标准化残差</h4>
<p>也称为原始残差，<span class="math inline">\(e_i=y_i-\hat
y_i\)</span>。由于原始残差的方差与帽子统计量有关，各不相同，为了便于进行比较，提出了标准化残差和学生化残差。</p>
<h4 id="标准化残差">标准化残差</h4>
<p>将未标准化残差进行标准化，即先减去均值（有截距项时，残差的均值为0），再除以标准差，即<span
class="math inline">\(\frac{e_i}{\hat
\sigma}\)</span>，标准化残差使得残差具备可比性，一般认为标准化残差绝对值大于3为离群点。标准化残差因为没有解决残差之间方差不等的问题，为此又有了学生化残差。</p>
<h4 id="学生化残差">学生化残差</h4>
<p>公式为<span class="math inline">\(\frac{e_i}{\hat
\sigma(1-h_{ii})}\)</span>，<span
class="math inline">\(h_{ii}\)</span>为第<span
class="math inline">\(i\)</span>条数据的帽子统计量。学生化残差相比较标准化残差进一步解决了方差不等问题，因此更适合做残差比较。不过当<span
class="math inline">\(y\)</span>本身存在异常时，会将回归线拉向自身，会减弱残差的判断作用，为此又有了剔除残差。</p>
<h4 id="剔除残差">剔除残差</h4>
<p>剔除该条记录后，拟合新的模型。新模型输入该条记录的自变量得到预测值，得到的未标准化残差。</p>
<h4 id="学生化剔除残差">学生化剔除残差</h4>
<p>剔除残差进行学生化转换。显然学生化剔除残差更能判断离群点。</p>
<p>如果学生化残差绝对值<span class="math inline">\(\ge
2\)</span>，可以认为是一个可疑点；如果学生化残差绝对值<span
class="math inline">\(\ge 3\)</span>，基本可以认定是一个离群点。</p>
<h3 id="强影响点">强影响点</h3>
<h4 id="cook距离">Cook距离</h4>
<p>将学生化残差与帽子统计量求积： <span class="math display">\[
\frac{e_i^2}{(1-h_{ii})^2\hat\sigma^2}\frac{h_{ii}}{\frac{p+1}{n}}=n\frac{e_i^2}{(1-h_{ii})^2\hat\sigma^2}\frac{h_{ii}}{p+1}
\]</span> <span
class="math inline">\(n\)</span>为所有观测所共有，忽略后即为Cook距离：
<span class="math display">\[
D_i=\frac{e_i^2}{(1-h_{ii})^2\hat\sigma^2}\frac{h_{ii}}{p+1}
\]</span>
直观上看Cook距离综合了y空间的离群点和x空间的杠杆点。Cook距离判断强影响点的标准比较复杂，一般认为Cook距离大于1，可以认为是强影响点。</p>
<h4 id="dfbeta">DfBeta</h4>
<p>剔除第<span
class="math inline">\(i\)</span>条记录后，拟合新的模型。比较剔除前和剔除后，<strong>某一个</strong>回归系数估计值的变化（即求差值），变化较大表示强影响点。</p>
<h4 id="标准化dfbeta">标准化DfBeta</h4>
<p>即对每条观测某一个回归系数的DfBeta进行标准化。</p>
<h4 id="dffit">DfFit</h4>
<p>剔除第<span
class="math inline">\(i\)</span>条记录后，拟合新的模型。比较剔除前和提出后，第<span
class="math inline">\(i\)</span>条记录拟合值的变化（即求差值），变化较大表示强影响点。</p>
<h4 id="标准化dffit">标准化DfFit</h4>
<p>即对每条观测的DfFit进行标准化。</p>
<h2 id="共线性判断">共线性判断</h2>
<p>在近似多重共线性情况下，OLS仍然是最佳线性无偏估计，即仍然是最小方差的估计，但是这并不意味着方差的绝对值很小，实际上，由于多重共线性，OLS估计量的方差会很大，使得系数估计很不稳定，数据矩阵X轻微变动就可能导致参数估计值发生巨大变化，甚至方向与预期相反，此时<span
class="math inline">\(t\)</span>检验也往往不会通过。</p>
<h3 id="何时需要处理共线性">何时需要处理共线性</h3>
<ul>
<li>如果模型只是用来进行预测，则不必关注多重共线性；</li>
<li>如果关注具体的回归系数，但不关心的自变量出现了严重多重共线性，也不必关注；</li>
<li>如果关注具体的回归系数，且关心的自变量存在严重多重共线性，则需要进行处理。</li>
</ul>
<h3 id="如何判断共线性">如何判断共线性</h3>
<h4 id="方差膨胀因子">方差膨胀因子</h4>
<p>将第<span class="math inline">\(k\)</span>个自变量<span
class="math inline">\(x_k\)</span>对其余自变量做一个线性回归，可以计算出该回归方程的决定系数<span
class="math inline">\(R_k^2\)</span>，可以证明<span
class="math inline">\(x_k\)</span>的系数<span class="math inline">\(\hat
\beta_k\)</span>的方差为： <span class="math display">\[
Var(\hat \beta_k)=\frac{\sigma^2}{(1-R_k^2)S_k}
\]</span> <span class="math inline">\(S_k\)</span>为<span
class="math inline">\(x_k\)</span>的离差平方和，<span
class="math inline">\(\sigma^2\)</span>为扰动项的方差。<span
class="math inline">\(x_k\)</span>的方差膨胀因子VIF定义为： <span
class="math display">\[
VIF_k=\frac{1}{1-R_k^2}
\]</span> 因此 <span class="math display">\[
Var(\hat \beta_k)=VIF_k\frac{\sigma^2}{S_k}
\]</span> 从这个式子可以看出来方差膨胀因子的含义，因为<span
class="math inline">\(\sigma^2/S_k\)</span>已经固定，<span
class="math inline">\(VIF_k\)</span>实际上就是使<span
class="math inline">\(\hat \beta_k\)</span>的方差变为<span
class="math inline">\(\frac{\sigma^2}{S_k}\)</span>的倍数。另外VIF的分母部分，也就是<span
class="math inline">\(1-R_k^2\)</span>称为容忍度。</p>
<p>计算每个自变量的方差膨胀因子，一般认为方差膨胀因子大于10，认为该自变量与其他自变量之间存在严重的多重共线性。</p>
<h4 id="特征值">特征值</h4>
<p>数据矩阵<span class="math inline">\(X\)</span>标准化之后，对<span
class="math inline">\(X^TX\)</span>做特征值分解，求得特征值和特征向量。如果有<span
class="math inline">\(r,r\le
p\)</span>个特征值近似为0，以其对应的特征向量为系数的<span
class="math inline">\(X\)</span>的线性组合将近似为0，这意味着<span
class="math inline">\(X\)</span>中有<span
class="math inline">\(r\)</span>个变量与其他变量有严重共线关系。</p>
<h4 id="条件数">条件数</h4>
<p><span class="math inline">\(X^TX\)</span>（<span
class="math inline">\(X\)</span>提前标准化）为<span
class="math inline">\(p*p\)</span>的方阵，有<span
class="math inline">\(p\)</span>个特征值，其中最大特征值除以最小特征值再开方定义为矩阵<span
class="math inline">\(X^TX\)</span>的条件数，条件数较大（例如大于30）时认为有严重的多重共线性。</p>
<h3 id="如何处理多重共线性">如何处理多重共线性</h3>
<ul>
<li>增大样本容量</li>
<li>剔除严重共线性的变量</li>
<li><strong>将严重共线性的变量进行标准化</strong></li>
<li>修改模型的设定</li>
<li>使用其他模型：主成分回归、岭回归等。</li>
</ul>
<p>注意：多项式回归中容易出现多重共线性，例如<span
class="math inline">\(x\)</span>和<span
class="math inline">\(x^2\)</span>的相关性就比较强，此时解决办法是将<span
class="math inline">\(x\)</span>做标准化，并引入标准化变量的平方。</p>

    </div>

    
    
    

    <footer class="post-footer">
          <div class="reward-container">
  <div>Buy me a coffee</div>
  <button>
    赞赏
  </button>
  <div class="post-reward">
      <div>
        <img src="/images/wechatpay.jpg" alt="长水滔滔 微信">
        <span>微信</span>
      </div>

  </div>
</div>

          <div class="followme">
  <span>欢迎关注我的其它发布渠道</span>

  <div class="social-list">

      <div class="social-item">
          <span class="social-link">
            <span class="icon">
              <i class="fab fa-weixin"></i>
            </span>

            <span class="label">WeChat</span>
          </span>

          <img class="social-item-img" src="/images/wechat-qcode.jpg">
      </div>
  </div>
</div>

          <div class="post-tags">
              <a href="/tags/%E5%9B%9E%E5%BD%92/" rel="tag"><i class="fa fa-tag"></i> 回归</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2023/01/12/%E7%A1%AC%E9%97%B4%E9%9A%94SVM/" rel="prev" title="硬间隔支持向量机">
                  <i class="fa fa-chevron-left"></i> 硬间隔支持向量机
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2023/01/27/12.%E7%BA%BF%E6%80%A7%E6%B7%B7%E5%90%88%E6%A8%A1%E5%9E%8B/" rel="next" title="线性混合模型">
                  线性混合模型 <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 2022 – 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">长水滔滔</span>
</div>
<div class="wordcount">
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-line"></i>
    </span>
    <span title="站点总字数">33k</span>
  </span>
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    <span title="站点阅读时长">2:01</span>
  </span>
</div>

    </div>
  </footer>

  
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>
  <div class="reading-progress-bar"></div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/next-boot.js"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.4.1/search.js" integrity="sha256-1kfA5uHPf65M5cphT2dvymhkuyHPQp5A53EGZOnOLmc=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>

  <script class="next-config" data-name="pdf" type="application/json">{"object_url":{"url":"https://cdnjs.cloudflare.com/ajax/libs/pdfobject/2.2.8/pdfobject.min.js","integrity":"sha256-tu9j5pBilBQrWSDePOOajCUdz6hWsid/lBNzK4KgEPM="},"url":"/lib/pdf/web/viewer.html"}</script>
  <script src="/js/third-party/tags/pdf.js"></script>

  <script class="next-config" data-name="mermaid" type="application/json">{"enable":true,"theme":{"light":"default","dark":"dark"},"js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mermaid/9.3.0/mermaid.min.js","integrity":"sha256-QdTG1YTLLTwD3b95jLqFxpQX9uYuJMNAtVZgwKX4oYU="}}</script>
  <script src="/js/third-party/tags/mermaid.js"></script>



  




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>



</body>
</html>
