<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 6.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">

<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.2.1/css/all.min.css" integrity="sha256-Z1K5uhUaJXA7Ll0XrZ/0JhX4lAtZFpT6jkKrEDT0drU=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"example.com","root":"/","images":"/images","scheme":"Pisces","darkmode":false,"version":"8.14.2","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":{"enable":true,"style":null},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"/search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script>

    <meta name="description" content="​">
<meta property="og:type" content="article">
<meta property="og:title" content="广义线性模型总论">
<meta property="og:url" content="http://example.com/2023/03/08/13.%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/index.html">
<meta property="og:site_name" content="长水滔滔的博客">
<meta property="og:description" content="​">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2023-03-07T16:00:00.000Z">
<meta property="article:modified_time" content="2024-11-24T02:29:46.252Z">
<meta property="article:author" content="长水滔滔">
<meta property="article:tag" content="回归">
<meta property="article:tag" content="分类">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="http://example.com/2023/03/08/13.%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"http://example.com/2023/03/08/13.%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/","path":"2023/03/08/13.广义线性模型/","title":"广义线性模型总论"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>广义线性模型总论 | 长水滔滔的博客</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">长水滔滔的博客</p>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%A8%A1%E5%9E%8B"><span class="nav-number">1.</span> <span class="nav-text">模型</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%8C%87%E6%95%B0%E5%88%86%E5%B8%83%E6%97%8F"><span class="nav-number">2.</span> <span class="nav-text">指数分布族</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%AD%A3%E6%80%81%E5%88%86%E5%B8%83"><span class="nav-number">2.1.</span> <span class="nav-text">正态分布</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%A4%9A%E9%A1%B9%E5%88%86%E5%B8%83"><span class="nav-number">2.2.</span> <span class="nav-text">多项分布</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%B3%8A%E6%9D%BE%E5%88%86%E5%B8%83"><span class="nav-number">2.3.</span> <span class="nav-text">泊松分布</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E8%BF%9E%E6%8E%A5%E5%87%BD%E6%95%B0%E9%80%89%E6%8B%A9"><span class="nav-number">3.</span> <span class="nav-text">连接函数选择</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%A7%84%E8%8C%83%E8%BF%9E%E6%8E%A5%E5%87%BD%E6%95%B0"><span class="nav-number">3.1.</span> <span class="nav-text">规范连接函数</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%B8%B8%E7%94%A8%E5%88%86%E5%B8%83%E7%9A%84%E8%BF%9E%E6%8E%A5%E5%87%BD%E6%95%B0"><span class="nav-number">3.2.</span> <span class="nav-text">常用分布的连接函数</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%AD%A3%E6%80%81%E5%88%86%E5%B8%83-1"><span class="nav-number">3.2.1.</span> <span class="nav-text">正态分布</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BC%AF%E5%8A%AA%E5%88%A9%E5%88%86%E5%B8%83"><span class="nav-number">3.2.2.</span> <span class="nav-text">伯努利分布</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%A4%9A%E9%A1%B9%E5%88%86%E5%B8%83-1"><span class="nav-number">3.2.3.</span> <span class="nav-text">多项分布</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%B3%8A%E6%9D%BE%E5%88%86%E5%B8%83-1"><span class="nav-number">3.2.4.</span> <span class="nav-text">泊松分布</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1"><span class="nav-number">4.</span> <span class="nav-text">参数估计</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BC%BC%E7%84%B6%E5%87%BD%E6%95%B0"><span class="nav-number">4.1.</span> <span class="nav-text">似然函数</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%89%9B%E9%A1%BF%E6%B3%95"><span class="nav-number">4.1.1.</span> <span class="nav-text">牛顿法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%BF%AD%E4%BB%A3%E5%86%8D%E5%8A%A0%E6%9D%83%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E6%B3%95"><span class="nav-number">4.1.2.</span> <span class="nav-text">迭代再加权最小二乘法</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%81%87%E8%AE%BE%E6%A3%80%E9%AA%8C"><span class="nav-number">5.</span> <span class="nav-text">假设检验</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%8B%9F%E5%90%88%E4%BC%98%E5%BA%A6"><span class="nav-number">6.</span> <span class="nav-text">拟合优度</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%A5%B1%E5%92%8C%E6%A8%A1%E5%9E%8B"><span class="nav-number">6.1.</span> <span class="nav-text">饱和模型</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#residual-deviance"><span class="nav-number">6.2.</span> <span class="nav-text">residual deviance</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#null-deviance"><span class="nav-number">6.3.</span> <span class="nav-text">null deviance</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BC%AA%E5%86%B3%E5%AE%9A%E7%B3%BB%E6%95%B0"><span class="nav-number">6.4.</span> <span class="nav-text">伪决定系数</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#scaled-deviance"><span class="nav-number">6.5.</span> <span class="nav-text">scaled deviance</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%A8%A1%E5%9E%8B%E9%80%89%E6%8B%A9"><span class="nav-number">7.</span> <span class="nav-text">模型选择</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%A8%A1%E5%9E%8B%E8%AF%8A%E6%96%AD"><span class="nav-number">8.</span> <span class="nav-text">模型诊断</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#deviance-residuals"><span class="nav-number">8.1.</span> <span class="nav-text">deviance residuals</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#response-residual"><span class="nav-number">8.2.</span> <span class="nav-text">response residual</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#partial-residual"><span class="nav-number">8.3.</span> <span class="nav-text">partial residual</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%BA%BF%E6%80%A7"><span class="nav-number">8.4.</span> <span class="nav-text">线性</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%A6%82%E4%BD%95%E6%A3%80%E9%AA%8C"><span class="nav-number">8.4.1.</span> <span class="nav-text">如何检验</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#deviance-residuals-vs-eta"><span class="nav-number">8.4.1.1.</span> <span class="nav-text">deviance residuals VS \(\eta\)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#partial-residuals-plot"><span class="nav-number">8.4.1.2.</span> <span class="nav-text">partial residuals plot</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%A6%82%E4%BD%95%E8%A7%A3%E5%86%B3"><span class="nav-number">8.4.2.</span> <span class="nav-text">如何解决</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%9B%A0%E5%8F%98%E9%87%8F%E5%88%86%E5%B8%83"><span class="nav-number">8.5.</span> <span class="nav-text">因变量分布</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%A6%82%E4%BD%95%E6%A3%80%E9%AA%8C-1"><span class="nav-number">8.5.1.</span> <span class="nav-text">如何检验</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%A6%82%E4%BD%95%E6%A3%80%E9%AA%8C-2"><span class="nav-number">8.5.2.</span> <span class="nav-text">如何检验</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%8B%AC%E7%AB%8B%E6%80%A7"><span class="nav-number">8.6.</span> <span class="nav-text">独立性</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%A4%9A%E9%87%8D%E5%85%B1%E7%BA%BF%E6%80%A7"><span class="nav-number">8.7.</span> <span class="nav-text">多重共线性</span></a></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="长水滔滔"
      src="/images/avatar.png">
  <p class="site-author-name" itemprop="name">长水滔滔</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">16</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">3</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">6</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>

        </div>
      </div>
    </div>

    
        <div class="sidebar-inner sidebar-post-related">
          <div class="animated">
              <div class="links-of-blogroll-title"><i class="fa fa-signs-post fa-fw"></i>
    相关文章
  </div>
  <ul class="popular-posts">
    <li class="popular-posts-item">
      <a class="popular-posts-link" href="/2023/03/08/14.%E5%B8%B8%E7%94%A8%E7%9A%84%E5%B9%BF%E4%B9%89%E7%BA%BF%E5%9E%8B%E6%A8%A1%E5%9E%8B/" rel="bookmark">
        <time class="popular-posts-time">2023-03-08</time>
        <br>
      常用广义线性模型
      </a>
    </li>
    <li class="popular-posts-item">
      <a class="popular-posts-link" href="/2023/01/27/12.%E7%BA%BF%E6%80%A7%E6%B7%B7%E5%90%88%E6%A8%A1%E5%9E%8B/" rel="bookmark">
        <time class="popular-posts-time">2023-01-27</time>
        <br>
      线性混合模型
      </a>
    </li>
    <li class="popular-posts-item">
      <a class="popular-posts-link" href="/2022/12/24/1.%E5%9B%9E%E5%BD%92%E7%AE%97%E6%B3%95/" rel="bookmark">
        <time class="popular-posts-time">2022-12-24</time>
        <br>
      回归算法
      </a>
    </li>
    <li class="popular-posts-item">
      <a class="popular-posts-link" href="/2023/01/08/9.%E5%B8%B8%E7%94%A8%E5%81%87%E8%AE%BE%E6%A3%80%E9%AA%8C%E6%96%B9%E6%B3%95/" rel="bookmark">
        <time class="popular-posts-time">2023-01-08</time>
        <br>
      常用假设检验方法
      </a>
    </li>
    <li class="popular-posts-item">
      <a class="popular-posts-link" href="/2023/01/28/11.%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B/" rel="bookmark">
        <time class="popular-posts-time">2023-01-28</time>
        <br>
      线性回归模型
      </a>
    </li>
  </ul>

          </div>
        </div>
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/03/08/13.%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.png">
      <meta itemprop="name" content="长水滔滔">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="长水滔滔的博客">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="广义线性模型总论 | 长水滔滔的博客">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          广义线性模型总论
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2023-03-08 00:00:00" itemprop="dateCreated datePublished" datetime="2023-03-08T00:00:00+08:00">2023-03-08</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2024-11-24 10:29:46" itemprop="dateModified" datetime="2024-11-24T10:29:46+08:00">2024-11-24</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/" itemprop="url" rel="index"><span itemprop="name">数理统计</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>6.7k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>24 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <p>​</p>
<span id="more"></span>
<h1 id="模型">模型</h1>
<p>广义线性模型有三个主要组成部分：</p>
<ol type="1">
<li>给定<span
class="math inline">\(X_1=x_1,...,X_p=x_p\)</span>后，随机变量<span
class="math inline">\(y\)</span>的条件分布源于指数分布族；</li>
</ol>
<p><span class="math display">\[
\begin{aligned}
y|(X_1=x_2,X_2=x_2,...,X_p=x_p)\sim exp(\theta,\phi,a,b,c)
\end{aligned}
\]</span></p>
<p><span
class="math inline">\(exp(\theta,\phi,a,b,c)\)</span>表示指数分布族，后面我们会详细介绍指数分布族。</p>
<ol start="2" type="1">
<li>给定<span
class="math inline">\(X_1=x_1,...,X_p=x_p\)</span>后，随机变量<span
class="math inline">\(y\)</span>的条件均值用连接函数<span
class="math inline">\(g\)</span>进行转换，转换后为<span
class="math inline">\(\eta\)</span>，要求连接函数<span
class="math inline">\(g\)</span>单调可微；</li>
</ol>
<p><span class="math display">\[
\begin{aligned}
g(E(y|X_1=x_1,...,X_p=x_p))=\eta
\end{aligned}
\]</span></p>
<ol start="3" type="1">
<li>自变量对<span class="math inline">\(\eta\)</span>进行回归；</li>
</ol>
<p><span class="math display">\[
\begin{aligned}
\eta = \beta_0+\beta_1x_1+...+\beta_px_p=\vec x^T\vec \beta\\
\vec x\in R^{p+1},\vec \beta\in R^{p+1}
\end{aligned}
\]</span></p>
<p>因此，广义线性模型的基本形式其实为： <span class="math display">\[
\begin{aligned}
g(y|(X_1=x_1,...,X_p=x_p)=\vec x^T\vec \beta
\end{aligned}
\]</span> 或者可以写为： <span class="math display">\[
\begin{aligned}
y|(X_1=x_1,...,X_p=x_p)=g^{-1}(\vec x^T\vec \beta)
\end{aligned}
\]</span> 这里的<span
class="math inline">\(g^{-1}\)</span>为连接函数<span
class="math inline">\(g\)</span>的反函数，实际上可以看作是神经网络中的激活函数，下面会详细讨论如何选择连接函数或激活函数。</p>
<h1 id="指数分布族">指数分布族</h1>
<p>上面提到广义线性模型中随机变量<span
class="math inline">\(y\)</span>的条件分布来源于指数分布族，下面我们给出指数分布族的概率密度函数：
<span class="math display">\[
\begin{aligned}
f(y;\theta,\phi)=exp(\frac{y\theta-b(\theta)}{a(\phi)}+c(y,\phi))
\end{aligned}
\]</span> 这里的<span
class="math inline">\(a(.),b(.),c(.)\)</span>都是特定函数，随不同的分布有不同的形式。具体而言，<span
class="math inline">\(\theta\)</span>为自然参数，它和分布的均值<span
class="math inline">\(\mu\)</span>有密切关系，往往是我们感兴趣的参数。<span
class="math inline">\(\phi\)</span>为分散参数，它和分布的方差有关。<span
class="math inline">\(a(\phi)\)</span>是<span
class="math inline">\(\phi\)</span>的函数，称为分散函数，该函数的函数形式并不重要，本质上它就是代表了分散参数<span
class="math inline">\(\phi\)</span>，如果每个样本有相同的方差，常会直接令<span
class="math inline">\(a(\phi)=\phi\)</span>（如线性回归模型等），如果每个样本有不同的方差，则往往定义<span
class="math inline">\(a(\phi)\)</span>为： <span class="math display">\[
a(\phi)_i=\frac{\phi}{w_i}
\]</span> 这里的<span
class="math inline">\(w_i\)</span>为每个样本的权重，称为先验权重，当所有样本有相同的权重时，就等价于<span
class="math inline">\(a(\phi)=\phi\)</span>。<span
class="math inline">\(\phi\)</span>则是每个样本都是相同的。</p>
<p>对于广义线性模型，其期望<span
class="math inline">\(\mu\)</span>和方差<span
class="math inline">\(\sigma^2\)</span>与<span
class="math inline">\(b(\theta)\)</span>和<span
class="math inline">\(a(\phi)\)</span>有关： <span
class="math display">\[
\begin{aligned}
\mu = E(y)=b\prime(\theta)\\
\sigma^2 = Var(y)=b\prime\prime(\theta)a(\phi)
\end{aligned}
\]</span></p>
<p>可以看到自然参数<span
class="math inline">\(\theta\)</span>与期望<span
class="math inline">\(\mu\)</span>密切相关，同时也可以看到，方差<span
class="math inline">\(\sigma^2\)</span>由两部分构成：一部分是分散函数<span
class="math inline">\(a(\phi)\)</span>，另一部分是<span
class="math inline">\(b\prime\prime(\theta)\)</span>。</p>
<p><span class="math inline">\(b\prime\prime(\theta)\)</span>是<span
class="math inline">\(b(\theta)\)</span>关于<span
class="math inline">\(\theta\)</span>的二阶导数，它要么是一个常数，要么是一个关于参数<span
class="math inline">\(\theta\)</span>的函数。对于后者，因为<span
class="math inline">\(\theta\)</span>与<span
class="math inline">\(\mu\)</span>相关，这提示<span
class="math display">\[b\prime\prime(\theta)\]</span>可以写成<span
class="math inline">\(\mu\)</span>的函数，定义这个函数为方差函数<span
class="math inline">\(v(\mu)\)</span>： <span class="math display">\[
b\prime\prime(\theta)=v(\mu)
\]</span> 由于<span
class="math inline">\(b\prime\prime(\theta)\)</span>有两种情况，因此：</p>
<ul>
<li>当<span
class="math inline">\(b\prime\prime(\theta)\)</span>为常数，方差函数<span
class="math inline">\(v(\mu)\)</span>为常数，此时分布的方差<span
class="math inline">\(\sigma^2=a(\phi)\)</span>，这意味着方差与均值无关。；</li>
<li>当<span
class="math inline">\(b\prime\prime(\theta)\)</span>为关于均值<span
class="math inline">\(\mu\)</span>的函数，此时分布的方差<span
class="math inline">\(\sigma^2=a(\phi)v(\mu)\)</span>，这意味着方差与均值有关。</li>
</ul>
<p>后面会看到，在正态分布中<span
class="math inline">\(b\prime\prime(\theta)=1\)</span>，所以均值和方差是不相关的，但是对于其他分布则不成立，也就是说正态分布是一个特例。</p>
<p>几种常见分布的方差函数</p>
<table>
<thead>
<tr class="header">
<th>分布</th>
<th>方差函数</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Gaussian</td>
<td>1</td>
</tr>
<tr class="even">
<td>Bernoulli</td>
<td><span class="math inline">\(\mu(1-\mu)\)</span></td>
</tr>
<tr class="odd">
<td>Binomial(k)</td>
<td><span class="math inline">\(\mu(1-\mu/k)\)</span></td>
</tr>
<tr class="even">
<td>Poisson</td>
<td><span class="math inline">\(\mu\)</span></td>
</tr>
<tr class="odd">
<td>Gamma</td>
<td><span class="math inline">\(\mu^2\)</span></td>
</tr>
<tr class="even">
<td>Inverse Gaussian</td>
<td><span class="math inline">\(\mu^3\)</span></td>
</tr>
<tr class="odd">
<td>Negative binomial(α)</td>
<td><span class="math inline">\(\mu+\alpha\mu^3\)</span></td>
</tr>
<tr class="even">
<td>Power(k)</td>
<td><span class="math inline">\(\mu^k\)</span></td>
</tr>
</tbody>
</table>
<p>下面介绍几种常用的源于指数分布族的分布。</p>
<h2 id="正态分布">正态分布</h2>
<p><span class="math display">\[
f(y)=\frac{1}{\sqrt{2\pi}\sigma}exp(-\frac{(y-\mu)^2}{2\sigma^2})\\
=exp(ln(\frac{1}{\sqrt{2\pi}\sigma})-\frac{y^2-2y\mu+\mu^2}{2\sigma^2})\\
=exp(\frac{y\mu-\frac{1}{2}\mu^2}{\sigma^2}+ln(\frac{1}{\sqrt{2\pi}\sigma})-\frac{y^2}{2\sigma^2})
\]</span></p>
<p>易知 <span class="math display">\[
\theta=\mu\\
b(\theta)=\frac{1}{2}\mu^2\\
\phi=\sigma^2\\
a(\phi)=\sigma^2\\
b\prime(\theta)=\mu\\
b\prime\prime(\theta)a(\phi)=\sigma^2
\]</span>
因此对于正态分布来说，均值和方差是不相关的。特别的，对于异方差模型。
<span class="math display">\[
a(\phi)_i=\sigma^2/w_i
\]</span> ## 伯努利分布</p>
<p><span class="math display">\[
f(y)=p^y(1-p)^{1-y}\\
=exp(ylnp+(1-y)ln(1-p))\\
=exp(yln(\frac{p}{1-p})+ln(1-p))
\]</span></p>
<p>易知： <span class="math display">\[
\theta=ln(\frac{p}{1-p})\\
b(\theta)=-ln(1-p)=ln(1+e^\theta)\\
a(\phi)=1\\
p=\frac{e^\theta}{1+e^\theta}\\
\mu=b\prime(\theta)=\frac{e^\theta}{1+e^\theta}=p\\
\sigma^2=b\prime\prime(\theta)a(\phi)=p(1-p)=\mu(1-\mu)
\]</span></p>
<p>因此，对于伯努利分布，分散参数为1，或者说没有分散参数。</p>
<h2 id="多项分布">多项分布</h2>
<h2 id="泊松分布">泊松分布</h2>
<p><span class="math display">\[
f(y)=\frac{\mu^yexp(-\mu)}{y!}\\
=exp(yln\mu-\mu-lny!)
\]</span></p>
<p>因此： <span class="math display">\[
\theta=ln\mu\\
b(\theta)=\mu=exp(\theta)\\
a(\phi)=1
\]</span></p>
<h1 id="连接函数选择">连接函数选择</h1>
<h2 id="规范连接函数">规范连接函数</h2>
<p>我们来考虑一下连接函数<span
class="math inline">\(g\)</span>的选择。理论上连接函数的选择可以有很多种，实际上为了方便，在广义线性模型中常常选择规范连接函数（canonical
link
function）。通常来说，我们习惯对一个分布中的均数进行建模，根据上面的推导，我们已经知道：
<span class="math display">\[
\mu=b\prime(\theta)=g^{-1}(\eta)
\]</span> 如果我们令： <span class="math display">\[
\theta=\eta=\vec x^T\vec \beta
\]</span> 那么我们可以推导出： <span class="math display">\[
b\prime(.)=g^{-1}(.)
\]</span> 或者： <span class="math display">\[
g(.)=(b\prime)^{-1}(.)
\]</span> 这意味着连接函数<span
class="math inline">\(g\)</span>可以选择<span
class="math inline">\(b\)</span>函数一阶导数的反函数，或者说激活函数<span
class="math inline">\(g^{-1}\)</span>可以选择<span
class="math inline">\(b\)</span>函数的一阶导数，这时候的连接函数<span
class="math inline">\(g\)</span>称为规范连接函数。</p>
<p>可以看到，如果<span
class="math inline">\(g\)</span>选择规范连接函数，<span
class="math inline">\(g\)</span>其实和<span
class="math inline">\(b\prime\)</span>函数互为反函数，互反函数有一个性质就是导数得乘积等于1，即：
<span class="math display">\[
g\prime(\mu)b\prime\prime(\theta)=1
\]</span> 这一性质在下面参数估计推导中会有运用。</p>
<p>在选择经典连接函数的情况下，广义线性模型可以用一行进行总结： <span
class="math display">\[
y|(X_1=x_2,X_2=x_2,...,X_p=x_p)\sim exp(\eta,\phi,a,b,c)
\]</span> 注意这里用<span class="math inline">\(\eta\)</span>替换为<span
class="math inline">\(\theta\)</span>。</p>
<h2 id="常用分布的连接函数">常用分布的连接函数</h2>
<h3 id="正态分布-1">正态分布</h3>
<p>根据以上结论可知对于正态分布，有： <span class="math display">\[
b\prime(\theta)=g(\mu)=\mu
\]</span> 此时的连接函数其实为恒等连接，即<span
class="math inline">\(g(\mu)=\mu\)</span>。</p>
<h3 id="伯努利分布">伯努利分布</h3>
<p>对于伯努利分布，有： <span class="math display">\[
\mu=b\prime(\theta)=\frac{e^\theta}{1+e^\theta}=\frac{1}{1+e^{-\eta}}=\frac{1}{1+e^{-X\vec
\beta}}\\
g(\mu)=ln(\frac{\mu}{1-\mu})=\eta=X\vec \beta
\]</span>
即连接函数为logit函数，激活函数为sigmoid函数，此时为logistic回归。</p>
<h3 id="多项分布-1">多项分布</h3>
<h3 id="泊松分布-1">泊松分布</h3>
<p>对于泊松分布： <span class="math display">\[
\mu=b\prime(\theta)=exp(\theta)=exp(\eta)=exp(X\vec \beta)\\
g(\mu)=ln(\mu)=\eta=X\vec \beta
\]</span> 即连接函数为log函数，激活函数为指数函数，此时为泊松回归。</p>
<h1 id="参数估计">参数估计</h1>
<h2 id="似然函数">似然函数</h2>
<p>由于指数分布族的存在，所有广义线性模型都可以在一个统一的框架下进行参数估计，参数估计方法仍然是我们熟悉的最大似然估计法。</p>
<p>考虑数据集<span class="math inline">\(\{\{\vec
x_i,y_i\}\}_{i=1}^n\)</span>，应用规范连接函数，我们有： <span
class="math display">\[
\begin{aligned}
y_i|(X_{1}=x_{i1},...X_{p}=x_{ip})&amp;\sim exp(\theta_i,\phi,a,b,c)\\
i=&amp;1,2,...,n
\end{aligned}
\]</span> 其中： <span class="math display">\[
\begin{aligned}
\theta_i =\eta_i= \vec x_i^T\vec \beta,&amp; \quad \vec \beta\in
R^{p+1}\\
\mu_i=E(y_i|(X_{1}=x_{i1},...&amp;X_{p}=x_{ip}))=b\prime(\theta_i)=g^{-1}(\eta_i)
\end{aligned}
\]</span> 这样就可以写出似然函数为： <span class="math display">\[
L(\vec
\beta)=\prod_{i=1}^n[\frac{y_i\theta_i-b(\theta_i)}{a_i(\phi)}+c(y_i,\phi)]
\]</span> 对数似然函数为： <span class="math display">\[
lnL(\vec
\beta)=\sum_{i=1}^n[\frac{y_i\theta_i-b(\theta_i)}{a_i(\phi)}+c(y_i,\phi)]
\]</span> ## 优化算法</p>
<h3 id="牛顿法">牛顿法</h3>
<p>我们对<span class="math inline">\(\vec \beta\)</span>求偏导数得：
<span class="math display">\[
\begin{aligned}
\frac{\partial lnL(\vec \beta)}{\partial \vec
\beta}=\sum_{i=1}^n[\frac{\partial lnL(\vec \beta)}{\partial
\theta_i}\frac{\partial \theta_i}{\partial \vec \beta}]\\
=\sum_{i=1}^n[\frac{y_i-b\prime(\theta_i)}{a_i(\phi)}\vec x_i]
\end{aligned}
\]</span> 令一阶导数为<span
class="math inline">\(Z\)</span>，下面继续求二阶导数： <span
class="math display">\[
\begin{aligned}
\frac{\partial ^2lnL(\vec \beta)}{\partial \vec \beta\partial \vec
\beta^T}=\sum_{i=1}^n\frac{\partial Z}{\partial \theta_i}(\frac{\partial
\theta_i}{\partial\vec \beta})\\
=\sum_{i=1}^n[-\frac{b\prime\prime(\theta_i)}{a_i(\phi)}\vec x_i\vec
x_i^T]\\
=-\sum_{i=1}^n\frac{b\prime\prime(\theta_i)}{a_i(\phi)}\vec x_i\vec
x_i^T
\end{aligned}
\]</span></p>
<p>我们将以上推导的结果写成矩阵形式。因为<span
class="math inline">\(Var(y_i)=a_i(\phi)b\prime\prime(\theta_i),g\prime(\mu_i)b\prime\prime(\theta_i)=1\)</span>，可得<span
class="math inline">\(\frac{1}{a_i(\phi)}=\frac{1}{Var(y_i)g\prime(\mu_i)}\)</span>。我们令<span
class="math inline">\(A=diag(\frac{1}{a_i(\phi)})\)</span>，即： <span
class="math display">\[
\begin{aligned}
A=diag(\frac{1}{a_i(\phi)})=\begin{bmatrix}
\frac{1}{Var(y_1)g\prime(\mu_1)}&amp;\cdot\cdot\cdot&amp;0&amp;\cdot\cdot\cdot&amp;0\\
\cdot\cdot\cdot&amp;\cdot\cdot\cdot&amp;\cdot\cdot\cdot&amp;\cdot\cdot\cdot&amp;\cdot\cdot\cdot\\
0&amp;\cdot\cdot\cdot&amp;\frac{1}{Var(y_i)g\prime(\mu_i)}&amp;\cdot\cdot\cdot&amp;0\\
\cdot\cdot\cdot&amp;\cdot\cdot\cdot&amp;\cdot\cdot\cdot&amp;\cdot\cdot\cdot&amp;\cdot\cdot\cdot\\
0&amp;\cdot\cdot\cdot&amp;0&amp;\cdot\cdot\cdot&amp;\frac{1}{Var(y_n)g\prime(\mu_n)}\\
\end{bmatrix}
\end{aligned}
\]</span> 同时令<span
class="math inline">\(\frac{b\prime\prime(\theta_i)}{a_i(\phi)}=\frac{1}{Var(y_i)[g\prime(\mu_i)]^2}\)</span>以及<span
class="math inline">\(W=diag(\frac{b\prime\prime(\theta_i)}{a_i(\phi)})\)</span>，即：
<span class="math display">\[
\begin{aligned}
W=diag(\frac{b\prime\prime(\theta_i)}{a_i(\phi)})=\begin{bmatrix}
\frac{1}{Var(y_1)[g\prime(\mu_1)]^2}&amp;\cdot\cdot\cdot&amp;0&amp;\cdot\cdot\cdot&amp;0\\
\cdot\cdot\cdot&amp;\cdot\cdot\cdot&amp;\cdot\cdot\cdot&amp;\cdot\cdot\cdot&amp;\cdot\cdot\cdot\\
0&amp;\cdot\cdot\cdot&amp;\frac{1}{Var(y_i)[g\prime(\mu_i)]^2}&amp;\cdot\cdot\cdot&amp;0\\
\cdot\cdot\cdot&amp;\cdot\cdot\cdot&amp;\cdot\cdot\cdot&amp;\cdot\cdot\cdot&amp;\cdot\cdot\cdot\\
0&amp;\cdot\cdot\cdot&amp;0&amp;\cdot\cdot\cdot&amp;\frac{1}{Var(y_n)[g\prime(\mu_n)]^2}\\
\end{bmatrix}
\end{aligned}
\]</span></p>
<p>则对数似然函数的梯度为： <span class="math display">\[
\frac{\partial ln L(\vec \beta)}{\partial \vec \beta}=X^TA(\vec y-\vec
\mu)
\]</span> 对数似然函数的Hessian为： <span class="math display">\[
\frac{\partial^2 ln L(\vec \beta)}{\partial \vec \beta\partial \vec
\beta^T}=-X^TWX
\]</span></p>
<p>然后就可以用牛顿法求解了，第<span
class="math inline">\(k+1\)</span>轮牛顿迭代公式为： <span
class="math display">\[
\hat{\vec \beta}_{k+1}=\hat{\vec \beta}_k+(X^TW_kX)^{-1}X^TA_k(\vec
y-\vec \mu_k)
\]</span></p>
<h3 id="迭代再加权最小二乘法">迭代再加权最小二乘法</h3>
<p>当使用规范连接函数时，牛顿法实际上与迭代再加权最小二乘法等价，下面我们简单说明一下。我们对牛顿迭代公式稍作变化得：
<span class="math display">\[
\hat{\vec\beta}_{k+1}=(X^TW_kX)^{-1}(X^TW_kX\vec \beta_k+X^TA_k(\vec
y-\vec \mu_k))
\]</span> 注意到矩阵<span class="math inline">\(W\)</span>和矩阵<span
class="math inline">\(A\)</span>存在关系<span
class="math inline">\(A=Wg\prime(\vec
\mu)\)</span>，因此上述公式可以进一步化简为： <span
class="math display">\[
\begin{aligned}
\hat{\vec \beta}_{k+1}=&amp;(X^TW_kX)^{-1}X^TW_k(X\hat{\vec
\beta}_k+g\prime(\vec \mu_k)(\vec y-\vec \mu_k))\\
\hat{\vec \beta}_{k+1}=&amp;(X^TW_kX)^{-1}X^TW_k\vec z_k
\end{aligned}
\]</span> 这里的<span class="math inline">\(\vec z_k=X\hat{\vec
\beta}_k+g\prime(\vec \mu_k)(\vec y-\vec
\mu_k)\)</span>，在很多文献中<span class="math inline">\(\vec
z_k\)</span>被称为调整后的因变量或者工作变量。这样以来，牛顿迭代过程就可以看作是迭代再加权最小二乘法。</p>
<p>在迭代再加权最小二乘法中，第<span
class="math inline">\(k\)</span>轮我们使用：</p>
<ul>
<li>当前<span class="math inline">\(\vec \beta\)</span>的估计值<span
class="math inline">\(\hat{\vec \beta}_k\)</span>去计算新的权重矩阵<span
class="math inline">\(W_k\)</span>和新的工作向量<span
class="math inline">\(\vec z_k\)</span>；</li>
<li>然后以<span
class="math inline">\(W_k\)</span>为权重，用设计矩阵<span
class="math inline">\(X\)</span>对<span class="math inline">\(\vec
z_k\)</span>进行回归，从而得到<span class="math inline">\(\hat{\vec
\beta}_{k+1}\)</span></li>
</ul>
<p>使用迭代再加权最小二乘法进行参数迭代有两点好处，一个是利于编程，因为对于牛顿法不同分布有不同的迭代公式，而迭代再加权最小二乘法则有统一的公式，再者类似普通回归方便我们导出与模型诊断有关的统计量。</p>
<h1 id="假设检验">假设检验</h1>
<p>在线性回归中，参数的推断是精确推断，这是因为正态分布、线性和最小二乘法良好的性质。但是对于广义线性模型而言，参数的推断往往不是精确的，而是近似的。这是因为我们很难找出广义线性模型下统计量的精确分布，只知道大样本下的渐进分布，而这些推断都是基于这些渐进分布进行的。</p>
<p>根据似然理论，我们知道大样本下参数估计值<span
class="math inline">\(\hat {\vec \beta}\)</span>的渐进分布为： <span
class="math display">\[
\hat{\vec \beta}\mathop{\rightarrow}^dN(\vec \beta,(I(\vec \beta))^{-1})
\]</span> 这里的<span class="math inline">\(I(\vec
\beta)\)</span>为Fisher信息矩阵，由于<span class="math inline">\(\vec
\beta\)</span>是未知的，因此<span class="math inline">\(I(\vec
\beta)\)</span>是未知的，常用<span class="math inline">\(I(\hat{\vec
\beta})\)</span>来估计，而<span class="math inline">\(I(\hat{\vec
\beta})\)</span>等于样本Hessian矩阵取负： <span class="math display">\[
I(\hat{\vec \beta})=-\frac{\partial^2 ln L(\vec \beta)}{\partial \vec
\beta\partial \vec \beta^T}=X^TWX
\]</span> 有了<span class="math inline">\(\hat{\vec
\beta}\)</span>和<span class="math inline">\(I^{-1}(\vec
\beta)\)</span>的估计，接下来就可以使用Wald检验、似然比检验和LM检验了，同时也可以构造执行区间。</p>
<h1 id="拟合优度">拟合优度</h1>
<p>广义线性模型中，评价拟合优度的有两类统计量，一个deviance统计量，一类是Pearson卡方统计量。</p>
<h2 id="饱和模型">饱和模型</h2>
<p>引入deviance概念之前，还需要先介绍一下饱和模型的概念。饱和模型是一个完美拟合数据的模型。在这个意义上看，完美模型对第<span
class="math inline">\(i\)</span>个观测的预测值<span
class="math inline">\(\hat y_i\)</span>等于真实值<span
class="math inline">\(y_i\)</span>。前面提到对数似然函数等于： <span
class="math display">\[
lnL(\vec
\beta)=\sum_{i=1}^n[\frac{y_i\theta_i-b(\theta_i)}{a_i(\phi)}+c(y_i,\phi)]
\]</span> 计算饱和模型其实就是用<span
class="math inline">\(y_i\)</span>替换公式中的<span
class="math inline">\(\mu_i\)</span>，如果用的是规范连接函数，这等于设置<span
class="math inline">\(\theta_i=g(y_i)\)</span>。记饱和模型的对数似然函数为<span
class="math inline">\(L_s\)</span>，则： <span class="math display">\[
L_s=\sum_{i=1}^n[\frac{y_ig(y_i)-b(g(y_i))}{a_i(\phi)}+c(y_i,\phi)]
\]</span></p>
<h2 id="residual-deviance">residual deviance</h2>
<p><code>deviance</code>又称为<code>residual deviance</code>，是广义线性模型中的一个关键概念。<code>deviance</code>定义了当前拟合模型的对数似然函数值<span
class="math inline">\(L(\hat{\vec \beta})\)</span>与饱和模型<span
class="math inline">\(L_s\)</span>之间的差异，公式为： <span
class="math display">\[
D=-2(L(\hat {\vec \beta})-L_s)\phi
\]</span> <span class="math inline">\(L(\hat {\vec
\beta})\)</span>值永远小于等于<span
class="math inline">\(L_s\)</span>，因此<code>deviance</code>总是大于等于0，仅当当前模型拟合完美时为0。</p>
<p>如果使用规范连接函数，<code>deviance</code>还可以被定义为： <span
class="math display">\[
\begin{aligned}
D=-\frac{2}{a(\phi)}\sum_{i=1}^n[y_i\hat \theta_i-b(\hat
\theta_i)-y_ig(y_i)+b(g(y_i))]\phi\\
=\frac{2}{a(\phi)}\sum_{i=1}^n[y_i(g(y_i)-\hat
\theta_i)-b(g(y_i))+b(\hat \theta_i)]\phi
\end{aligned}
\]</span> 在大多数情况下，<span
class="math inline">\(a(\phi)\)</span>正比于<span
class="math inline">\(\phi\)</span>，因此偏差不依赖于<span
class="math inline">\(\phi\)</span>。</p>
<p><code>deviance</code>实际上是线性回归模型中残差平方和<code>SSE</code>的推广。为了证明这一点，我们考虑线性回归模型。已知线性回归模型中<span
class="math inline">\(\phi=\sigma^2,\theta=\mu=\eta,a(\phi)=\phi,b(\theta)=\frac{\mu^2}{2}\)</span>，因此：
<span class="math display">\[
\begin{aligned}
D=&amp;\frac{2}{\sigma^2}\sum_{i=1}^n[y_i(y_i-\hat\eta_i)-\frac{y_i^2}{2}+\frac{\hat
\eta_i^2}{2}]\sigma^2\\
=&amp;\sum_{i=1}^n[2y_i^2-2y_i\hat \eta_i-y_i^2+\hat \eta_i^2]\\
=&amp;\sum_{i=1}^n(y_i-\hat \eta_i)^2==SSE
\end{aligned}
\]</span></p>
<h2 id="null-deviance">null deviance</h2>
<p>除了<code>residual deviance</code>，还有<code>null deviance</code>，它的公式为：
<span class="math display">\[
D_0=-2(L(\hat {\vec \beta_0})-L_s)\phi
\]</span> <span class="math inline">\(L(\hat {\vec
\beta_0})\)</span>表示只有截距项的模型的对数似然函数值。<code>null residual</code>实际上是线性回归模型中总平方和<code>SST</code>的推广：
<span class="math display">\[
D_0=\sum_{i=1}^n(y_i-\hat\eta_i)^2
\]</span> 此时的<span class="math inline">\(\hat\eta_i=\hat \beta_0=\bar
y\)</span>，因此<span class="math inline">\(D_0=SST\)</span>。</p>
<h2 id="伪决定系数">伪决定系数</h2>
<p>类比线性回归，我们对决定系数<span
class="math inline">\(R^2\)</span>进行推广： <span
class="math display">\[
R^2=1-\frac{D}{D_0}
\]</span> 广义线性模型的<span
class="math inline">\(R^2\)</span>又与线性回归中的决定系数有些不同，一方面它并不是模型解释方差的占比，而是测量了当前模型与完美模型的接近程度；另一方面，它与相关系数无关。正因为如此，有些资料又将这个<span
class="math inline">\(R^2\)</span>称为伪<span
class="math inline">\(R^2\)</span>。</p>
<h2 id="scaled-deviance">scaled deviance</h2>
<p>还有一个与<code>deviance</code>有关的度量指标是<code>scaled deviance</code>：
<span class="math display">\[
D^{*}=\frac{D}{\phi}=-2[L(\hat{\vec \beta})-L_s]
\]</span> 如果<span
class="math inline">\(\phi\)</span>为1，则<code>deviance</code>和<code>scaled deviance</code>是一致的（例如二项或泊松回归）。<code>scaled deviance</code>有一个重要的性质：
<span class="math display">\[
D^{*}\mathop{\rightarrow}^d\chi^2(n-p-1)
\]</span> 注意以上是渐进分布，不过在线性回归的情景下，<span
class="math inline">\(D^{*}=\frac{1}{\sigma^2}SSE\sim
\chi^2(n-p-1)\)</span>，此时是精确分布。</p>
<p>在<span class="math inline">\(\phi\)</span>未知的情况下，由于<span
class="math inline">\(D^{*}\mathop{\rightarrow}^d\chi^2(n-p-1)\)</span>，这实际上为我们提供了一种估计<span
class="math inline">\(\phi\)</span>的方法。因为<span
class="math inline">\(E(\chi^2(n-p-1))=n-p-1\)</span>，因此<span
class="math inline">\(\phi\)</span>的估计为： <span
class="math display">\[
\hat \phi = \frac{-2[L(\hat{\vec \beta})-L_s]}{n-p-1}
\]</span> 在线性回归模型中<span class="math inline">\(\hat \phi=\hat
\sigma^2\)</span>，这和预期是一致的。</p>
<p><code>scaled deviance</code>还有更重要的作用，就是可用于对广义线性模型的系数集进行假设检验。</p>
<p>假定我们现在有一个模型<span
class="math inline">\(M_2\)</span>，有<span
class="math inline">\(p_2\)</span>个自变量（不包括截距），另有一个模型<span
class="math inline">\(M_1\)</span>，自变量个数为<span
class="math inline">\(p_1\)</span>，同时有<span
class="math inline">\(p_1\lt p_2\)</span>，且<span
class="math inline">\(M_1\)</span>的自变量是<span
class="math inline">\(M_2\)</span>的子集。换句话说，<span
class="math inline">\(M_1\)</span>嵌套于<span
class="math inline">\(M_2\)</span>中。然后我们可以做一个假设检验，检验的零假设为：<span
class="math inline">\(M_2\)</span>中多余的自变量系数同时为0。例如，<span
class="math inline">\(M_1\)</span>模型的系数为<span
class="math inline">\(\beta_0,\beta_1,...,\beta_{p_{1}}\)</span>，<span
class="math inline">\(M_2\)</span>模型的系数为<span
class="math inline">\(\beta_0,\beta_1,...,\beta_{p_{1}},\beta_{p_{1}+1},...,\beta_{p_{2}}\)</span>，零假设为：
<span class="math display">\[
H_0:\beta_{p_1+1}=...=\beta_{p_2}=0
\]</span> 我们可以构造如下统计量进行检验： <span class="math display">\[
D_{p1}^*-D_{p2}^*\mathop{\rightarrow}^d\chi^2(p_2-p_1)
\]</span> 如果<span class="math inline">\(H_0\)</span>成立，则<span
class="math inline">\(D_{p1}^*-D_{p2}\)</span>应该比较小，反之，我们有理由拒绝<span
class="math inline">\(H_0\)</span>。</p>
<p><code>scaled deviance</code>很显然移除了<span
class="math inline">\(\phi\)</span>项，但是仍然依赖于<span
class="math inline">\(\phi\)</span>，因为似然函数中仍然包含<span
class="math inline">\(\phi\)</span>，因此在不知道<span
class="math inline">\(\phi\)</span>时是无法计算<span
class="math inline">\(D^*\)</span>的，这使得上面式子应用受到了限制。幸运的是如果构造如下统计量，就可以消掉<span
class="math inline">\(\phi\)</span>了： <span class="math display">\[
F=\frac{(D^*_{p_1}-D^*_{p_2})/(p_2-p_1)}{D^*_{p_2}/(n-p_2-1)}=\frac{(D_{p_1}-D_{p_2})/(p_2-p_1)}{D_{p_2}/(n-p_2-1)}\mathop{\rightarrow}^d
F(p_2-p_1,n-p_2-1)
\]</span> 注意到当<span
class="math inline">\(p_1=0,p_2=p\)</span>时，这个检验实际上是线性回归中<span
class="math inline">\(F\)</span>检验的扩展。</p>
<p>借助<code>deviance</code>来对模型进行的分析称为<code>analysis of deviance</code>。在<span
class="math inline">\(R\)</span>中<code>deviance</code>的计算和相关的检验都可以通过<code>anova</code>函数进行。</p>
<h1 id="模型选择">模型选择</h1>
<h1 id="模型诊断">模型诊断</h1>
<p>上面已经提到，广义线性模型是建立在一些概率假设之上的，这要求应该对模型参数<span
class="math inline">\(\vec \beta\)</span>和<span
class="math inline">\(\phi\)</span>进行合理的推断。</p>
<p>总的来说，如果我们应用了规范连接函数，我们假定数据是从下面分布中生成的：
<span class="math display">\[
y|(X_1=x_1,...,X_p=x_p)\sim exp(\eta,\phi,a,b,c)
\]</span> 通过这种方式： <span class="math display">\[
\mu=E(y|X_1=x_1,...,X_p=x_p)=g^{-1}(\eta)\\
\eta=\vec x^T\vec \beta
\]</span>
对于具有规范链接函数的逻辑回归和泊松回归，一般模型采用如下形式(独立性是隐含的)：
<span class="math display">\[
y|(X_1=x_1,...,X_p=x_p)\sim Ber(logistic(\eta))\\
y|(X_1=x_1,...,X_p=x_p)\sim Pois(e^{\eta})
\]</span> 上面式子背后的假定是：</p>
<ol type="1">
<li>线性：期望值转换后与自变量之间是线性的；</li>
<li>因变量<span
class="math inline">\(y\)</span>的分布是对应的指数族分布；</li>
<li>独立性</li>
</ol>
<p>这些假设的背后还应该注意几点：</p>
<ol type="1">
<li>等方差在那里？等方差只是某些指数分布族所特定的，这些分布族中<span
class="math inline">\(\theta\)</span>并不影响方差，例如正态分布。然而对于二项分布和泊松分布而言，他们本身就是异方差的。</li>
<li>误差项在那里？误差不是建立线性模型的基础，而只是一个与最小二乘相关的有用概念。</li>
<li>没有提到<span
class="math inline">\(X_1,...,X_p\)</span>的分布，他们可以是确定的，也可以是随机的，可以是离散的，也可以是连续的。另外<span
class="math inline">\(X_1,...,X_p\)</span>并不要求相互独立。</li>
</ol>
<p>检验上述假定要比线性回归复杂的多，原因在于因变量<span
class="math inline">\(y\)</span>的异质性和异方差性，这使得残差<span
class="math inline">\(y_i-\hat
y_i\)</span>的检验变得复杂。而我们第一步要做的就是构造这个残差<span
class="math inline">\(\epsilon_i\)</span>。</p>
<h2 id="deviance-residuals"><strong>deviance residuals</strong></h2>
<p><code>deviance residuals</code>是线性模型中残差<span
class="math inline">\(\epsilon_i=y_i-\hat
y_i\)</span>的推广，它们是参考<code>residuals deviance</code>和<code>SSE</code>之间的类比关系来设计的。<code>residuals deviance</code>可以被表达为下面的<span
class="math inline">\(d_i\)</span>之和： <span class="math display">\[
D=\sum_{i=1}^nd_i
\]</span> 对于线性模型，<span class="math inline">\(d_i=\hat
\epsilon_i^2\)</span>，因为<span
class="math inline">\(D=SSE\)</span>。基于这些，我们可以定义<code>deviance residuals</code>为<span
class="math inline">\(\hat \epsilon^D_i\)</span>： <span
class="math display">\[
\hat \epsilon_i^D=sign(y_i-\mu_i)\sqrt{d_i},\quad i=1,2,...,n
\]</span> 这就是广义线性模型对线性回归中残差<span
class="math inline">\(\hat
\epsilon_i\)</span>的推广。根据前面介绍的<span
class="math inline">\(D^*\mathop{\rightarrow}^d
\chi^2(n-p-1)\)</span>，我们可以导出<span class="math inline">\(\hat
\epsilon_i^D\)</span>为渐进正态分布。<code>deviance residuals</code>还可以标准化得到标准化<code>deviance residuals</code>：
<span class="math display">\[
\hat \epsilon_i^{D_s}=\frac{\hat
\epsilon_i^D}{\sqrt{\hat\phi(1-h_{ii})}}
\]</span> <span class="math inline">\(h_{ii}\)</span>为帽子矩阵<span
class="math inline">\(W^{\frac{1}{2}}X(X^TWX)^{-1}X^TW^{\frac{1}{2}}\)</span>第<span
class="math inline">\(i\)</span>行第<span
class="math inline">\(i\)</span>列的取值。</p>
<p><code>deviance residuals</code>是广义线性模型诊断的关键，当我们提<code>residual</code>时，我们一般指的是<code>deviance residuals</code>（还有其他的residual定义），这也是R中<code>residuals</code>返回的残差。</p>
<p><code>deviance residuals</code>与线性回归中的残差还有一些有趣的联系：</p>
<ol type="1">
<li>类似线性回归，如果使用规范链接函数，<span
class="math inline">\(\sum_{i=1}^n\hat \epsilon_i^D=0\)</span>;</li>
<li>尺度参数的估计可以看作<span class="math inline">\(\hat
\phi_D=\frac{\sum_{i=1}^n(\hat
\epsilon_i^D)^2}{n-p-1}\)</span>，这和线性回归中也是类似的；</li>
<li>因此<span class="math inline">\(\hat \phi_D\)</span>实际上是<span
class="math inline">\(\hat
\epsilon_1^D,...,\hat\epsilon_n^D\)</span>的样本方差，这表明<span
class="math inline">\(\phi\)</span>是总体<code>deviance residuals</code>的渐近方差，即<span
class="math inline">\(Var(\epsilon^D)\approx \phi\)</span>;</li>
</ol>
<h2 id="response-residual">response residual</h2>
<p><code>response residual</code>的定义和线性回归中残差定义是类似的：
<span class="math display">\[
y_i-\mu_i
\]</span>
<code>response residual</code>的缺点是方差不是常数，因而很少使用。</p>
<blockquote>
<p>在R中<code>residuals(glmfit,type='response')</code>返回<code>response residual</code>。</p>
</blockquote>
<h2 id="partial-residual">partial residual</h2>
<p><code>partial residual</code>定义为： <span class="math display">\[
z_i-\hat \eta_i
\]</span>
<code>partial residual</code>可以用来评价某个自变量的非线性关系。</p>
<blockquote>
<p>在R中<code>residuals(glmfit,type='working')</code>或<code>residuals(glmfit,type='partial')</code>返回<code>partial residual</code>。</p>
</blockquote>
<h2 id="线性">线性</h2>
<p>随机变量<span
class="math inline">\(y\)</span>的期望的转换值与自变量<span
class="math inline">\(x_1,…,x_p\)</span>之间的线性关系是广义线性模型的基石。如果这个假设不成立，那么我们从分析中得出的所有结论都被怀疑是有缺陷的。因此，这是一个关键的假设。那么如何检验它呢？</p>
<h3 id="如何检验">如何检验</h3>
<h4 id="deviance-residuals-vs-eta">deviance residuals VS <span
class="math inline">\(\eta\)</span></h4>
<p>我们可以画出标准化的<code>deviance residuals</code>和拟合值的散点图，对于广义线性模型而言，这是<span
class="math inline">\(\hat \eta_i\)</span>和<span
class="math inline">\(\hat \epsilon_i^D\)</span>的散点图，注意不是<span
class="math inline">\(\hat y_i\)</span>和<span
class="math inline">\(\hat
\epsilon_i^D\)</span>的散点图。在线性假定之下，<span
class="math inline">\(\hat \eta_i\)</span>和<span
class="math inline">\(\hat
\epsilon_i^D\)</span>之间应该没有什么趋势。如果观察到非线性，则最好用偏残差图来寻找非线性关系。</p>
<h4 id="partial-residuals-plot">partial residuals plot</h4>
<h3 id="如何解决">如何解决</h3>
<p>对有问题的自变量进行非线性转换，或者添加交互作用项可能有帮助。另外，考虑对<span
class="math inline">\(y\)</span>做一个非线性转换也可能是有用的。</p>
<h2 id="因变量分布">因变量分布</h2>
<p><code>deviance residuals</code>的近似正态性允许我们对因变量分布进行检验。好消息是，我们可以做到这一点，而不必专用工具。坏消息是，我们必须在不精确性方面付出重要的代价，因为我们采用了渐近分布。这种渐近收敛的速度和有效性在很大程度上取决于几个方面：因变量的分布、样本量和自变量的分布。</p>
<h3 id="如何检验-1">如何检验</h3>
<p>QQ图允许我们检查标准化残差是否遵循<span
class="math inline">\(N(0,1)\)</span>。在正确的因变量分布下，我们期望这些点与对角线对齐。通常情况下，即使在正态数据下，在极端情况下也会偏离对角线，而不是在中心，尽管如果数据是非正态的，这些偏离会更加明显。不幸的是，即使模型完全正确，也有可能严重偏离正态。原因很简单，<code>deviance residuals</code>明显是非正态的，这在逻辑回归中经常发生。</p>
<h3 id="如何检验-2">如何检验</h3>
<p>修正分布假设并不容易，需要考虑更灵活的模型。一种可能是通过转换<span
class="math inline">\(y\)</span>，当然代价是对转换后的因变量建模，而不是对<span
class="math inline">\(y\)</span>建模。</p>
<h2 id="独立性">独立性</h2>
<p>独立性也是一个关键的假设:它保证了我们拥有的关于<span
class="math inline">\(y\)</span>和<span
class="math inline">\(x1,…,xp\)</span>与n个观测值之间关系的信息量是最大的。</p>
<p>残差中是否存在自相关可以用残差的连续图来检验。在不相关条件下，我们期望序列没有残差的跟踪，这是序列正相关的标志。负序列相关可以通过残差的小-大或正-负系统交替的形式来识别。这可以通过延迟更好地进行探索。</p>
<p>在线性模型中，如果数据中存在依赖关系，一旦收集到这些数据，就不能做什么。如果存在序列依赖性，则响应的分化可能导致独立的观测。</p>
<h2 id="多重共线性">多重共线性</h2>
<p>多重共线性也可以出现在广义线性模型中。尽管自变量对因变量是非线性影响，但自变量的转换值是线性的。因此，如果两个或多个预测因子之间高度相关，则模型的拟合将受到影响，因为每个预测因子的单独线性效应将难以与其他相关预测因子分离。</p>
<p>检测多重共线性的一种有效方法是检验每个自变量的VIF。这种情况与线性回归完全相同。</p>
<p>正态分布 <span class="math display">\[
\begin{aligned}
f(y)=\frac{1}{\sqrt{2\pi}\sigma}exp(-\frac{1}{2\sigma^2}(y-\mu)^2)\\
=exp(ln(\frac{1}{\sqrt{2\pi}\sigma})-\frac{1}{2\sigma^2}(y^2-2y\mu+\mu^2))\\
=exp(\frac{y\mu-\frac{1}{2}\mu^2}{\sigma^2}-\frac{y^2}{2\sigma^2}+ln(\frac{1}{\sqrt{2\pi}\sigma}))
\end{aligned}
\]</span> 因此： <span class="math display">\[
\begin{aligned}
b(\theta)=\frac{1}{2}\mu^2\\
\theta=\mu\\
a(\phi)=\sigma^2\\
g(\mu)=\eta=\theta=\mu=X\vec \beta\\
Var(y)=\sigma^2\\
g\prime(\mu)=1
\end{aligned}
\]</span> 进一步可得<span class="math inline">\(A\)</span>矩阵和<span
class="math inline">\(W\)</span>矩阵为： <span class="math display">\[
\begin{aligned}
A=diag(\frac{1}{Var(y_i)g\prime(\mu_i)})=diag(\frac{1}{\sigma^2_i})=\begin{bmatrix}
\frac{1}{\sigma^2}&amp;\cdot\cdot\cdot&amp;0&amp;\cdot\cdot\cdot&amp;0\\
\cdot\cdot\cdot&amp;\cdot\cdot\cdot&amp;\cdot\cdot\cdot&amp;\cdot\cdot\cdot&amp;\cdot\cdot\cdot\\
0&amp;\cdot\cdot\cdot&amp;\frac{1}{\sigma^2}&amp;\cdot\cdot\cdot&amp;0\\
\cdot\cdot\cdot&amp;\cdot\cdot\cdot&amp;\cdot\cdot\cdot&amp;\cdot\cdot\cdot&amp;\cdot\cdot\cdot\\
0&amp;\cdot\cdot\cdot&amp;0&amp;\cdot\cdot\cdot&amp;\frac{1}{\sigma^2}\\
\end{bmatrix}\\
W=diag(\frac{1}{Var(y_i)(g\prime(\mu_i))^2})=diag(\frac{1}{\sigma^2_i})=\begin{bmatrix}
\frac{1}{\sigma^2}&amp;\cdot\cdot\cdot&amp;0&amp;\cdot\cdot\cdot&amp;0\\
\cdot\cdot\cdot&amp;\cdot\cdot\cdot&amp;\cdot\cdot\cdot&amp;\cdot\cdot\cdot&amp;\cdot\cdot\cdot\\
0&amp;\cdot\cdot\cdot&amp;\frac{1}{\sigma^2}&amp;\cdot\cdot\cdot&amp;0\\
\cdot\cdot\cdot&amp;\cdot\cdot\cdot&amp;\cdot\cdot\cdot&amp;\cdot\cdot\cdot&amp;\cdot\cdot\cdot\\
0&amp;\cdot\cdot\cdot&amp;0&amp;\cdot\cdot\cdot&amp;\frac{1}{\sigma^2}\\
\end{bmatrix}
\end{aligned}
\]</span> 梯度以及解为： <span class="math display">\[
\begin{aligned}
X^TA(\vec y-\vec \mu)=X^T\frac{1}{\sigma^2}I(\vec y-X\vec \beta)=0\\
\hat {\vec \beta}=(X^TX)^TX^T\vec y
\end{aligned}
\]</span> Hessian以及<span class="math inline">\(\hat {\vec
\beta}\)</span>的渐进方差为： <span class="math display">\[
\begin{aligned}
-\frac{1}{\sigma^2}X^TX\\
[\frac{1}{\sigma^2}X^TX]^{-1}
\end{aligned}
\]</span> 伯努利分布 <span class="math display">\[
\begin{aligned}
f(y)=p^y(1-p)^{1-y}\\
=exp(ln(p^y(1-p)^{1-y}))\\
=exp(ylnp+(1-y)ln(1-p))\\
=exp(ylnp-yln(1-p)+ln(1-p))\\
=exp(yln\frac{p}{1-p}+ln(1-p))
\end{aligned}
\]</span> 因此： <span class="math display">\[
\begin{aligned}
\theta= ln\frac{p}{1-p}\\
b\prime(\theta)
\end{aligned}
\]</span></p>
<p><span class="math display">\[
\frac{e^{-\lambda}\lambda^y}{y!}=\frac{e^{-y}y^y}{y!}
\]</span></p>

    </div>

    
    
    

    <footer class="post-footer">
          <div class="reward-container">
  <div>Buy me a coffee</div>
  <button>
    赞赏
  </button>
  <div class="post-reward">
      <div>
        <img src="/images/wechatpay.jpg" alt="长水滔滔 微信">
        <span>微信</span>
      </div>

  </div>
</div>

          <div class="followme">
  <span>欢迎关注我的其它发布渠道</span>

  <div class="social-list">

      <div class="social-item">
          <span class="social-link">
            <span class="icon">
              <i class="fab fa-weixin"></i>
            </span>

            <span class="label">WeChat</span>
          </span>

          <img class="social-item-img" src="/images/wechat-qcode.jpg">
      </div>
  </div>
</div>

          <div class="post-tags">
              <a href="/tags/%E5%9B%9E%E5%BD%92/" rel="tag"><i class="fa fa-tag"></i> 回归</a>
              <a href="/tags/%E5%88%86%E7%B1%BB/" rel="tag"><i class="fa fa-tag"></i> 分类</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2023/03/08/14.%E5%B8%B8%E7%94%A8%E7%9A%84%E5%B9%BF%E4%B9%89%E7%BA%BF%E5%9E%8B%E6%A8%A1%E5%9E%8B/" rel="prev" title="常用广义线性模型">
                  <i class="fa fa-chevron-left"></i> 常用广义线性模型
                </a>
            </div>
            <div class="post-nav-item">
            </div>
          </div>
    </footer>
  </article>
</div>






</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 2022 – 
  <span itemprop="copyrightYear">2024</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">长水滔滔</span>
</div>
<div class="wordcount">
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-line"></i>
    </span>
    <span title="站点总字数">43k</span>
  </span>
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    <span title="站点阅读时长">2:36</span>
  </span>
</div>

    </div>
  </footer>

  
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>
  <div class="reading-progress-bar"></div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/next-boot.js"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.4.1/search.js" integrity="sha256-1kfA5uHPf65M5cphT2dvymhkuyHPQp5A53EGZOnOLmc=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>

  <script class="next-config" data-name="pdf" type="application/json">{"object_url":{"url":"https://cdnjs.cloudflare.com/ajax/libs/pdfobject/2.2.8/pdfobject.min.js","integrity":"sha256-tu9j5pBilBQrWSDePOOajCUdz6hWsid/lBNzK4KgEPM="},"url":"/lib/pdf/web/viewer.html"}</script>
  <script src="/js/third-party/tags/pdf.js"></script>

  <script class="next-config" data-name="mermaid" type="application/json">{"enable":true,"theme":{"light":"default","dark":"dark"},"js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mermaid/9.3.0/mermaid.min.js","integrity":"sha256-QdTG1YTLLTwD3b95jLqFxpQX9uYuJMNAtVZgwKX4oYU="}}</script>
  <script src="/js/third-party/tags/mermaid.js"></script>



  




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>



</body>
</html>
